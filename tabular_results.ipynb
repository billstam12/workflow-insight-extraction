{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9463ef71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult\n",
      "full\n",
      "no_variance_filter\n",
      "Could not load F1 scores for Adult_no_variance_filter: [Errno 2] No such file or directory: './results/adult/no_variance_filter/predictive_quality/predictive_quality_metrics.csv'\n",
      "no_dim_reduction\n",
      "Could not load F1 scores for Adult_no_dim_reduction: [Errno 2] No such file or directory: './results/adult/no_dim_reduction/predictive_quality/predictive_quality_metrics.csv'\n",
      "no_iterative_filter\n",
      "taxi\n",
      "full\n",
      "no_variance_filter\n",
      "Could not load F1 scores for Taxi_no_variance_filter: [Errno 2] No such file or directory: './results/taxi/no_variance_filter/predictive_quality/predictive_quality_metrics.csv'\n",
      "no_dim_reduction\n",
      "no_iterative_filter\n",
      "wine\n",
      "full\n",
      "no_variance_filter\n",
      "no_dim_reduction\n",
      "no_iterative_filter\n",
      "\n",
      "Results Table for Paper (sorted by Pipeline):\n",
      "================================================================================================================================================================\n",
      "Pipeline Ablation  N_Clusters Metrics_avg SiS_avg SiS_min SiS_max DBI_avg Ratio_avg Ratio_min Ratio_max F1_avg QSE_avg\n",
      "     Inc     FULL           4           5  0.9216  0.8153  0.9656  0.1395    0.0271    0.0273    0.0153      0  0.9265\n",
      "     Inc      NDR           2         N/A  0.0106 -0.7027  0.3027  0.9382       N/A       N/A       N/A      0  0.8867\n",
      "     Inc      NIS           4           9  0.9216  0.8153  0.9656  0.1395    0.2283    0.0899    0.3055      0  0.9265\n",
      "     Inc      NVF           9           5  0.6356  0.0000  0.8039  0.4704    0.0652    0.0304    0.1089      0  0.6990\n",
      "    Taxi     FULL           4           3  0.8044  0.5723  0.9742  0.2305    0.2655    0.0099    0.2975      0  0.8613\n",
      "    Taxi      NDR           4           2  0.3974 -0.2092  0.8618  1.0542    0.0903    0.0084    0.1069      0  0.8465\n",
      "    Taxi      NIS           4           9  0.8044  0.5723  0.9742  0.2305    0.3622    0.1652    0.3284      0  0.8613\n",
      "    Taxi      NVF          17           3  0.6436 -0.2423  0.9645  0.3998    0.1521    0.0000    0.1252      0  0.7679\n",
      "    Wine     FULL           3           6  0.9078  0.7700  0.9487  0.1319    0.4587    0.3577    0.5293      0  0.9512\n",
      "    Wine      NDR           2         N/A  0.0298 -0.2279  0.2256  8.0719       N/A       N/A       N/A      0  0.6407\n",
      "    Wine      NIS           3          10  0.9078  0.7700  0.9487  0.1319    1.8436    2.3536    1.4172      0  0.9512\n",
      "    Wine      NVF           3           9  0.9128  0.7971  0.9478  0.1405    0.3222    0.1013    0.4392      0  0.9512\n",
      "================================================================================================================================================================\n",
      "\n",
      "Table saved to: ./results_table_for_paper.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/qdpty4297ln507zdcjn9040h0000gn/T/ipykernel_95012/2325361582.py:226: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '\\blue{4}' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  csv_df.loc[idx, col] = f\"\\\\blue{{{current_val_str}}}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "# Define the datasets and ablation types\n",
    "datasets = ['adult', 'taxi', 'wine']\n",
    "pipeline = ['Inc', 'Taxi', 'Wine']\n",
    "ablation_types = ['full', 'no_variance_filter', 'no_dim_reduction', 'no_iterative_filter']\n",
    "\n",
    "# Create mapping from dataset to pipeline name\n",
    "dataset_to_pipeline = {dataset: pipe for dataset, pipe in zip(datasets, pipeline)}\n",
    "\n",
    "# Create mapping for ablation names\n",
    "ablation_name_map = {\n",
    "    'full': 'FULL',\n",
    "    'no_dim_reduction': 'NDR',\n",
    "    'no_iterative_filter': 'NIS',\n",
    "    'no_variance_filter': 'NVF'\n",
    "}\n",
    " \n",
    "# Create results dictionary\n",
    "results = {}\n",
    " \n",
    "base_path = \"./results\"\n",
    " \n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    for ablation in ablation_types:\n",
    "        print(ablation)\n",
    " \n",
    "        key = f\"{dataset.capitalize()}_{ablation}\"\n",
    "        \n",
    "        try:\n",
    "            # Read Silhouette and DBI\n",
    "            cluster_quality_path = f\"{base_path}/{dataset}/{ablation}/cluster_quality/overall_cluster_quality_scores.csv\"\n",
    "            cluster_df = pd.read_csv(cluster_quality_path)\n",
    "            silhouette_avg = cluster_df['Silhouette_Score'].values[0]\n",
    "            dbi_avg = cluster_df['Davies_Bouldin_Index'].values[0]\n",
    "            \n",
    "            # Read min and max silhouette from per_cluster_silhouette_metrics.csv\n",
    "            per_cluster_path = f\"{base_path}/{dataset}/{ablation}/cluster_quality/per_cluster_silhouette_metrics.csv\"\n",
    "            try:\n",
    "                per_cluster_df = pd.read_csv(per_cluster_path)\n",
    "                n_clusters = len(per_cluster_df)\n",
    "                silhouette_min = per_cluster_df['silhouette_min'].min()\n",
    "                silhouette_max = per_cluster_df['silhouette_max'].max()\n",
    "            except:\n",
    "                n_clusters = None\n",
    "                silhouette_min = None\n",
    "                silhouette_max = None\n",
    "            \n",
    "            # Read representative quality detailed for CV metrics\n",
    "            rep_quality_path = f\"{base_path}/{dataset}/{ablation}/representative_quality/representative_quality_detailed.csv\"\n",
    "            try:\n",
    "                rep_df = pd.read_csv(rep_quality_path)\n",
    "                \n",
    "                # Average these counts across clusters\n",
    "                selected_cvs = rep_df[rep_df['Type'] == 'Selected'].groupby('Cluster')['CV'].sum()\n",
    "                non_selected_cvs = rep_df[rep_df['Type'] == 'Non-Selected'].groupby('Cluster')['CV'].sum()\n",
    "\n",
    "                # Metrics: average number of selected metrics per cluster\n",
    "                metrics_avg =  rep_df[rep_df['Type'] == 'Selected'].groupby('Cluster').count()[\"Metric\"].mean()\n",
    "\n",
    "                # Get CV statistics for selected\n",
    "                selected_cv_avg = selected_cvs.mean()\n",
    "                selected_cv_min = selected_cvs.min()\n",
    "                selected_cv_max = selected_cvs.max()\n",
    "                \n",
    "                # Get CV statistics for non-selected\n",
    "                non_selected_cv_avg = non_selected_cvs.mean()\n",
    "                non_selected_cv_min = non_selected_cvs.min()\n",
    "                non_selected_cv_max = non_selected_cvs.max()\n",
    "                \n",
    "                # Compute ratio of selected vs non-selected CV\n",
    "                ratio_avg = selected_cv_avg / non_selected_cv_avg if non_selected_cv_avg != 0 else None\n",
    "                ratio_min = selected_cv_min / non_selected_cv_min if non_selected_cv_min != 0 else None\n",
    "                ratio_max = selected_cv_max / non_selected_cv_max if non_selected_cv_max != 0 else None\n",
    "                \n",
    "                # Handle NaN/Inf cases\n",
    "                if ratio_avg is not None and (pd.isna(ratio_avg) or np.isinf(ratio_avg)):\n",
    "                    ratio_avg = None\n",
    "                if ratio_min is not None and (pd.isna(ratio_min) or np.isinf(ratio_min)):\n",
    "                    ratio_min = None\n",
    "                if ratio_max is not None and (pd.isna(ratio_max) or np.isinf(ratio_max)):\n",
    "                    ratio_max = None\n",
    "                    \n",
    "                # Handle metrics_avg NaN\n",
    "                try:\n",
    "                    if metrics_avg != metrics_avg:  # NaN check\n",
    "                        metrics_avg = None\n",
    "                except:\n",
    "                    metrics_avg = None\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load representative quality for {key}: {e}\")\n",
    "                ratio_avg = None\n",
    "                ratio_min = None\n",
    "                ratio_max = None\n",
    "                metrics_avg = None\n",
    "            \n",
    "            # Read QSE - try multiple possible filenames\n",
    "            qse_path = None\n",
    "            for filename in ['qse_metrics.csv', 'qse_best_per_cluster.csv', 'qse_all_rules.csv']:\n",
    "                test_path = f\"{base_path}/{dataset}/{ablation}/explanation_quality/{filename}\"\n",
    "                try:\n",
    "                    qse_df = pd.read_csv(test_path)\n",
    "                    qse_path = test_path\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if qse_path:\n",
    "                qse_df = pd.read_csv(qse_path)\n",
    "                if 'qse' in qse_df.columns:\n",
    "                    qse_avg = qse_df['qse'].mean()\n",
    "                elif 'QSE' in qse_df.columns:\n",
    "                    qse_avg = qse_df['QSE'].mean()\n",
    "                else:\n",
    "                    # Try the first numeric column\n",
    "                    numeric_cols = qse_df.select_dtypes(include=[np.number]).columns\n",
    "                    qse_avg = qse_df[numeric_cols[0]].mean() if len(numeric_cols) > 0 else None\n",
    "            else:\n",
    "                qse_avg = None\n",
    "            \n",
    "            # Read F1 Score from predictive quality metrics\n",
    "            f1_path = f\"{base_path}/{dataset}/{ablation}/predictive_quality/predictive_quality_metrics.csv\"\n",
    "            try:\n",
    "                f1_df = pd.read_csv(f1_path)\n",
    "                if 'F1 Score' in f1_df.columns:\n",
    "                    f1_scores = f1_df['F1 Score'].values\n",
    "                    f1_avg = f1_scores.mean()\n",
    "                    # Check if all zeros\n",
    "                    if np.all(f1_scores == 0):\n",
    "                        f1_str = \"0\"\n",
    "                    else:\n",
    "                        f1_str = f\"{f1_avg:.4f}\"\n",
    "                        f1_str = \"0\"\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load F1 scores for {key}: {e}\")\n",
    "                f1_str = \"0\"\n",
    "            \n",
    "            results[key] = {\n",
    "                'Pipeline': dataset_to_pipeline[dataset],\n",
    "                'Ablation': ablation_name_map.get(ablation, ablation.replace('_', ' ').title()),\n",
    "                'N_Clusters': n_clusters if n_clusters is not None else \"N/A\",\n",
    "                'Metrics_avg': f\"{int(metrics_avg)}\" if metrics_avg is not None else \"N/A\",\n",
    "                'SiS_avg': f\"{silhouette_avg:.4f}\",\n",
    "                'SiS_min': f\"{silhouette_min:.4f}\" if silhouette_min is not None else \"N/A\",\n",
    "                'SiS_max': f\"{silhouette_max:.4f}\" if silhouette_max is not None else \"N/A\",\n",
    "                'DBI_avg': f\"{dbi_avg:.4f}\",\n",
    "                'Ratio_avg': f\"{ratio_avg:.4f}\" if ratio_avg is not None else \"N/A\",\n",
    "                'Ratio_min': f\"{ratio_min:.4f}\" if ratio_min is not None else \"N/A\",\n",
    "                'Ratio_max': f\"{ratio_max:.4f}\" if ratio_max is not None else \"N/A\",\n",
    "                'F1_avg': f1_str,\n",
    "                'QSE_avg': f\"{qse_avg:.4f}\" if qse_avg is not None else \"N/A\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {key}: {e}\")\n",
    " \n",
    "# Create dataframe from results and sort by Pipeline then Ablation\n",
    "results_df = pd.DataFrame(list(results.values()))\n",
    "results_df = results_df.sort_values(by=['Pipeline', 'Ablation']).reset_index(drop=True)\n",
    "\n",
    "# Display the table\n",
    "print(\"\\nResults Table for Paper (sorted by Pipeline):\")\n",
    "print(\"=\" * 160)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\" * 160)\n",
    "\n",
    "# Create a version with bolded winners for CSV\n",
    "csv_df = results_df.copy()\n",
    "\n",
    "# Define which columns are \"higher is better\" and \"lower is better\"\n",
    "higher_is_better = ['SiS_avg', 'SiS_min', 'SiS_max',  'F1_avg', 'QSE_avg']  # These want max\n",
    "lower_is_better = ['DBI_avg', 'Ratio_avg', 'Ratio_min', 'Ratio_max']  # These want min\n",
    "\n",
    "# Bold the winners in each column, per dataset\n",
    "for col in csv_df.columns:\n",
    "    if col in ['Pipeline', 'Ablation', 'N_Clusters', 'Metrics_avg']:\n",
    "        continue  # Skip non-numeric or non-comparative columns\n",
    "    \n",
    "    try:\n",
    "        # For each pipeline, bold the best value\n",
    "        for pipeline_name in csv_df['Pipeline'].unique():\n",
    "            pipeline_indices = csv_df[csv_df['Pipeline'] == pipeline_name].index\n",
    "            \n",
    "            # Convert strings to floats for comparison within this pipeline\n",
    "            numeric_values = pd.to_numeric(csv_df.loc[pipeline_indices, col], errors='coerce')\n",
    "            \n",
    "            if numeric_values.isna().all():\n",
    "                continue  # Skip if all N/A\n",
    "            \n",
    "            if col in higher_is_better:\n",
    "                # Bold the maximum value(s) in this pipeline\n",
    "                max_val = numeric_values.max()\n",
    "                if pd.notna(max_val):\n",
    "                    idx = numeric_values[numeric_values == max_val].index[0]\n",
    "                    current_val = csv_df.loc[idx, col]\n",
    "                    if not current_val.startswith('\\\\textbf'):\n",
    "                        csv_df.loc[idx, col] = f\"\\\\textbf{{{current_val}}}\"\n",
    "            \n",
    "            elif col in lower_is_better:\n",
    "                # Bold the minimum value(s) in this pipeline\n",
    "                min_val = numeric_values.min()\n",
    "                if pd.notna(min_val):\n",
    "                    idx = numeric_values[numeric_values == min_val].index[0]\n",
    "                    current_val = csv_df.loc[idx, col]\n",
    "                    if not current_val.startswith('\\\\textbf'):\n",
    "                        csv_df.loc[idx, col] = f\"\\\\textbf{{{current_val}}}\"\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Color the \"Full\" ablation rows in blue (entire row)\n",
    "full_indices = csv_df[csv_df['Ablation'] == 'FULL'].index\n",
    "for idx in full_indices:\n",
    "    for col in csv_df.columns:\n",
    "        try:\n",
    "            current_val = csv_df.loc[idx, col]\n",
    "            # Convert to string to safely call startswith\n",
    "            current_val_str = str(current_val)\n",
    "            if not current_val_str.startswith('\\\\blue'):\n",
    "                # If already bolded, wrap the entire textbf with blue, otherwise just wrap value\n",
    "                if current_val_str.startswith('\\\\textbf'):\n",
    "                    csv_df.loc[idx, col] = f\"\\\\blue{{{current_val_str}}}\"\n",
    "                else:\n",
    "                    csv_df.loc[idx, col] = f\"\\\\blue{{{current_val_str}}}\"\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# Save to CSV\n",
    "csv_df.to_csv(\"./results_table_for_paper.csv\", index=False)\n",
    "print(\"\\nTable saved to: ./results_table_for_paper.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff86ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e2199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xxp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
