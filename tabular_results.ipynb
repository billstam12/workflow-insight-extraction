{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9463ef71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult\n",
      "full\n",
      "no_variance_filter\n",
      "Could not load F1 scores for Adult_no_variance_filter: [Errno 2] No such file or directory: './results/adult/no_variance_filter/predictive_quality/predictive_quality_metrics.csv'\n",
      "no_dim_reduction\n",
      "Could not load F1 scores for Adult_no_dim_reduction: [Errno 2] No such file or directory: './results/adult/no_dim_reduction/predictive_quality/predictive_quality_metrics.csv'\n",
      "no_iterative_filter\n",
      "taxi\n",
      "full\n",
      "no_variance_filter\n",
      "Could not load F1 scores for Taxi_no_variance_filter: [Errno 2] No such file or directory: './results/taxi/no_variance_filter/predictive_quality/predictive_quality_metrics.csv'\n",
      "no_dim_reduction\n",
      "no_iterative_filter\n",
      "wine\n",
      "full\n",
      "no_variance_filter\n",
      "no_dim_reduction\n",
      "no_iterative_filter\n",
      "\n",
      "Results Table for Paper (sorted by Dataset):\n",
      "================================================================================================================================================================\n",
      "Dataset            Ablation  N_Clusters SiS_avg SiS_min SiS_max DBI_avg CV_avg CV_min CV_max F1_avg QSE_avg\n",
      "  Adult                Full           4  0.9216  0.8153  0.9656  0.1395 0.1412 0.0423 0.2043 1.0000  0.9265\n",
      "  Adult    No Dim Reduction           2  0.0106 -0.7027  0.3027  0.9382    N/A    N/A    N/A    N/A  0.8867\n",
      "  Adult No Iterative Filter           4  0.9216  0.8153  0.9656  0.1395 0.9936 0.1211 3.1375 1.0000  0.9265\n",
      "  Adult  No Variance Filter           9  0.6356  0.0000  0.8039  0.4704 0.3832 0.0429 1.5787    N/A  0.6990\n",
      "   Taxi                Full           4  0.8044  0.5723  0.9742  0.2305 1.0013 0.0113 2.2014 0.9722  0.8613\n",
      "   Taxi    No Dim Reduction           4  0.3974 -0.2092  0.8618  1.0542 0.4013 0.0113 0.7913 0.9963  0.8465\n",
      "   Taxi No Iterative Filter           4  0.8044  0.5723  0.9742  0.2305 1.2882 0.1876 2.2074 0.9722  0.8613\n",
      "   Taxi  No Variance Filter          17  0.6436 -0.2423  0.9645  0.3998 0.9500 0.0001 2.3599    N/A  0.7679\n",
      "   Wine                Full           3  0.9078  0.7700  0.9487  0.1319 1.4293 0.9431 1.9155 1.0000  0.9512\n",
      "   Wine    No Dim Reduction           2  0.0298 -0.2279  0.2256  8.0719    N/A    N/A    N/A 0.5178  0.6407\n",
      "   Wine No Iterative Filter           3  0.9078  0.7700  0.9487  0.1319 3.3654 2.9944 3.7364 1.0000  0.9512\n",
      "   Wine  No Variance Filter           3  0.9128  0.7971  0.9478  0.1405 1.0968 0.2721 1.9215 1.0000  0.9512\n",
      "================================================================================================================================================================\n",
      "\n",
      "Table saved to: ./results_table_for_paper.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "# Define the datasets and ablation types\n",
    "datasets = ['adult', 'taxi', 'wine']\n",
    "ablation_types = ['full', 'no_variance_filter', 'no_dim_reduction', 'no_iterative_filter']\n",
    " \n",
    "# Create results dictionary\n",
    "results = {}\n",
    " \n",
    "base_path = \"./results\"\n",
    " \n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    for ablation in ablation_types:\n",
    "        print(ablation)\n",
    " \n",
    "        key = f\"{dataset.capitalize()}_{ablation}\"\n",
    "        \n",
    "        try:\n",
    "            # Read Silhouette and DBI\n",
    "            cluster_quality_path = f\"{base_path}/{dataset}/{ablation}/cluster_quality/overall_cluster_quality_scores.csv\"\n",
    "            cluster_df = pd.read_csv(cluster_quality_path)\n",
    "            silhouette_avg = cluster_df['Silhouette_Score'].values[0]\n",
    "            dbi_avg = cluster_df['Davies_Bouldin_Index'].values[0]\n",
    "            \n",
    "            # Read min and max silhouette from per_cluster_silhouette_metrics.csv\n",
    "            per_cluster_path = f\"{base_path}/{dataset}/{ablation}/cluster_quality/per_cluster_silhouette_metrics.csv\"\n",
    "            try:\n",
    "                per_cluster_df = pd.read_csv(per_cluster_path)\n",
    "                n_clusters = len(per_cluster_df)\n",
    "                silhouette_min = per_cluster_df['silhouette_min'].min()\n",
    "                silhouette_max = per_cluster_df['silhouette_max'].max()\n",
    "            except:\n",
    "                n_clusters = None\n",
    "                silhouette_min = None\n",
    "                silhouette_max = None\n",
    "            \n",
    "            # Read representative quality detailed for CV metrics\n",
    "            rep_quality_path = f\"{base_path}/{dataset}/{ablation}/representative_quality/representative_quality_detailed.csv\"\n",
    "            try:\n",
    "                rep_df = pd.read_csv(rep_quality_path)\n",
    "                \n",
    "                # Average these counts across clusters\n",
    "                selected_cvs = rep_df[rep_df['Type'] == 'Selected'].groupby('Cluster')['CV'].sum()\n",
    "                non_selected_cvs = rep_df[rep_df['Type'] == 'Non-Selected'].groupby('Cluster')['CV'].sum()\n",
    "\n",
    "                # Get CV statistics (all CV values across all clusters)\n",
    "                cv_avg = selected_cvs.mean()\n",
    "                cv_min = selected_cvs.min()\n",
    "                cv_max = selected_cvs.max()\n",
    "                \n",
    "                n_selected_str = f\"{selected_cvs.mean():.1f}\"\n",
    "                n_non_selected_str = f\"{non_selected_cvs.mean():.1f}\"\n",
    "                if(n_selected_str == \"nan\" or n_non_selected_str == \"nan\"):\n",
    "                    n_selected_str = \"N/A\"\n",
    "                    n_non_selected_str = \"N/A\"\n",
    "                    cv_avg = None\n",
    "                    cv_min = None\n",
    "                    cv_max = None\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load representative quality for {key}: {e}\")\n",
    "                n_selected_str = \"N/A\"\n",
    "                n_non_selected_str = \"N/A\"\n",
    "                cv_avg = None\n",
    "                cv_min = None\n",
    "                cv_max = None\n",
    "            \n",
    "            # Read QSE - try multiple possible filenames\n",
    "            qse_path = None\n",
    "            for filename in ['qse_metrics.csv', 'qse_best_per_cluster.csv', 'qse_all_rules.csv']:\n",
    "                test_path = f\"{base_path}/{dataset}/{ablation}/explanation_quality/{filename}\"\n",
    "                try:\n",
    "                    qse_df = pd.read_csv(test_path)\n",
    "                    qse_path = test_path\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if qse_path:\n",
    "                qse_df = pd.read_csv(qse_path)\n",
    "                if 'qse' in qse_df.columns:\n",
    "                    qse_avg = qse_df['qse'].mean()\n",
    "                elif 'QSE' in qse_df.columns:\n",
    "                    qse_avg = qse_df['QSE'].mean()\n",
    "                else:\n",
    "                    # Try the first numeric column\n",
    "                    numeric_cols = qse_df.select_dtypes(include=[np.number]).columns\n",
    "                    qse_avg = qse_df[numeric_cols[0]].mean() if len(numeric_cols) > 0 else None\n",
    "            else:\n",
    "                qse_avg = None\n",
    "            \n",
    "            # Read F1 Score from predictive quality metrics\n",
    "            f1_path = f\"{base_path}/{dataset}/{ablation}/predictive_quality/predictive_quality_metrics.csv\"\n",
    "            try:\n",
    "                f1_df = pd.read_csv(f1_path)\n",
    "                if 'F1 Score' in f1_df.columns:\n",
    "                    f1_scores = f1_df['F1 Score'].values\n",
    "                    f1_avg = f1_scores.mean()\n",
    "                    # Check if all zeros\n",
    "                    if np.all(f1_scores == 0):\n",
    "                        f1_str = \"0.0000\"\n",
    "                    else:\n",
    "                        f1_str = f\"{f1_avg:.4f}\"\n",
    "                else:\n",
    "                    f1_str = \"N/A\"\n",
    "            except Exception as e:\n",
    "                print(f\"Could not load F1 scores for {key}: {e}\")\n",
    "                f1_str = \"N/A\"\n",
    "            \n",
    "            results[key] = {\n",
    "                'Dataset': dataset.capitalize(),\n",
    "                'Ablation': ablation.replace('_', ' ').title(),\n",
    "                'N_Clusters': n_clusters if n_clusters is not None else \"N/A\",\n",
    "                'SiS_avg': f\"{silhouette_avg:.4f}\",\n",
    "                'SiS_min': f\"{silhouette_min:.4f}\" if silhouette_min is not None else \"N/A\",\n",
    "                'SiS_max': f\"{silhouette_max:.4f}\" if silhouette_max is not None else \"N/A\",\n",
    "                'DBI_avg': f\"{dbi_avg:.4f}\",\n",
    "                'CV_avg': f\"{cv_avg:.4f}\" if cv_avg is not None else \"N/A\",\n",
    "                'CV_min': f\"{cv_min:.4f}\" if cv_min is not None else \"N/A\",\n",
    "                'CV_max': f\"{cv_max:.4f}\" if cv_max is not None else \"N/A\",\n",
    "                'F1_avg': f1_str,\n",
    "                'QSE_avg': f\"{qse_avg:.4f}\" if qse_avg is not None else \"N/A\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {key}: {e}\")\n",
    " \n",
    "# Create dataframe from results and sort by Dataset then Ablation\n",
    "results_df = pd.DataFrame(list(results.values()))\n",
    "results_df = results_df.sort_values(by=['Dataset', 'Ablation']).reset_index(drop=True)\n",
    "\n",
    "# Display the table\n",
    "print(\"\\nResults Table for Paper (sorted by Dataset):\")\n",
    "print(\"=\" * 160)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\" * 160)\n",
    "\n",
    "# Create a version with bolded winners for CSV\n",
    "csv_df = results_df.copy()\n",
    "\n",
    "# Define which columns are \"higher is better\" and \"lower is better\"\n",
    "higher_is_better = ['SiS_avg', 'SiS_min', 'SiS_max', 'F1_avg', 'QSE_avg']  # These want max\n",
    "lower_is_better = ['DBI_avg', 'CV_avg', 'CV_min', 'CV_max']  # These want min\n",
    "\n",
    "# Bold the winners in each column, per dataset\n",
    "for col in csv_df.columns:\n",
    "    if col in ['Dataset', 'Ablation', 'N_Clusters']:\n",
    "        continue  # Skip non-numeric columns\n",
    "    \n",
    "    try:\n",
    "        # For each dataset, bold the best value\n",
    "        for dataset in csv_df['Dataset'].unique():\n",
    "            dataset_indices = csv_df[csv_df['Dataset'] == dataset].index\n",
    "            \n",
    "            # Convert strings to floats for comparison within this dataset\n",
    "            numeric_values = pd.to_numeric(csv_df.loc[dataset_indices, col], errors='coerce')\n",
    "            \n",
    "            if numeric_values.isna().all():\n",
    "                continue  # Skip if all N/A\n",
    "            \n",
    "            if col in higher_is_better:\n",
    "                # Bold the maximum value(s) in this dataset\n",
    "                max_val = numeric_values.max()\n",
    "                if pd.notna(max_val):\n",
    "                    idx = numeric_values[numeric_values == max_val].index[0]\n",
    "                    current_val = csv_df.loc[idx, col]\n",
    "                    if not current_val.startswith('\\\\textbf'):\n",
    "                        csv_df.loc[idx, col] = f\"\\\\textbf{{{current_val}}}\"\n",
    "            \n",
    "            elif col in lower_is_better:\n",
    "                # Bold the minimum value(s) in this dataset\n",
    "                min_val = numeric_values.min()\n",
    "                if pd.notna(min_val):\n",
    "                    idx = numeric_values[numeric_values == min_val].index[0]\n",
    "                    current_val = csv_df.loc[idx, col]\n",
    "                    if not current_val.startswith('\\\\textbf'):\n",
    "                        csv_df.loc[idx, col] = f\"\\\\textbf{{{current_val}}}\"\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Save to CSV\n",
    "csv_df.to_csv(\"./results_table_for_paper.csv\", index=False)\n",
    "print(\"\\nTable saved to: ./results_table_for_paper.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff86ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xxp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
