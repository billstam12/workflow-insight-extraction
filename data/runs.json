[{"id":"cf297299ee394608a3c9feaf24d7dad7","name":"Run_107","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499259244,"endTime":1757499265049,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.5724693740422665,"timestamp":1757499264088,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5645790022796775,"timestamp":1757499264088,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.28567308841103173,"timestamp":1757499264088,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757499264536,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.5213845225422628,"timestamp":1757499264088,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.942529411764706,"timestamp":1757499264867,"step":340000,"task":null},{"name":"Recall","value":0.4481707317073171,"timestamp":1757499263936,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30235.8,"timestamp":1757499264536,"step":4,"task":null},{"name":"Test Fairness","value":0.2433414043583535,"timestamp":1757499264088,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499264536,"step":4,"task":null},{"name":"Statistical Parity","value":0.8440030765054517,"timestamp":1757499264088,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.4526984126984127,"timestamp":1757499264088,"step":0,"task":null},{"name":"AUC-ROC","value":0.7912622366019578,"timestamp":1757499263936,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7931764705882354,"timestamp":1757499264958,"step":340000,"task":null},{"name":"Well Calibration","value":0.656846602771707,"timestamp":1757499264088,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3977364935483471,"timestamp":1757499263936,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":8.620175000018207,"timestamp":1757499264536,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.5582633167468799,"timestamp":1757499264088,"step":0,"task":null},{"name":"Cohen Kappa","value":0.38384405436737234,"timestamp":1757499263936,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3005366726296959,"timestamp":1757499264088,"step":0,"task":null},{"name":"Equalized Odds","value":0.28567308841103173,"timestamp":1757499264088,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.3504864375173977,"timestamp":1757499264088,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499264536,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499264536,"step":4,"task":null},{"name":"Accuracy","value":0.7610536218250236,"timestamp":1757499263936,"step":0,"task":null},{"name":"F1 Score","value":0.5364963503649635,"timestamp":1757499263936,"step":0,"task":null},{"name":"Log Loss","value":0.49375581819216213,"timestamp":1757499263936,"step":0,"task":null},{"name":"Treatment Equality","value":0.8958972783668204,"timestamp":1757499264088,"step":0,"task":null},{"name":"Precision","value":0.6681818181818182,"timestamp":1757499263936,"step":0,"task":null},{"name":"Disparate Impact","value":0.4526984126984127,"timestamp":1757499264088,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":9.9,"timestamp":1757499264536,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":7.4178649999958,"timestamp":1757499264536,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf297299ee394608a3c9feaf24d7dad7/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_107","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"cf297299ee394608a3c9feaf24d7dad7\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:14:20.924581\", \"model_uuid\": \"3671bad2424040919118c6df3168073e\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"0b8deb1e724c42a49240c92ae7948a9c","name":"Run_106","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499241618,"endTime":1757499259205,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.32311627793662645,"timestamp":1757499258277,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40535320742836445,"timestamp":1757499258277,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8713207093528363,"timestamp":1757499258277,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757499258761,"step":13,"task":null},{"name":"Balance for Negative Class","value":0.9921722896039176,"timestamp":1757499258277,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9636470588235294,"timestamp":1757499259060,"step":340000,"task":null},{"name":"Recall","value":0.6468401486988847,"timestamp":1757499258135,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30235.6,"timestamp":1757499258761,"step":13,"task":null},{"name":"Test Fairness","value":0.3023255813953488,"timestamp":1757499258277,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499258761,"step":13,"task":null},{"name":"Statistical Parity","value":0.8056656458295802,"timestamp":1757499258277,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3360517002615787,"timestamp":1757499258277,"step":0,"task":null},{"name":"AUC-ROC","value":0.919418875769011,"timestamp":1757499258135,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.852,"timestamp":1757499259130,"step":340000,"task":null},{"name":"Well Calibration","value":0.3986123151018957,"timestamp":1757499258277,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6224378135171351,"timestamp":1757499258135,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":6.702817999990657,"timestamp":1757499258761,"step":13,"task":null},{"name":"Equal Negative Predictive Value","value":0.4137953099283335,"timestamp":1757499258277,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6180494465328699,"timestamp":1757499258135,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9494851994851996,"timestamp":1757499258277,"step":0,"task":null},{"name":"Equalized Odds","value":0.8713207093528363,"timestamp":1757499258277,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.15659722643006874,"timestamp":1757499258277,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499258761,"step":13,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499258761,"step":13,"task":null},{"name":"Accuracy","value":0.863593603010348,"timestamp":1757499258135,"step":0,"task":null},{"name":"F1 Score","value":0.7058823529411765,"timestamp":1757499258135,"step":0,"task":null},{"name":"Log Loss","value":0.305327127430211,"timestamp":1757499258135,"step":0,"task":null},{"name":"Treatment Equality","value":0.2951816658022374,"timestamp":1757499258277,"step":0,"task":null},{"name":"Precision","value":0.7767857142857143,"timestamp":1757499258135,"step":0,"task":null},{"name":"Disparate Impact","value":0.3360517002615787,"timestamp":1757499258277,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":7.6,"timestamp":1757499258761,"step":13,"task":null},{"name":"system_network_receive_megabytes","value":6.7021319999767,"timestamp":1757499258761,"step":13,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0b8deb1e724c42a49240c92ae7948a9c/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_106","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"0b8deb1e724c42a49240c92ae7948a9c\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:14:14.736984\", \"model_uuid\": \"c1abdd2f9c214b15ba1185ffcd91066e\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"3ac639c41b884b58acbc9d41a9980496","name":"Run_105","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499235308,"endTime":1757499241564,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.33029663966855144,"timestamp":1757499240496,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.4054106445917069,"timestamp":1757499240496,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.9032179032783226,"timestamp":1757499240496,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":30.0,"timestamp":1757499240610,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.9591137509004196,"timestamp":1757499240496,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.964764705882353,"timestamp":1757499241388,"step":340000,"task":null},{"name":"Recall","value":0.654275092936803,"timestamp":1757499240344,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30378.5,"timestamp":1757499240610,"step":4,"task":null},{"name":"Test Fairness","value":0.3589743589743589,"timestamp":1757499240496,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499240610,"step":4,"task":null},{"name":"Statistical Parity","value":0.8083158617698092,"timestamp":1757499240496,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3472534236036313,"timestamp":1757499240496,"step":0,"task":null},{"name":"AUC-ROC","value":0.9182717968406168,"timestamp":1757499240344,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8505882352941176,"timestamp":1757499241475,"step":340000,"task":null},{"name":"Well Calibration","value":0.3991511536598711,"timestamp":1757499240496,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.630677156735474,"timestamp":1757499240344,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.083324000006542,"timestamp":1757499240610,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.41334506246445907,"timestamp":1757499240496,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6264378625591478,"timestamp":1757499240344,"step":0,"task":null},{"name":"Equal Opportunity","value":0.979386385426654,"timestamp":1757499240496,"step":0,"task":null},{"name":"Equalized Odds","value":0.9032179032783226,"timestamp":1757499240496,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.1551899240281613,"timestamp":1757499240496,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499240610,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499240610,"step":4,"task":null},{"name":"Accuracy","value":0.8664158043273753,"timestamp":1757499240344,"step":0,"task":null},{"name":"F1 Score","value":0.7125506072874493,"timestamp":1757499240344,"step":0,"task":null},{"name":"Log Loss","value":0.3359093947034844,"timestamp":1757499240344,"step":0,"task":null},{"name":"Treatment Equality","value":0.3177831986943243,"timestamp":1757499240496,"step":0,"task":null},{"name":"Precision","value":0.7822222222222223,"timestamp":1757499240344,"step":0,"task":null},{"name":"Disparate Impact","value":0.3472534236036313,"timestamp":1757499240496,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":15.8,"timestamp":1757499240610,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.082663999986835,"timestamp":1757499240610,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ac639c41b884b58acbc9d41a9980496/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_105","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"3ac639c41b884b58acbc9d41a9980496\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:13:56.932383\", \"model_uuid\": \"93574c88219c4369aaf738537b7ba02b\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"5661ae9b90974e2ab22ce42c4d8c150b","name":"Run_104","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499229215,"endTime":1757499235255,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.5612444843551634,"timestamp":1757499234336,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5655747324424282,"timestamp":1757499234336,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.2877180781860828,"timestamp":1757499234336,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":30.0,"timestamp":1757499234472,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.520196858481802,"timestamp":1757499234336,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9441176470588235,"timestamp":1757499235077,"step":340000,"task":null},{"name":"Recall","value":0.4481707317073171,"timestamp":1757499234192,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30366.7,"timestamp":1757499234472,"step":4,"task":null},{"name":"Test Fairness","value":0.243161094224924,"timestamp":1757499234336,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499234472,"step":4,"task":null},{"name":"Statistical Parity","value":0.8455865719585577,"timestamp":1757499234336,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.4552274541101356,"timestamp":1757499234336,"step":0,"task":null},{"name":"AUC-ROC","value":0.7914364526298324,"timestamp":1757499234192,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7948235294117648,"timestamp":1757499235169,"step":340000,"task":null},{"name":"Well Calibration","value":0.6583299924696931,"timestamp":1757499234336,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3999611176225641,"timestamp":1757499234192,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.357385000010254,"timestamp":1757499234472,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.5584900667643707,"timestamp":1757499234336,"step":0,"task":null},{"name":"Cohen Kappa","value":0.38569930812682596,"timestamp":1757499234192,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3005366726296959,"timestamp":1757499234336,"step":0,"task":null},{"name":"Equalized Odds","value":0.2877180781860828,"timestamp":1757499234336,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.3436141544288213,"timestamp":1757499234336,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499234472,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499234472,"step":4,"task":null},{"name":"Accuracy","value":0.761994355597366,"timestamp":1757499234192,"step":0,"task":null},{"name":"F1 Score","value":0.5374771480804388,"timestamp":1757499234192,"step":0,"task":null},{"name":"Log Loss","value":0.4937397040467982,"timestamp":1757499234192,"step":0,"task":null},{"name":"Treatment Equality","value":0.9138152239341568,"timestamp":1757499234336,"step":0,"task":null},{"name":"Precision","value":0.6712328767123288,"timestamp":1757499234192,"step":0,"task":null},{"name":"Disparate Impact","value":0.4552274541101356,"timestamp":1757499234336,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":15.8,"timestamp":1757499234472,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.22218700000667,"timestamp":1757499234472,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5661ae9b90974e2ab22ce42c4d8c150b/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_104","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"5661ae9b90974e2ab22ce42c4d8c150b\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:13:50.907795\", \"model_uuid\": \"8af987444e8c42e890632a88206c08ac\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"de03f86f9ec147cd8ce9f70a89efc42c","name":"Run_103","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499210514,"endTime":1757499229161,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.32311627793662645,"timestamp":1757499228253,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40535320742836445,"timestamp":1757499228253,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8713207093528363,"timestamp":1757499228253,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757499228969,"step":14,"task":null},{"name":"Balance for Negative Class","value":0.9921722896039176,"timestamp":1757499228253,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9641176470588236,"timestamp":1757499228984,"step":340000,"task":null},{"name":"Recall","value":0.6468401486988847,"timestamp":1757499228106,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30189.5,"timestamp":1757499228969,"step":14,"task":null},{"name":"Test Fairness","value":0.30952380952380953,"timestamp":1757499228253,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499228969,"step":14,"task":null},{"name":"Statistical Parity","value":0.8056656458295802,"timestamp":1757499228253,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3360517002615787,"timestamp":1757499228253,"step":0,"task":null},{"name":"AUC-ROC","value":0.919461013362299,"timestamp":1757499228106,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8501176470588236,"timestamp":1757499229071,"step":340000,"task":null},{"name":"Well Calibration","value":0.3994312208960823,"timestamp":1757499228253,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6224378135171351,"timestamp":1757499228106,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":14.480101000022842,"timestamp":1757499228969,"step":14,"task":null},{"name":"Equal Negative Predictive Value","value":0.4137953099283335,"timestamp":1757499228253,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6180494465328699,"timestamp":1757499228106,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9494851994851996,"timestamp":1757499228253,"step":0,"task":null},{"name":"Equalized Odds","value":0.8713207093528363,"timestamp":1757499228253,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.15659722643006874,"timestamp":1757499228253,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.9,"timestamp":1757499228969,"step":14,"task":null},{"name":"system_disk_available_megabytes","value":17.4,"timestamp":1757499228969,"step":14,"task":null},{"name":"Accuracy","value":0.863593603010348,"timestamp":1757499228106,"step":0,"task":null},{"name":"F1 Score","value":0.7058823529411765,"timestamp":1757499228106,"step":0,"task":null},{"name":"Log Loss","value":0.3052737454084053,"timestamp":1757499228106,"step":0,"task":null},{"name":"Treatment Equality","value":0.2951816658022374,"timestamp":1757499228253,"step":0,"task":null},{"name":"Precision","value":0.7767857142857143,"timestamp":1757499228106,"step":0,"task":null},{"name":"Disparate Impact","value":0.3360517002615787,"timestamp":1757499228253,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":5.2,"timestamp":1757499228969,"step":14,"task":null},{"name":"system_network_receive_megabytes","value":14.189615999988746,"timestamp":1757499228969,"step":14,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/de03f86f9ec147cd8ce9f70a89efc42c/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_103","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"de03f86f9ec147cd8ce9f70a89efc42c\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:13:45.209544\", \"model_uuid\": \"ac8147ad1dbc4fa6875539626866543c\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"5fa352be7a8c4eb6834f535a003f4914","name":"Run_102","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499204361,"endTime":1757499210460,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.33780338147920036,"timestamp":1757499209418,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.4066976625110456,"timestamp":1757499209418,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8753127253209002,"timestamp":1757499209418,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":30.0,"timestamp":1757499209639,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.9878301789273138,"timestamp":1757499209418,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9632941176470589,"timestamp":1757499210277,"step":340000,"task":null},{"name":"Recall","value":0.6468401486988847,"timestamp":1757499209270,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30360.6,"timestamp":1757499209639,"step":4,"task":null},{"name":"Test Fairness","value":0.33333333333333337,"timestamp":1757499209418,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499209639,"step":4,"task":null},{"name":"Statistical Parity","value":0.8086831201585299,"timestamp":1757499209418,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.33955223880597013,"timestamp":1757499209418,"step":0,"task":null},{"name":"AUC-ROC","value":0.9182858427050463,"timestamp":1757499209270,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8522352941176472,"timestamp":1757499210370,"step":340000,"task":null},{"name":"Well Calibration","value":0.40183750301725546,"timestamp":1757499209418,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6271855634063113,"timestamp":1757499209270,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.082841000024928,"timestamp":1757499209639,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.4140633911935815,"timestamp":1757499209418,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6223350733048942,"timestamp":1757499209270,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9494851994851996,"timestamp":1757499209418,"step":0,"task":null},{"name":"Equalized Odds","value":0.8753127253209002,"timestamp":1757499209418,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.16371528217689002,"timestamp":1757499209418,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499209639,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499209639,"step":4,"task":null},{"name":"Accuracy","value":0.8654750705550329,"timestamp":1757499209270,"step":0,"task":null},{"name":"F1 Score","value":0.7087576374745418,"timestamp":1757499209270,"step":0,"task":null},{"name":"Log Loss","value":0.3359270743723519,"timestamp":1757499209270,"step":0,"task":null},{"name":"Treatment Equality","value":0.3085990142477937,"timestamp":1757499209418,"step":0,"task":null},{"name":"Precision","value":0.7837837837837838,"timestamp":1757499209270,"step":0,"task":null},{"name":"Disparate Impact","value":0.33955223880597013,"timestamp":1757499209418,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":13.9,"timestamp":1757499209639,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.081372999993619,"timestamp":1757499209639,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fa352be7a8c4eb6834f535a003f4914/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_102","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"5fa352be7a8c4eb6834f535a003f4914\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:13:25.975922\", \"model_uuid\": \"e0098d663f1f465198a95fe65d9f700d\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"66b164f892f64e20983c0fbcf932ec7a","name":"Run_101","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499198622,"endTime":1757499204312,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.5724693740422665,"timestamp":1757499203597,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.55995130553968,"timestamp":1757499203597,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.3190660520365535,"timestamp":1757499203597,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":30.0,"timestamp":1757499203886,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.5337984397456499,"timestamp":1757499203597,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.943,"timestamp":1757499204140,"step":340000,"task":null},{"name":"Recall","value":0.45426829268292684,"timestamp":1757499203467,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30336.0,"timestamp":1757499203886,"step":4,"task":null},{"name":"Test Fairness","value":0.3039664378337147,"timestamp":1757499203597,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499203886,"step":4,"task":null},{"name":"Statistical Parity","value":0.8494836159632794,"timestamp":1757499203597,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.47533333333333333,"timestamp":1757499203597,"step":0,"task":null},{"name":"AUC-ROC","value":0.7886738841878215,"timestamp":1757499203467,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7917647058823529,"timestamp":1757499204226,"step":340000,"task":null},{"name":"Well Calibration","value":0.667665369556801,"timestamp":1757499203597,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40334021991386354,"timestamp":1757499203467,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.360866000002716,"timestamp":1757499203886,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.5546616179291581,"timestamp":1757499203597,"step":0,"task":null},{"name":"Cohen Kappa","value":0.38982911862383773,"timestamp":1757499203467,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3339296362552177,"timestamp":1757499203597,"step":0,"task":null},{"name":"Equalized Odds","value":0.3190660520365535,"timestamp":1757499203597,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.38942937501933084,"timestamp":1757499203597,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499203886,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499203886,"step":4,"task":null},{"name":"Accuracy","value":0.7629350893697083,"timestamp":1757499203467,"step":0,"task":null},{"name":"F1 Score","value":0.5418181818181819,"timestamp":1757499203467,"step":0,"task":null},{"name":"Log Loss","value":0.49742331582449434,"timestamp":1757499203467,"step":0,"task":null},{"name":"Treatment Equality","value":0.9172281659469828,"timestamp":1757499203597,"step":0,"task":null},{"name":"Precision","value":0.6711711711711712,"timestamp":1757499203467,"step":0,"task":null},{"name":"Disparate Impact","value":0.47533333333333333,"timestamp":1757499203597,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":12.1,"timestamp":1757499203886,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.2254959999991115,"timestamp":1757499203886,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/66b164f892f64e20983c0fbcf932ec7a/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_101","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"66b164f892f64e20983c0fbcf932ec7a\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:13:20.085881\", \"model_uuid\": \"5b9b168d55ce4c5c8235f17c2ad32cdf\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"7b8241a17d48485785e706a0435b0fd7","name":"Run_100","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499179674,"endTime":1757499198575,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.22750023650640025,"timestamp":1757499197822,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40212778758489753,"timestamp":1757499197822,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8852916594894074,"timestamp":1757499197822,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":30.0,"timestamp":1757499197543,"step":13,"task":null},{"name":"Balance for Negative Class","value":0.9597263792420276,"timestamp":1757499197822,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9626470588235294,"timestamp":1757499198399,"step":340000,"task":null},{"name":"Recall","value":0.6579925650557621,"timestamp":1757499197678,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30348.3,"timestamp":1757499197543,"step":13,"task":null},{"name":"Test Fairness","value":0.2702702702702703,"timestamp":1757499197822,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499197543,"step":13,"task":null},{"name":"Statistical Parity","value":0.7981219600072059,"timestamp":1757499197822,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3276081902047551,"timestamp":1757499197822,"step":0,"task":null},{"name":"AUC-ROC","value":0.9202452407929358,"timestamp":1757499197678,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8475294117647059,"timestamp":1757499198488,"step":340000,"task":null},{"name":"Well Calibration","value":0.38758622094459044,"timestamp":1757499197822,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6265777830729742,"timestamp":1757499197678,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":6.125593000004301,"timestamp":1757499197543,"step":13,"task":null},{"name":"Equal Negative Predictive Value","value":0.41352144042646627,"timestamp":1757499197822,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6231350265404804,"timestamp":1757499197678,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9728571428571429,"timestamp":1757499197822,"step":0,"task":null},{"name":"Equalized Odds","value":0.8852916594894074,"timestamp":1757499197822,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.10760840849096927,"timestamp":1757499197822,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499197543,"step":13,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499197543,"step":13,"task":null},{"name":"Accuracy","value":0.8645343367826905,"timestamp":1757499197678,"step":0,"task":null},{"name":"F1 Score","value":0.7108433734939759,"timestamp":1757499197678,"step":0,"task":null},{"name":"Log Loss","value":0.304470887161266,"timestamp":1757499197678,"step":0,"task":null},{"name":"Treatment Equality","value":0.21607511449682767,"timestamp":1757499197822,"step":0,"task":null},{"name":"Precision","value":0.7729257641921398,"timestamp":1757499197678,"step":0,"task":null},{"name":"Disparate Impact","value":0.3276081902047551,"timestamp":1757499197822,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":22.2,"timestamp":1757499197543,"step":13,"task":null},{"name":"system_network_receive_megabytes","value":5.7075410000106785,"timestamp":1757499197543,"step":13,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b8241a17d48485785e706a0435b0fd7/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_100","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"7b8241a17d48485785e706a0435b0fd7\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:13:14.239182\", \"model_uuid\": \"61d4ddaf921b429ba1bd18ce89ae950c\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"da238ac4c3a2430ba166f9d1b0d1f08b","name":"Run_99","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499173971,"endTime":1757499179622,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.23223982476695024,"timestamp":1757499178879,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40019734902166737,"timestamp":1757499178879,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8924040928982347,"timestamp":1757499178879,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757499179263,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.8749991069802777,"timestamp":1757499178879,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.962,"timestamp":1757499179440,"step":340000,"task":null},{"name":"Recall","value":0.654275092936803,"timestamp":1757499178746,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30221.5,"timestamp":1757499179263,"step":4,"task":null},{"name":"Test Fairness","value":0.3142857142857143,"timestamp":1757499178879,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499179263,"step":4,"task":null},{"name":"Statistical Parity","value":0.8052884615384616,"timestamp":1757499178879,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.34371002132196166,"timestamp":1757499178879,"step":0,"task":null},{"name":"AUC-ROC","value":0.9175695036191511,"timestamp":1757499178746,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8494117647058823,"timestamp":1757499179531,"step":340000,"task":null},{"name":"Well Calibration","value":0.39611146894757665,"timestamp":1757499178879,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6259710488739406,"timestamp":1757499178746,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.085481999994954,"timestamp":1757499179263,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.41074811887619767,"timestamp":1757499178879,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6221582634366931,"timestamp":1757499178746,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9779735682819383,"timestamp":1757499178879,"step":0,"task":null},{"name":"Equalized Odds","value":0.8924040928982347,"timestamp":1757499178879,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.10451466674685389,"timestamp":1757499178879,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499179263,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499179263,"step":4,"task":null},{"name":"Accuracy","value":0.8645343367826905,"timestamp":1757499178746,"step":0,"task":null},{"name":"F1 Score","value":0.7096774193548387,"timestamp":1757499178746,"step":0,"task":null},{"name":"Log Loss","value":0.3647106493122627,"timestamp":1757499178746,"step":0,"task":null},{"name":"Treatment Equality","value":0.2424706540518379,"timestamp":1757499178879,"step":0,"task":null},{"name":"Precision","value":0.775330396475771,"timestamp":1757499178746,"step":0,"task":null},{"name":"Disparate Impact","value":0.34371002132196166,"timestamp":1757499178879,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":4.3,"timestamp":1757499179263,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.084696000005351,"timestamp":1757499179263,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/da238ac4c3a2430ba166f9d1b0d1f08b/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_99","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"da238ac4c3a2430ba166f9d1b0d1f08b\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:12:55.437332\", \"model_uuid\": \"5ac31f6fc4d54e309d7e2219a2ff7835\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"822caebf39074a7c853c0fae5a54ae76","name":"Run_98","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499168028,"endTime":1757499173917,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.5612444843551634,"timestamp":1757499173214,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5609388739797853,"timestamp":1757499173214,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.3211110418116046,"timestamp":1757499173214,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":30.0,"timestamp":1757499173355,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.5325824979694639,"timestamp":1757499173214,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9394705882352941,"timestamp":1757499173768,"step":340000,"task":null},{"name":"Recall","value":0.45426829268292684,"timestamp":1757499173081,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30390.9,"timestamp":1757499173355,"step":4,"task":null},{"name":"Test Fairness","value":0.3216374269005848,"timestamp":1757499173214,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499173355,"step":4,"task":null},{"name":"Statistical Parity","value":0.8510773938543924,"timestamp":1757499173214,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.4779888268156424,"timestamp":1757499173214,"step":0,"task":null},{"name":"AUC-ROC","value":0.7890098722415795,"timestamp":1757499173081,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7887058823529411,"timestamp":1757499173842,"step":340000,"task":null},{"name":"Well Calibration","value":0.6686245868985252,"timestamp":1757499173214,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40556020508374363,"timestamp":1757499173081,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.143187999987276,"timestamp":1757499173355,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.5548869050433103,"timestamp":1757499173214,"step":0,"task":null},{"name":"Cohen Kappa","value":0.39168648301114195,"timestamp":1757499173081,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3339296362552177,"timestamp":1757499173214,"step":0,"task":null},{"name":"Equalized Odds","value":0.3211110418116046,"timestamp":1757499173214,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.3817935049209126,"timestamp":1757499173214,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499173355,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499173355,"step":4,"task":null},{"name":"Accuracy","value":0.7638758231420508,"timestamp":1757499173081,"step":0,"task":null},{"name":"F1 Score","value":0.5428051001821493,"timestamp":1757499173081,"step":0,"task":null},{"name":"Log Loss","value":0.49745036192937464,"timestamp":1757499173081,"step":0,"task":null},{"name":"Treatment Equality","value":0.9355727292659224,"timestamp":1757499173214,"step":0,"task":null},{"name":"Precision","value":0.6742081447963801,"timestamp":1757499173081,"step":0,"task":null},{"name":"Disparate Impact","value":0.4779888268156424,"timestamp":1757499173214,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":17.8,"timestamp":1757499173355,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.13346900002216,"timestamp":1757499173355,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/822caebf39074a7c853c0fae5a54ae76/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_98","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"822caebf39074a7c853c0fae5a54ae76\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:12:49.488734\", \"model_uuid\": \"ec21016531e8449dac9d01fcd07c6203\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"cbc7df468edd42888ae815b2189caf95","name":"Run_97","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499147888,"endTime":1757499167968,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.22750023650640025,"timestamp":1757499167343,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.4014598012267831,"timestamp":1757499167343,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8918209020589185,"timestamp":1757499167343,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":30.0,"timestamp":1757499166888,"step":14,"task":null},{"name":"Balance for Negative Class","value":0.9474221948927709,"timestamp":1757499167343,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9604705882352942,"timestamp":1757499167837,"step":340000,"task":null},{"name":"Recall","value":0.654275092936803,"timestamp":1757499167196,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30376.9,"timestamp":1757499166888,"step":14,"task":null},{"name":"Test Fairness","value":0.2702702702702703,"timestamp":1757499167343,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499166888,"step":14,"task":null},{"name":"Statistical Parity","value":0.7996306971716808,"timestamp":1757499167343,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.32926277702397105,"timestamp":1757499167343,"step":0,"task":null},{"name":"AUC-ROC","value":0.9204465648497561,"timestamp":1757499167196,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8482352941176471,"timestamp":1757499167895,"step":340000,"task":null},{"name":"Well Calibration","value":0.38805217109667833,"timestamp":1757499167343,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6236366968296819,"timestamp":1757499167196,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":6.403072000015527,"timestamp":1757499166888,"step":14,"task":null},{"name":"Equal Negative Predictive Value","value":0.4127412112935861,"timestamp":1757499167343,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6200267720456456,"timestamp":1757499167196,"step":0,"task":null},{"name":"Equal Opportunity","value":0.979386385426654,"timestamp":1757499167343,"step":0,"task":null},{"name":"Equalized Odds","value":0.8918209020589185,"timestamp":1757499167343,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.10689101910102945,"timestamp":1757499167343,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499166888,"step":14,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499166888,"step":14,"task":null},{"name":"Accuracy","value":0.863593603010348,"timestamp":1757499167196,"step":0,"task":null},{"name":"F1 Score","value":0.7082494969818913,"timestamp":1757499167196,"step":0,"task":null},{"name":"Log Loss","value":0.3042315876739282,"timestamp":1757499167196,"step":0,"task":null},{"name":"Treatment Equality","value":0.2188812848149683,"timestamp":1757499167343,"step":0,"task":null},{"name":"Precision","value":0.7719298245614035,"timestamp":1757499167196,"step":0,"task":null},{"name":"Disparate Impact","value":0.32926277702397105,"timestamp":1757499167343,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":28.4,"timestamp":1757499166888,"step":14,"task":null},{"name":"system_network_receive_megabytes","value":5.902398000005633,"timestamp":1757499166888,"step":14,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cbc7df468edd42888ae815b2189caf95/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_97","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"cbc7df468edd42888ae815b2189caf95\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:12:43.892959\", \"model_uuid\": \"14b090c33fc24eedbe3d53d2449f6903\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"7f773c01658b443485b9eb4045032203","name":"Run_96","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499142291,"endTime":1757499147835,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.23223982476695024,"timestamp":1757499147116,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3995314632495847,"timestamp":1757499147116,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8857961633828164,"timestamp":1757499147116,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.7,"timestamp":1757499147601,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.8640616181430243,"timestamp":1757499147116,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9616470588235295,"timestamp":1757499147659,"step":340000,"task":null},{"name":"Recall","value":0.6505576208178439,"timestamp":1757499146978,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30107.8,"timestamp":1757499147601,"step":4,"task":null},{"name":"Test Fairness","value":0.33333333333333337,"timestamp":1757499147116,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499147601,"step":4,"task":null},{"name":"Statistical Parity","value":0.8068021616541353,"timestamp":1757499147116,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3454726368159204,"timestamp":1757499147116,"step":0,"task":null},{"name":"AUC-ROC","value":0.9175741855739608,"timestamp":1757499146978,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8505882352941176,"timestamp":1757499147745,"step":340000,"task":null},{"name":"Well Calibration","value":0.39913158885541816,"timestamp":1757499147116,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6230293537866536,"timestamp":1757499146978,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.101388999988558,"timestamp":1757499147601,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.409977484506824,"timestamp":1757499147116,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6190406750420789,"timestamp":1757499146978,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9713656387665199,"timestamp":1757499147116,"step":0,"task":null},{"name":"Equalized Odds","value":0.8857961633828164,"timestamp":1757499147116,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.10380848656613191,"timestamp":1757499147116,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499147601,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499147601,"step":4,"task":null},{"name":"Accuracy","value":0.863593603010348,"timestamp":1757499146978,"step":0,"task":null},{"name":"F1 Score","value":0.7070707070707071,"timestamp":1757499146978,"step":0,"task":null},{"name":"Log Loss","value":0.36474143767460016,"timestamp":1757499146978,"step":0,"task":null},{"name":"Treatment Equality","value":0.24553990283730417,"timestamp":1757499147116,"step":0,"task":null},{"name":"Precision","value":0.7743362831858407,"timestamp":1757499146978,"step":0,"task":null},{"name":"Disparate Impact","value":0.3454726368159204,"timestamp":1757499147116,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":5.5,"timestamp":1757499147601,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.099243000004208,"timestamp":1757499147601,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f773c01658b443485b9eb4045032203/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_96","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"7f773c01658b443485b9eb4045032203\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:12:23.743395\", \"model_uuid\": \"9601e26509ce469aa7de21be1a716b45\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"37daabe3af4a4a80969a5f2ce05e542f","name":"Run_95","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499137079,"endTime":1757499142239,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.5475794012578202,"timestamp":1757499141680,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5612639993921658,"timestamp":1757499141680,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.32573991797020385,"timestamp":1757499141680,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757499141338,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.5370092574283305,"timestamp":1757499141680,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9352941176470588,"timestamp":1757499142064,"step":340000,"task":null},{"name":"Recall","value":0.45121951219512196,"timestamp":1757499141541,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30309.3,"timestamp":1757499141338,"step":3,"task":null},{"name":"Test Fairness","value":0.9117647058823529,"timestamp":1757499141680,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499141338,"step":3,"task":null},{"name":"Statistical Parity","value":0.8538496329223221,"timestamp":1757499141680,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.48936951316839583,"timestamp":1757499141680,"step":0,"task":null},{"name":"AUC-ROC","value":0.7897336983573918,"timestamp":1757499141541,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7887058823529411,"timestamp":1757499142150,"step":340000,"task":null},{"name":"Well Calibration","value":0.6717668228403628,"timestamp":1757499141680,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3983297531229129,"timestamp":1757499141541,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.0731139999988955,"timestamp":1757499141338,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5543000023108053,"timestamp":1757499141680,"step":0,"task":null},{"name":"Cohen Kappa","value":0.3849864925811697,"timestamp":1757499141541,"step":0,"task":null},{"name":"Equal Opportunity","value":0.33653846153846156,"timestamp":1757499141680,"step":0,"task":null},{"name":"Equalized Odds","value":0.32573991797020385,"timestamp":1757499141680,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.3754078010546402,"timestamp":1757499141680,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499141338,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499141338,"step":3,"task":null},{"name":"Accuracy","value":0.7610536218250236,"timestamp":1757499141541,"step":0,"task":null},{"name":"F1 Score","value":0.5381818181818182,"timestamp":1757499141541,"step":0,"task":null},{"name":"Log Loss","value":0.5070887064062609,"timestamp":1757499141541,"step":0,"task":null},{"name":"Treatment Equality","value":0.9690142537851377,"timestamp":1757499141680,"step":0,"task":null},{"name":"Precision","value":0.6666666666666666,"timestamp":1757499141541,"step":0,"task":null},{"name":"Disparate Impact","value":0.48936951316839583,"timestamp":1757499141680,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":29.1,"timestamp":1757499141338,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.071988000010606,"timestamp":1757499141338,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/37daabe3af4a4a80969a5f2ce05e542f/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_95","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"37daabe3af4a4a80969a5f2ce05e542f\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:12:18.416019\", \"model_uuid\": \"ab8213f6fe564d9586383f568b3059a6\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"7b6152e2c7b14122a0e7a73b42ae2272","name":"Run_94","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499120061,"endTime":1757499137024,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.2804405431148078,"timestamp":1757499136409,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3987878557943253,"timestamp":1757499136409,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.89244495177677,"timestamp":1757499136409,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757499136347,"step":12,"task":null},{"name":"Balance for Negative Class","value":0.8794680476092737,"timestamp":1757499136409,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9568235294117647,"timestamp":1757499136848,"step":340000,"task":null},{"name":"Recall","value":0.6579925650557621,"timestamp":1757499136241,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30305.7,"timestamp":1757499136347,"step":12,"task":null},{"name":"Test Fairness","value":0.38636363636363635,"timestamp":1757499136409,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499136347,"step":12,"task":null},{"name":"Statistical Parity","value":0.7988340042795489,"timestamp":1757499136409,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3442589035022905,"timestamp":1757499136409,"step":0,"task":null},{"name":"AUC-ROC","value":0.9145449608120382,"timestamp":1757499136241,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8449411764705882,"timestamp":1757499136936,"step":340000,"task":null},{"name":"Well Calibration","value":0.38966429993091334,"timestamp":1757499136409,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6151061756656189,"timestamp":1757499136241,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.246033000003081,"timestamp":1757499136347,"step":12,"task":null},{"name":"Equal Negative Predictive Value","value":0.41091356970157605,"timestamp":1757499136409,"step":0,"task":null},{"name":"Cohen Kappa","value":0.612553418934092,"timestamp":1757499136241,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9845814977973568,"timestamp":1757499136409,"step":0,"task":null},{"name":"Equalized Odds","value":0.89244495177677,"timestamp":1757499136409,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.12705913591254447,"timestamp":1757499136409,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499136347,"step":12,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499136347,"step":12,"task":null},{"name":"Accuracy","value":0.8598306679209784,"timestamp":1757499136241,"step":0,"task":null},{"name":"F1 Score","value":0.7037773359840954,"timestamp":1757499136241,"step":0,"task":null},{"name":"Log Loss","value":0.34070408545451253,"timestamp":1757499136241,"step":0,"task":null},{"name":"Treatment Equality","value":0.28908848937826004,"timestamp":1757499136409,"step":0,"task":null},{"name":"Precision","value":0.7564102564102564,"timestamp":1757499136241,"step":0,"task":null},{"name":"Disparate Impact","value":0.3442589035022905,"timestamp":1757499136409,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":31.5,"timestamp":1757499136347,"step":12,"task":null},{"name":"system_network_receive_megabytes","value":5.245674000005238,"timestamp":1757499136347,"step":12,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7b6152e2c7b14122a0e7a73b42ae2272/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_94","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"7b6152e2c7b14122a0e7a73b42ae2272\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:12:13.355202\", \"model_uuid\": \"af93d3bd49f04a97b4896f86ab63ec6b\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"8915c4b21f3f423d95db005f48ec1719","name":"Run_93","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499114823,"endTime":1757499119992,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.4846744169049396,"timestamp":1757499119415,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.4072668008447684,"timestamp":1757499119415,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8846057309090843,"timestamp":1757499119415,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757499119083,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.9977782738078028,"timestamp":1757499119415,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9566470588235294,"timestamp":1757499119818,"step":340000,"task":null},{"name":"Recall","value":0.6431226765799256,"timestamp":1757499119279,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30270.1,"timestamp":1757499119083,"step":3,"task":null},{"name":"Test Fairness","value":0.37222222222222223,"timestamp":1757499119415,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499119083,"step":3,"task":null},{"name":"Statistical Parity","value":0.8125022667101875,"timestamp":1757499119415,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3603124274998067,"timestamp":1757499119415,"step":0,"task":null},{"name":"AUC-ROC","value":0.9152566179431237,"timestamp":1757499119279,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8468235294117645,"timestamp":1757499119906,"step":340000,"task":null},{"name":"Well Calibration","value":0.4038437577810718,"timestamp":1757499119415,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6147860238385814,"timestamp":1757499119279,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.056251000001794,"timestamp":1757499119083,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.41317284835064644,"timestamp":1757499119415,"step":0,"task":null},{"name":"Cohen Kappa","value":0.610653546892633,"timestamp":1757499119279,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9559442824748947,"timestamp":1757499119415,"step":0,"task":null},{"name":"Equalized Odds","value":0.8846057309090843,"timestamp":1757499119415,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.233308705593447,"timestamp":1757499119415,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499119083,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499119083,"step":3,"task":null},{"name":"Accuracy","value":0.8607714016933208,"timestamp":1757499119279,"step":0,"task":null},{"name":"F1 Score","value":0.7004048582995951,"timestamp":1757499119279,"step":0,"task":null},{"name":"Log Loss","value":0.3686177624485241,"timestamp":1757499119279,"step":0,"task":null},{"name":"Treatment Equality","value":0.44837721387681645,"timestamp":1757499119415,"step":0,"task":null},{"name":"Precision","value":0.7688888888888888,"timestamp":1757499119279,"step":0,"task":null},{"name":"Disparate Impact","value":0.3603124274998067,"timestamp":1757499119415,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.0,"timestamp":1757499119083,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.055454999994254,"timestamp":1757499119083,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8915c4b21f3f423d95db005f48ec1719/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_93","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"8915c4b21f3f423d95db005f48ec1719\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:11:56.141113\", \"model_uuid\": \"d3af97d5828a4da2b28539c8458af49e\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"d9cb97a8ca5c4aed9db3d14b3bafe240","name":"Run_92","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499109678,"endTime":1757499114766,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.5350530750852557,"timestamp":1757499114208,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5615918117914253,"timestamp":1757499114208,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.33040987799153126,"timestamp":1757499114208,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757499113930,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.541405992826332,"timestamp":1757499114208,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9352352941176469,"timestamp":1757499114585,"step":340000,"task":null},{"name":"Recall","value":0.4481707317073171,"timestamp":1757499114072,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30273.1,"timestamp":1757499113930,"step":3,"task":null},{"name":"Test Fairness","value":0.4117647058823529,"timestamp":1757499114208,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499113930,"step":3,"task":null},{"name":"Statistical Parity","value":0.856639991199846,"timestamp":1757499114208,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.5007501995211492,"timestamp":1757499114208,"step":0,"task":null},{"name":"AUC-ROC","value":0.7894682263149162,"timestamp":1757499114072,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.792235294117647,"timestamp":1757499114678,"step":340000,"task":null},{"name":"Well Calibration","value":0.6730422482294774,"timestamp":1757499114208,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.39112645665824225,"timestamp":1757499114072,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.059041000000434,"timestamp":1757499113930,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5537160937668367,"timestamp":1757499114208,"step":0,"task":null},{"name":"Cohen Kappa","value":0.37829891335267685,"timestamp":1757499114072,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3391883706844337,"timestamp":1757499114208,"step":0,"task":null},{"name":"Equalized Odds","value":0.33040987799153126,"timestamp":1757499114208,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.3697083876785341,"timestamp":1757499114208,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499113930,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499113930,"step":3,"task":null},{"name":"Accuracy","value":0.7582314205079962,"timestamp":1757499114072,"step":0,"task":null},{"name":"F1 Score","value":0.5335753176043557,"timestamp":1757499114072,"step":0,"task":null},{"name":"Log Loss","value":0.5078818222680181,"timestamp":1757499114072,"step":0,"task":null},{"name":"Treatment Equality","value":0.997973697542047,"timestamp":1757499114208,"step":0,"task":null},{"name":"Precision","value":0.6591928251121076,"timestamp":1757499114072,"step":0,"task":null},{"name":"Disparate Impact","value":0.5007501995211492,"timestamp":1757499114208,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.5,"timestamp":1757499113930,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.058382999995956,"timestamp":1757499113930,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d9cb97a8ca5c4aed9db3d14b3bafe240/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_92","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"d9cb97a8ca5c4aed9db3d14b3bafe240\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:11:51.005022\", \"model_uuid\": \"d0304af05f6e49219096a484daedb6b8\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"951ef276bddc49fa9222fe08cf4b1dfe","name":"Run_91","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499093240,"endTime":1757499109631,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.29143821147225124,"timestamp":1757499109085,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3994558421524398,"timestamp":1757499109085,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8898290382294153,"timestamp":1757499109085,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757499108768,"step":12,"task":null},{"name":"Balance for Negative Class","value":0.872212038626964,"timestamp":1757499109085,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9558235294117647,"timestamp":1757499109461,"step":340000,"task":null},{"name":"Recall","value":0.654275092936803,"timestamp":1757499108942,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30248.2,"timestamp":1757499108768,"step":12,"task":null},{"name":"Test Fairness","value":0.38095238095238093,"timestamp":1757499109085,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499108768,"step":12,"task":null},{"name":"Statistical Parity","value":0.8033900917564284,"timestamp":1757499109085,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3494487362184055,"timestamp":1757499109085,"step":0,"task":null},{"name":"AUC-ROC","value":0.914610508179375,"timestamp":1757499108942,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8480000000000001,"timestamp":1757499109546,"step":340000,"task":null},{"name":"Well Calibration","value":0.3892592635537635,"timestamp":1757499109085,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6167068046550759,"timestamp":1757499108942,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.222016999992775,"timestamp":1757499108768,"step":12,"task":null},{"name":"Equal Negative Predictive Value","value":0.4104072793527337,"timestamp":1757499109085,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6136652734871888,"timestamp":1757499108942,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9779735682819383,"timestamp":1757499109085,"step":0,"task":null},{"name":"Equalized Odds","value":0.8898290382294153,"timestamp":1757499109085,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.1311556602313461,"timestamp":1757499109085,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499108768,"step":12,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499108768,"step":12,"task":null},{"name":"Accuracy","value":0.8607714016933208,"timestamp":1757499108942,"step":0,"task":null},{"name":"F1 Score","value":0.704,"timestamp":1757499108942,"step":0,"task":null},{"name":"Log Loss","value":0.3403287663877174,"timestamp":1757499108942,"step":0,"task":null},{"name":"Treatment Equality","value":0.3042768992023064,"timestamp":1757499109085,"step":0,"task":null},{"name":"Precision","value":0.7619047619047619,"timestamp":1757499108942,"step":0,"task":null},{"name":"Disparate Impact","value":0.3494487362184055,"timestamp":1757499109085,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":23.8,"timestamp":1757499108768,"step":12,"task":null},{"name":"system_network_receive_megabytes","value":5.2145869999949355,"timestamp":1757499108768,"step":12,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/951ef276bddc49fa9222fe08cf4b1dfe/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_91","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"951ef276bddc49fa9222fe08cf4b1dfe\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:11:46.057989\", \"model_uuid\": \"493fcac5983741a1864e1cbaf717ec1f\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"15514d17e1e04320b3ff83bc2ce1cbbf","name":"Run_90","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499088110,"endTime":1757499093188,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.4846744169049396,"timestamp":1757499092640,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.4079433237697597,"timestamp":1757499092640,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8781466479193892,"timestamp":1757499092640,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757499092369,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.9853060453852053,"timestamp":1757499092640,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9568823529411764,"timestamp":1757499093017,"step":340000,"task":null},{"name":"Recall","value":0.6468401486988847,"timestamp":1757499092503,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30256.0,"timestamp":1757499092369,"step":3,"task":null},{"name":"Test Fairness","value":0.36875,"timestamp":1757499092640,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499092369,"step":3,"task":null},{"name":"Statistical Parity","value":0.810983570884561,"timestamp":1757499092640,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.358455146945684,"timestamp":1757499092640,"step":0,"task":null},{"name":"AUC-ROC","value":0.9152659818527432,"timestamp":1757499092503,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8444705882352942,"timestamp":1757499093103,"step":340000,"task":null},{"name":"Well Calibration","value":0.40610042789156137,"timestamp":1757499092640,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6177408852519041,"timestamp":1757499092503,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.051645999978064,"timestamp":1757499092369,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.4139465802763967,"timestamp":1757499092640,"step":0,"task":null},{"name":"Cohen Kappa","value":0.613786063663349,"timestamp":1757499092503,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9494851994851996,"timestamp":1757499092640,"step":0,"task":null},{"name":"Equalized Odds","value":0.8781466479193892,"timestamp":1757499092640,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.23489583964510308,"timestamp":1757499092640,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499092369,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499092369,"step":3,"task":null},{"name":"Accuracy","value":0.8617121354656632,"timestamp":1757499092503,"step":0,"task":null},{"name":"F1 Score","value":0.703030303030303,"timestamp":1757499092503,"step":0,"task":null},{"name":"Log Loss","value":0.3681893703910613,"timestamp":1757499092503,"step":0,"task":null},{"name":"Treatment Equality","value":0.44277249870335605,"timestamp":1757499092640,"step":0,"task":null},{"name":"Precision","value":0.7699115044247787,"timestamp":1757499092503,"step":0,"task":null},{"name":"Disparate Impact","value":0.358455146945684,"timestamp":1757499092640,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":45.6,"timestamp":1757499092369,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.0511200000182725,"timestamp":1757499092369,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/15514d17e1e04320b3ff83bc2ce1cbbf/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_90","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"15514d17e1e04320b3ff83bc2ce1cbbf\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:11:29.395286\", \"model_uuid\": \"d7359c486e3c4abba133b8a191681133\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"b027f411218b4d0f86bc96bf49f73963","name":"Run_89","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499082681,"endTime":1757499088056,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757499087323,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5348285279048074,"timestamp":1757499087323,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.03476482617586907,"timestamp":1757499087323,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757499086986,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.6937613498789346,"timestamp":1757499087323,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7545882352941176,"timestamp":1757499087881,"step":340000,"task":null},{"name":"Recall","value":0.22560975609756098,"timestamp":1757499087205,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30251.2,"timestamp":1757499086986,"step":3,"task":null},{"name":"Test Fairness","value":0.24811632518175808,"timestamp":1757499087323,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499086986,"step":3,"task":null},{"name":"Statistical Parity","value":0.8723702664796634,"timestamp":1757499087323,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.0,"timestamp":1757499087323,"step":0,"task":null},{"name":"AUC-ROC","value":0.7712543554006969,"timestamp":1757499087205,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7515294117647059,"timestamp":1757499087970,"step":340000,"task":null},{"name":"Well Calibration","value":0.8084969412224796,"timestamp":1757499087323,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.33427967926023333,"timestamp":1757499087205,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.0555720000120346,"timestamp":1757499086986,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5299843747187151,"timestamp":1757499087323,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2531183481505105,"timestamp":1757499087205,"step":0,"task":null},{"name":"Equal Opportunity","value":0.0,"timestamp":1757499087323,"step":0,"task":null},{"name":"Equalized Odds","value":-0.03476482617586907,"timestamp":1757499087323,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757499087323,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499086986,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499086986,"step":3,"task":null},{"name":"Accuracy","value":0.7450611476952023,"timestamp":1757499087205,"step":0,"task":null},{"name":"F1 Score","value":0.3532219570405728,"timestamp":1757499087205,"step":0,"task":null},{"name":"Log Loss","value":0.533890463627045,"timestamp":1757499087205,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757499087323,"step":0,"task":null},{"name":"Precision","value":0.8131868131868132,"timestamp":1757499087205,"step":0,"task":null},{"name":"Disparate Impact","value":0.0,"timestamp":1757499087323,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.6,"timestamp":1757499086986,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.055246000003535,"timestamp":1757499086986,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b027f411218b4d0f86bc96bf49f73963/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_89","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"b027f411218b4d0f86bc96bf49f73963\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:11:24.101604\", \"model_uuid\": \"146057304e3f4bf089df8ab3d15f6db4\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"724dc69346cd4f9eb680cbd4e875da6d","name":"Run_88","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499066082,"endTime":1757499082631,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757499081919,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3954138430671146,"timestamp":1757499081919,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.10830525537845585,"timestamp":1757499081919,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757499081600,"step":11,"task":null},{"name":"Balance for Negative Class","value":0.8048728913720855,"timestamp":1757499081919,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.787470588235294,"timestamp":1757499082456,"step":340000,"task":null},{"name":"Recall","value":0.18587360594795538,"timestamp":1757499081786,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30237.9,"timestamp":1757499081600,"step":11,"task":null},{"name":"Test Fairness","value":0.508029197080292,"timestamp":1757499081919,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499081600,"step":11,"task":null},{"name":"Statistical Parity","value":0.9341070606040666,"timestamp":1757499081919,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.04346268656716418,"timestamp":1757499081919,"step":0,"task":null},{"name":"AUC-ROC","value":0.8821083778899367,"timestamp":1757499081786,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7830588235294117,"timestamp":1757499082545,"step":340000,"task":null},{"name":"Well Calibration","value":0.494019931469424,"timestamp":1757499081919,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.37555716822497986,"timestamp":1757499081786,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":9.31302400000277,"timestamp":1757499081600,"step":11,"task":null},{"name":"Equal Negative Predictive Value","value":0.3868410778163413,"timestamp":1757499081919,"step":0,"task":null},{"name":"Cohen Kappa","value":0.252179251859479,"timestamp":1757499081786,"step":0,"task":null},{"name":"Equal Opportunity","value":0.11030126336248784,"timestamp":1757499081919,"step":0,"task":null},{"name":"Equalized Odds","value":0.10830525537845585,"timestamp":1757499081919,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757499081919,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499081600,"step":11,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499081600,"step":11,"task":null},{"name":"Accuracy","value":0.793038570084666,"timestamp":1757499081786,"step":0,"task":null},{"name":"F1 Score","value":0.3125,"timestamp":1757499081786,"step":0,"task":null},{"name":"Log Loss","value":0.4242426023011633,"timestamp":1757499081786,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757499081919,"step":0,"task":null},{"name":"Precision","value":0.9803921568627451,"timestamp":1757499081786,"step":0,"task":null},{"name":"Disparate Impact","value":0.04346268656716418,"timestamp":1757499081919,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":21.7,"timestamp":1757499081600,"step":11,"task":null},{"name":"system_network_receive_megabytes","value":8.427860000025248,"timestamp":1757499081600,"step":11,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/724dc69346cd4f9eb680cbd4e875da6d/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_88","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"724dc69346cd4f9eb680cbd4e875da6d\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:11:18.945777\", \"model_uuid\": \"23338b3a0c534b83b6daabe9717f173e\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"0c4f3877bf194cc3ab4f05b19f2ce8cc","name":"Run_87","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499060688,"endTime":1757499066035,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757499065313,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3954138430671146,"timestamp":1757499065313,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.10830525537845585,"timestamp":1757499065313,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757499064945,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.8048728913720855,"timestamp":1757499065313,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7836470588235294,"timestamp":1757499065863,"step":340000,"task":null},{"name":"Recall","value":0.18587360594795538,"timestamp":1757499065188,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30236.9,"timestamp":1757499064945,"step":3,"task":null},{"name":"Test Fairness","value":0.508029197080292,"timestamp":1757499065313,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499064945,"step":3,"task":null},{"name":"Statistical Parity","value":0.9341070606040666,"timestamp":1757499065313,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.04346268656716418,"timestamp":1757499065313,"step":0,"task":null},{"name":"AUC-ROC","value":0.8824244098395964,"timestamp":1757499065188,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7814117647058825,"timestamp":1757499065950,"step":340000,"task":null},{"name":"Well Calibration","value":0.4939147390475009,"timestamp":1757499065313,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.37555716822497986,"timestamp":1757499065188,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.060147000011057,"timestamp":1757499064945,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.3868410778163413,"timestamp":1757499065313,"step":0,"task":null},{"name":"Cohen Kappa","value":0.252179251859479,"timestamp":1757499065188,"step":0,"task":null},{"name":"Equal Opportunity","value":0.11030126336248784,"timestamp":1757499065313,"step":0,"task":null},{"name":"Equalized Odds","value":0.10830525537845585,"timestamp":1757499065313,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757499065313,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499064945,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499064945,"step":3,"task":null},{"name":"Accuracy","value":0.793038570084666,"timestamp":1757499065188,"step":0,"task":null},{"name":"F1 Score","value":0.3125,"timestamp":1757499065188,"step":0,"task":null},{"name":"Log Loss","value":0.4241863453611334,"timestamp":1757499065188,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757499065313,"step":0,"task":null},{"name":"Precision","value":0.9803921568627451,"timestamp":1757499065188,"step":0,"task":null},{"name":"Disparate Impact","value":0.04346268656716418,"timestamp":1757499065313,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.7,"timestamp":1757499064945,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.058973000006517,"timestamp":1757499064945,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0c4f3877bf194cc3ab4f05b19f2ce8cc/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_87","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"0c4f3877bf194cc3ab4f05b19f2ce8cc\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:11:02.134518\", \"model_uuid\": \"b2269adb5d144be19cb8b74648bd9b08\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"4b0352f7d24545d1867449673e7c2ecd","name":"Run_86","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499055274,"endTime":1757499060649,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757499059917,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5348285279048074,"timestamp":1757499059917,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.03476482617586907,"timestamp":1757499059917,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757499059564,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.6937613498789346,"timestamp":1757499059917,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7535294117647059,"timestamp":1757499060488,"step":340000,"task":null},{"name":"Recall","value":0.22560975609756098,"timestamp":1757499059791,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30232.8,"timestamp":1757499059564,"step":3,"task":null},{"name":"Test Fairness","value":0.24811632518175808,"timestamp":1757499059917,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499059564,"step":3,"task":null},{"name":"Statistical Parity","value":0.8723702664796634,"timestamp":1757499059917,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.0,"timestamp":1757499059917,"step":0,"task":null},{"name":"AUC-ROC","value":0.7712294673967148,"timestamp":1757499059791,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7475294117647059,"timestamp":1757499060580,"step":340000,"task":null},{"name":"Well Calibration","value":0.8084637679741107,"timestamp":1757499059917,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.33427967926023333,"timestamp":1757499059791,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.062743000016781,"timestamp":1757499059564,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5299843747187151,"timestamp":1757499059917,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2531183481505105,"timestamp":1757499059791,"step":0,"task":null},{"name":"Equal Opportunity","value":0.0,"timestamp":1757499059917,"step":0,"task":null},{"name":"Equalized Odds","value":-0.03476482617586907,"timestamp":1757499059917,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757499059917,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499059564,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499059564,"step":3,"task":null},{"name":"Accuracy","value":0.7450611476952023,"timestamp":1757499059791,"step":0,"task":null},{"name":"F1 Score","value":0.3532219570405728,"timestamp":1757499059791,"step":0,"task":null},{"name":"Log Loss","value":0.5338962989931526,"timestamp":1757499059791,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757499059917,"step":0,"task":null},{"name":"Precision","value":0.8131868131868132,"timestamp":1757499059791,"step":0,"task":null},{"name":"Disparate Impact","value":0.0,"timestamp":1757499059917,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.4,"timestamp":1757499059564,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.0615689999831375,"timestamp":1757499059564,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4b0352f7d24545d1867449673e7c2ecd/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_86","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"4b0352f7d24545d1867449673e7c2ecd\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:10:56.687599\", \"model_uuid\": \"25ac8cb9da2e431f831c27452a72abf0\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"8926bdd8eecc4928841c0356fdc0f83a","name":"Run_85","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499037889,"endTime":1757499055219,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757499054551,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3954138430671146,"timestamp":1757499054551,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.10830525537845585,"timestamp":1757499054551,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.7,"timestamp":1757499055196,"step":13,"task":null},{"name":"Balance for Negative Class","value":0.8048728913720855,"timestamp":1757499054551,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7852941176470589,"timestamp":1757499055052,"step":340000,"task":null},{"name":"Recall","value":0.18587360594795538,"timestamp":1757499054418,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30114.8,"timestamp":1757499055196,"step":13,"task":null},{"name":"Test Fairness","value":0.508029197080292,"timestamp":1757499054551,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499055196,"step":13,"task":null},{"name":"Statistical Parity","value":0.9341070606040666,"timestamp":1757499054551,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.04346268656716418,"timestamp":1757499054551,"step":0,"task":null},{"name":"AUC-ROC","value":0.8821083778899367,"timestamp":1757499054418,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7828235294117647,"timestamp":1757499055139,"step":340000,"task":null},{"name":"Well Calibration","value":0.494019931469424,"timestamp":1757499054551,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.37555716822497986,"timestamp":1757499054418,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.209490999986883,"timestamp":1757499055196,"step":13,"task":null},{"name":"Equal Negative Predictive Value","value":0.3868410778163413,"timestamp":1757499054551,"step":0,"task":null},{"name":"Cohen Kappa","value":0.252179251859479,"timestamp":1757499054418,"step":0,"task":null},{"name":"Equal Opportunity","value":0.11030126336248784,"timestamp":1757499054551,"step":0,"task":null},{"name":"Equalized Odds","value":0.10830525537845585,"timestamp":1757499054551,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757499054551,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499055196,"step":13,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499055196,"step":13,"task":null},{"name":"Accuracy","value":0.793038570084666,"timestamp":1757499054418,"step":0,"task":null},{"name":"F1 Score","value":0.3125,"timestamp":1757499054418,"step":0,"task":null},{"name":"Log Loss","value":0.4242426023011633,"timestamp":1757499054418,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757499054551,"step":0,"task":null},{"name":"Precision","value":0.9803921568627451,"timestamp":1757499054418,"step":0,"task":null},{"name":"Disparate Impact","value":0.04346268656716418,"timestamp":1757499054551,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":6.1,"timestamp":1757499055196,"step":13,"task":null},{"name":"system_network_receive_megabytes","value":5.211684999987483,"timestamp":1757499055196,"step":13,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8926bdd8eecc4928841c0356fdc0f83a/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_85","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"8926bdd8eecc4928841c0356fdc0f83a\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:10:51.415740\", \"model_uuid\": \"4a301843597541acbfe97e26c0c86696\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"0d3cbf428cd445bbaecb6f12e1f38243","name":"Run_84","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499032388,"endTime":1757499037840,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757499037072,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3954138430671146,"timestamp":1757499037072,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.10830525537845585,"timestamp":1757499037072,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.7,"timestamp":1757499037755,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.8048728913720855,"timestamp":1757499037072,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7830588235294117,"timestamp":1757499037655,"step":340000,"task":null},{"name":"Recall","value":0.18587360594795538,"timestamp":1757499036951,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30105.9,"timestamp":1757499037755,"step":4,"task":null},{"name":"Test Fairness","value":0.508029197080292,"timestamp":1757499037072,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499037755,"step":4,"task":null},{"name":"Statistical Parity","value":0.9341070606040666,"timestamp":1757499037072,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.04346268656716418,"timestamp":1757499037072,"step":0,"task":null},{"name":"AUC-ROC","value":0.8824244098395964,"timestamp":1757499036951,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7807058823529412,"timestamp":1757499037745,"step":340000,"task":null},{"name":"Well Calibration","value":0.4939147390475009,"timestamp":1757499037072,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.37555716822497986,"timestamp":1757499036951,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.093644000007771,"timestamp":1757499037755,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.3868410778163413,"timestamp":1757499037072,"step":0,"task":null},{"name":"Cohen Kappa","value":0.252179251859479,"timestamp":1757499036951,"step":0,"task":null},{"name":"Equal Opportunity","value":0.11030126336248784,"timestamp":1757499037072,"step":0,"task":null},{"name":"Equalized Odds","value":0.10830525537845585,"timestamp":1757499037072,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757499037072,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499037755,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499037755,"step":4,"task":null},{"name":"Accuracy","value":0.793038570084666,"timestamp":1757499036951,"step":0,"task":null},{"name":"F1 Score","value":0.3125,"timestamp":1757499036951,"step":0,"task":null},{"name":"Log Loss","value":0.4241863453611334,"timestamp":1757499036951,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757499037072,"step":0,"task":null},{"name":"Precision","value":0.9803921568627451,"timestamp":1757499036951,"step":0,"task":null},{"name":"Disparate Impact","value":0.04346268656716418,"timestamp":1757499037072,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":3.9,"timestamp":1757499037755,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.102309999987483,"timestamp":1757499037755,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0d3cbf428cd445bbaecb6f12e1f38243/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_84","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"0d3cbf428cd445bbaecb6f12e1f38243\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:10:33.893935\", \"model_uuid\": \"8ed3cec486e74a8a86986923ebfdcb90\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"485bdd2aef6e43e3b4ebcf71cc4e6f6f","name":"Run_83","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499027103,"endTime":1757499032336,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757499031715,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5297365091758119,"timestamp":1757499031715,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.006895286036390343,"timestamp":1757499031715,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757499031395,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.7113439402104297,"timestamp":1757499031715,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7505294117647059,"timestamp":1757499032158,"step":340000,"task":null},{"name":"Recall","value":0.2225609756097561,"timestamp":1757499031590,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30218.8,"timestamp":1757499031395,"step":3,"task":null},{"name":"Test Fairness","value":0.25,"timestamp":1757499031715,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499031395,"step":3,"task":null},{"name":"Statistical Parity","value":0.8762764379895273,"timestamp":1757499031715,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.022634920634920636,"timestamp":1757499031715,"step":0,"task":null},{"name":"AUC-ROC","value":0.7699477351916375,"timestamp":1757499031590,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7475294117647058,"timestamp":1757499032245,"step":340000,"task":null},{"name":"Well Calibration","value":0.8010260003678731,"timestamp":1757499031715,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.32700022461497524,"timestamp":1757499031590,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.05706600000849,"timestamp":1757499031395,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5265040240423994,"timestamp":1757499031715,"step":0,"task":null},{"name":"Cohen Kappa","value":0.24760630643944415,"timestamp":1757499031590,"step":0,"task":null},{"name":"Equal Opportunity","value":0.029914529914529912,"timestamp":1757499031715,"step":0,"task":null},{"name":"Equalized Odds","value":-0.006895286036390343,"timestamp":1757499031715,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757499031715,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499031395,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499031395,"step":3,"task":null},{"name":"Accuracy","value":0.7431796801505174,"timestamp":1757499031590,"step":0,"task":null},{"name":"F1 Score","value":0.34844868735083534,"timestamp":1757499031590,"step":0,"task":null},{"name":"Log Loss","value":0.5331006643293007,"timestamp":1757499031590,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757499031715,"step":0,"task":null},{"name":"Precision","value":0.8021978021978022,"timestamp":1757499031590,"step":0,"task":null},{"name":"Disparate Impact","value":0.022634920634920636,"timestamp":1757499031715,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":28.4,"timestamp":1757499031395,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.0561260000104085,"timestamp":1757499031395,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/485bdd2aef6e43e3b4ebcf71cc4e6f6f/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_83","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"485bdd2aef6e43e3b4ebcf71cc4e6f6f\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:10:28.452816\", \"model_uuid\": \"3167412b5b03454f86b47d21a3260f7a\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"fdce23056a3e463989b1289c7abac521","name":"Run_82","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499010055,"endTime":1757499027044,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757499026420,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3996541862246443,"timestamp":1757499026420,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.009980039920159722,"timestamp":1757499026420,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.7,"timestamp":1757499026987,"step":13,"task":null},{"name":"Balance for Negative Class","value":0.7564480602529489,"timestamp":1757499026420,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7861176470588236,"timestamp":1757499026878,"step":340000,"task":null},{"name":"Recall","value":0.21189591078066913,"timestamp":1757499026292,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30082.3,"timestamp":1757499026987,"step":13,"task":null},{"name":"Test Fairness","value":0.4511070110701107,"timestamp":1757499026420,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499026987,"step":13,"task":null},{"name":"Statistical Parity","value":0.9148351648351648,"timestamp":1757499026420,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.0,"timestamp":1757499026420,"step":0,"task":null},{"name":"AUC-ROC","value":0.8823729083366887,"timestamp":1757499026292,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7823529411764706,"timestamp":1757499026968,"step":340000,"task":null},{"name":"Well Calibration","value":0.5058103510487548,"timestamp":1757499026420,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.38141154362707635,"timestamp":1757499026292,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.217295999987982,"timestamp":1757499026987,"step":13,"task":null},{"name":"Equal Negative Predictive Value","value":0.39183034063580136,"timestamp":1757499026420,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2757514199505804,"timestamp":1757499026292,"step":0,"task":null},{"name":"Equal Opportunity","value":0.0,"timestamp":1757499026420,"step":0,"task":null},{"name":"Equalized Odds","value":-0.009980039920159722,"timestamp":1757499026420,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757499026420,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499026987,"step":13,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499026987,"step":13,"task":null},{"name":"Accuracy","value":0.7958607714016933,"timestamp":1757499026292,"step":0,"task":null},{"name":"F1 Score","value":0.34441087613293053,"timestamp":1757499026292,"step":0,"task":null},{"name":"Log Loss","value":0.42216810085448053,"timestamp":1757499026292,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757499026420,"step":0,"task":null},{"name":"Precision","value":0.9193548387096774,"timestamp":1757499026292,"step":0,"task":null},{"name":"Disparate Impact","value":0.0,"timestamp":1757499026420,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":3.8,"timestamp":1757499026987,"step":13,"task":null},{"name":"system_network_receive_megabytes","value":5.214611999981571,"timestamp":1757499026987,"step":13,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fdce23056a3e463989b1289c7abac521/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_82","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"fdce23056a3e463989b1289c7abac521\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:10:23.173281\", \"model_uuid\": \"cb8517be06b7453483d9dba935c95c96\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"13bd6d133cc045b984ced73bca24d5bc","name":"Run_81","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757499004767,"endTime":1757499010004,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757499009429,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3996541862246443,"timestamp":1757499009429,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.009980039920159722,"timestamp":1757499009429,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757499009083,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.7564480602529489,"timestamp":1757499009429,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7925294117647059,"timestamp":1757499009832,"step":340000,"task":null},{"name":"Recall","value":0.21189591078066913,"timestamp":1757499009306,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30166.2,"timestamp":1757499009083,"step":3,"task":null},{"name":"Test Fairness","value":0.4511070110701107,"timestamp":1757499009429,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499009083,"step":3,"task":null},{"name":"Statistical Parity","value":0.9148351648351648,"timestamp":1757499009429,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.0,"timestamp":1757499009429,"step":0,"task":null},{"name":"AUC-ROC","value":0.8821294466865807,"timestamp":1757499009306,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7858823529411765,"timestamp":1757499009914,"step":340000,"task":null},{"name":"Well Calibration","value":0.5056505239113448,"timestamp":1757499009429,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.38141154362707635,"timestamp":1757499009306,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.056664000003366,"timestamp":1757499009083,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.39183034063580136,"timestamp":1757499009429,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2757514199505804,"timestamp":1757499009306,"step":0,"task":null},{"name":"Equal Opportunity","value":0.0,"timestamp":1757499009429,"step":0,"task":null},{"name":"Equalized Odds","value":-0.009980039920159722,"timestamp":1757499009429,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757499009429,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499009083,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499009083,"step":3,"task":null},{"name":"Accuracy","value":0.7958607714016933,"timestamp":1757499009306,"step":0,"task":null},{"name":"F1 Score","value":0.34441087613293053,"timestamp":1757499009306,"step":0,"task":null},{"name":"Log Loss","value":0.4221140018000568,"timestamp":1757499009306,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757499009429,"step":0,"task":null},{"name":"Precision","value":0.9193548387096774,"timestamp":1757499009306,"step":0,"task":null},{"name":"Disparate Impact","value":0.0,"timestamp":1757499009429,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.5,"timestamp":1757499009083,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.055816000007326,"timestamp":1757499009083,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13bd6d133cc045b984ced73bca24d5bc/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_81","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"13bd6d133cc045b984ced73bca24d5bc\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:10:06.111722\", \"model_uuid\": \"bf8620cb25494a0dbf84c202fbf4f84b\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"49b363df8d454f5590c5058790f713da","name":"Run_80","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498999423,"endTime":1757499004717,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757499004131,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5297365091758119,"timestamp":1757499004131,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.006895286036390343,"timestamp":1757499004131,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757499003720,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.7113439402104297,"timestamp":1757499004131,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7494705882352941,"timestamp":1757499004544,"step":340000,"task":null},{"name":"Recall","value":0.2225609756097561,"timestamp":1757499003988,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30149.0,"timestamp":1757499003720,"step":3,"task":null},{"name":"Test Fairness","value":0.25,"timestamp":1757499004131,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757499003720,"step":3,"task":null},{"name":"Statistical Parity","value":0.8762764379895273,"timestamp":1757499004131,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.022634920634920636,"timestamp":1757499004131,"step":0,"task":null},{"name":"AUC-ROC","value":0.7699269951883192,"timestamp":1757499003988,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7451764705882352,"timestamp":1757499004632,"step":340000,"task":null},{"name":"Well Calibration","value":0.800959533110023,"timestamp":1757499004131,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.32700022461497524,"timestamp":1757499003988,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.061360000021523,"timestamp":1757499003720,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5265040240423994,"timestamp":1757499004131,"step":0,"task":null},{"name":"Cohen Kappa","value":0.24760630643944415,"timestamp":1757499003988,"step":0,"task":null},{"name":"Equal Opportunity","value":0.029914529914529912,"timestamp":1757499004131,"step":0,"task":null},{"name":"Equalized Odds","value":-0.006895286036390343,"timestamp":1757499004131,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757499004131,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757499003720,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757499003720,"step":3,"task":null},{"name":"Accuracy","value":0.7431796801505174,"timestamp":1757499003988,"step":0,"task":null},{"name":"F1 Score","value":0.34844868735083534,"timestamp":1757499003988,"step":0,"task":null},{"name":"Log Loss","value":0.5331125784417615,"timestamp":1757499003988,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757499004131,"step":0,"task":null},{"name":"Precision","value":0.8021978021978022,"timestamp":1757499003988,"step":0,"task":null},{"name":"Disparate Impact","value":0.022634920634920636,"timestamp":1757499004131,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.5,"timestamp":1757499003720,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.060469999996712,"timestamp":1757499003720,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/49b363df8d454f5590c5058790f713da/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_80","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"49b363df8d454f5590c5058790f713da\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:10:00.780943\", \"model_uuid\": \"7c88ce8e7771497a8786b3d55651492e\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"03fb79c667d340dca6b84773b7e0367d","name":"Run_79","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498982843,"endTime":1757498999374,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498998740,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3996541862246443,"timestamp":1757498998740,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.009980039920159722,"timestamp":1757498998740,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.7,"timestamp":1757498998446,"step":11,"task":null},{"name":"Balance for Negative Class","value":0.7564480602529489,"timestamp":1757498998740,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7875294117647058,"timestamp":1757498999199,"step":340000,"task":null},{"name":"Recall","value":0.21189591078066913,"timestamp":1757498998622,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30114.5,"timestamp":1757498998446,"step":11,"task":null},{"name":"Test Fairness","value":0.4511070110701107,"timestamp":1757498998740,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498998446,"step":11,"task":null},{"name":"Statistical Parity","value":0.9148351648351648,"timestamp":1757498998740,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.0,"timestamp":1757498998740,"step":0,"task":null},{"name":"AUC-ROC","value":0.8823729083366887,"timestamp":1757498998622,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7832941176470588,"timestamp":1757498999286,"step":340000,"task":null},{"name":"Well Calibration","value":0.5058103510487548,"timestamp":1757498998740,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.38141154362707635,"timestamp":1757498998622,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.197016999998596,"timestamp":1757498998446,"step":11,"task":null},{"name":"Equal Negative Predictive Value","value":0.39183034063580136,"timestamp":1757498998740,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2757514199505804,"timestamp":1757498998622,"step":0,"task":null},{"name":"Equal Opportunity","value":0.0,"timestamp":1757498998740,"step":0,"task":null},{"name":"Equalized Odds","value":-0.009980039920159722,"timestamp":1757498998740,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498998740,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498998446,"step":11,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498998446,"step":11,"task":null},{"name":"Accuracy","value":0.7958607714016933,"timestamp":1757498998622,"step":0,"task":null},{"name":"F1 Score","value":0.34441087613293053,"timestamp":1757498998622,"step":0,"task":null},{"name":"Log Loss","value":0.42216810085448053,"timestamp":1757498998622,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498998740,"step":0,"task":null},{"name":"Precision","value":0.9193548387096774,"timestamp":1757498998622,"step":0,"task":null},{"name":"Disparate Impact","value":0.0,"timestamp":1757498998740,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":22.8,"timestamp":1757498998446,"step":11,"task":null},{"name":"system_network_receive_megabytes","value":5.195888999995077,"timestamp":1757498998446,"step":11,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/03fb79c667d340dca6b84773b7e0367d/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_79","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"03fb79c667d340dca6b84773b7e0367d\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:09:55.821048\", \"model_uuid\": \"71fc477200854ede9758407827f0433c\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"c867e215ff874f82963afcd030b01231","name":"Run_78","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498977659,"endTime":1757498982794,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498982206,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3996541862246443,"timestamp":1757498982206,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.009980039920159722,"timestamp":1757498982206,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498981926,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.7564480602529489,"timestamp":1757498982206,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.791,"timestamp":1757498982616,"step":340000,"task":null},{"name":"Recall","value":0.21189591078066913,"timestamp":1757498982084,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30228.5,"timestamp":1757498981926,"step":3,"task":null},{"name":"Test Fairness","value":0.4511070110701107,"timestamp":1757498982206,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498981926,"step":3,"task":null},{"name":"Statistical Parity","value":0.9148351648351648,"timestamp":1757498982206,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.0,"timestamp":1757498982206,"step":0,"task":null},{"name":"AUC-ROC","value":0.8821294466865807,"timestamp":1757498982084,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7870588235294118,"timestamp":1757498982707,"step":340000,"task":null},{"name":"Well Calibration","value":0.5056505239113448,"timestamp":1757498982206,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.38141154362707635,"timestamp":1757498982084,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.051795000006678,"timestamp":1757498981926,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.39183034063580136,"timestamp":1757498982206,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2757514199505804,"timestamp":1757498982084,"step":0,"task":null},{"name":"Equal Opportunity","value":0.0,"timestamp":1757498982206,"step":0,"task":null},{"name":"Equalized Odds","value":-0.009980039920159722,"timestamp":1757498982206,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498982206,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498981926,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498981926,"step":3,"task":null},{"name":"Accuracy","value":0.7958607714016933,"timestamp":1757498982084,"step":0,"task":null},{"name":"F1 Score","value":0.34441087613293053,"timestamp":1757498982084,"step":0,"task":null},{"name":"Log Loss","value":0.4221140018000568,"timestamp":1757498982084,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498982206,"step":0,"task":null},{"name":"Precision","value":0.9193548387096774,"timestamp":1757498982084,"step":0,"task":null},{"name":"Disparate Impact","value":0.0,"timestamp":1757498982206,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":28.8,"timestamp":1757498981926,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.0513930000015534,"timestamp":1757498981926,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c867e215ff874f82963afcd030b01231/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_78","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"c867e215ff874f82963afcd030b01231\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:09:38.976635\", \"model_uuid\": \"d4c6f6d7e5e447448375e5b8fe7a36ba\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"3ce51725a4ae44be9c0ee911452ec105","name":"Run_77","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498972688,"endTime":1757498977610,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498977109,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.548004913119052,"timestamp":1757498977109,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.012982446399985206,"timestamp":1757498977109,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498976934,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.5877760194987086,"timestamp":1757498977109,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7418823529411764,"timestamp":1757498977426,"step":340000,"task":null},{"name":"Recall","value":0.3170731707317073,"timestamp":1757498976972,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30239.4,"timestamp":1757498976934,"step":3,"task":null},{"name":"Test Fairness","value":0.24619240408714094,"timestamp":1757498977109,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498976934,"step":3,"task":null},{"name":"Statistical Parity","value":0.8237816575583177,"timestamp":1757498977109,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.03158361018826135,"timestamp":1757498977109,"step":0,"task":null},{"name":"AUC-ROC","value":0.7609714617554338,"timestamp":1757498976972,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7392941176470588,"timestamp":1757498977523,"step":340000,"task":null},{"name":"Well Calibration","value":0.7837846708349147,"timestamp":1757498977109,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.39393058117604535,"timestamp":1757498976972,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.0823139999993145,"timestamp":1757498976934,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5493533532426528,"timestamp":1757498977109,"step":0,"task":null},{"name":"Cohen Kappa","value":0.336254698605158,"timestamp":1757498976972,"step":0,"task":null},{"name":"Equal Opportunity","value":0.042232277526395176,"timestamp":1757498977109,"step":0,"task":null},{"name":"Equalized Odds","value":-0.012982446399985206,"timestamp":1757498977109,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498977109,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498976934,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498976934,"step":3,"task":null},{"name":"Accuracy","value":0.7638758231420508,"timestamp":1757498976972,"step":0,"task":null},{"name":"F1 Score","value":0.4531590413943355,"timestamp":1757498976972,"step":0,"task":null},{"name":"Log Loss","value":0.5298051861747594,"timestamp":1757498976972,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498977109,"step":0,"task":null},{"name":"Precision","value":0.7938931297709924,"timestamp":1757498976972,"step":0,"task":null},{"name":"Disparate Impact","value":0.03158361018826135,"timestamp":1757498977109,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.5,"timestamp":1757498976934,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.091593999997713,"timestamp":1757498976934,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3ce51725a4ae44be9c0ee911452ec105/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_77","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"3ce51725a4ae44be9c0ee911452ec105\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:09:33.967383\", \"model_uuid\": \"053a32d61ce7460a8a71f972e8e5f0d3\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"8e6a714c20894521bd322dadccd588fc","name":"Run_76","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498955232,"endTime":1757498972638,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498972150,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.39909807318775464,"timestamp":1757498972150,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.18564875262007313,"timestamp":1757498972150,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498972334,"step":13,"task":null},{"name":"Balance for Negative Class","value":0.789495290139751,"timestamp":1757498972150,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7892941176470588,"timestamp":1757498972476,"step":340000,"task":null},{"name":"Recall","value":0.21933085501858737,"timestamp":1757498972005,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30235.1,"timestamp":1757498972334,"step":13,"task":null},{"name":"Test Fairness","value":0.3688725490196078,"timestamp":1757498972150,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498972334,"step":13,"task":null},{"name":"Statistical Parity","value":0.9244752994752995,"timestamp":1757498972150,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.07366557045282064,"timestamp":1757498972150,"step":0,"task":null},{"name":"AUC-ROC","value":0.8951897596284402,"timestamp":1757498972005,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7872941176470588,"timestamp":1757498972555,"step":340000,"task":null},{"name":"Well Calibration","value":0.5084309095741382,"timestamp":1757498972150,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40529463521718573,"timestamp":1757498972005,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.218286999996053,"timestamp":1757498972334,"step":13,"task":null},{"name":"Equal Negative Predictive Value","value":0.39008968693957746,"timestamp":1757498972150,"step":0,"task":null},{"name":"Cohen Kappa","value":0.29127092951580635,"timestamp":1757498972005,"step":0,"task":null},{"name":"Equal Opportunity","value":0.189640768588137,"timestamp":1757498972150,"step":0,"task":null},{"name":"Equalized Odds","value":0.18564875262007313,"timestamp":1757498972150,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498972150,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498972334,"step":13,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498972334,"step":13,"task":null},{"name":"Accuracy","value":0.8005644402634055,"timestamp":1757498972005,"step":0,"task":null},{"name":"F1 Score","value":0.3575757575757576,"timestamp":1757498972005,"step":0,"task":null},{"name":"Log Loss","value":0.4180305912163541,"timestamp":1757498972005,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498972150,"step":0,"task":null},{"name":"Precision","value":0.9672131147540983,"timestamp":1757498972005,"step":0,"task":null},{"name":"Disparate Impact","value":0.07366557045282064,"timestamp":1757498972150,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":23.1,"timestamp":1757498972334,"step":13,"task":null},{"name":"system_network_receive_megabytes","value":5.216637000004994,"timestamp":1757498972334,"step":13,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8e6a714c20894521bd322dadccd588fc/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_76","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"8e6a714c20894521bd322dadccd588fc\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:09:29.169893\", \"model_uuid\": \"f9720d7de2404fc786ce1951fcdb6e75\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"ecc5ddd0f0f4440999750bf81a158edb","name":"Run_75","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498950333,"endTime":1757498955183,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498954673,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.39909807318775464,"timestamp":1757498954673,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.18564875262007313,"timestamp":1757498954673,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498954600,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.789495290139751,"timestamp":1757498954673,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7873529411764706,"timestamp":1757498955018,"step":340000,"task":null},{"name":"Recall","value":0.21933085501858737,"timestamp":1757498954526,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30236.7,"timestamp":1757498954600,"step":3,"task":null},{"name":"Test Fairness","value":0.3688725490196078,"timestamp":1757498954673,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498954600,"step":3,"task":null},{"name":"Statistical Parity","value":0.9244752994752995,"timestamp":1757498954673,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.07366557045282064,"timestamp":1757498954673,"step":0,"task":null},{"name":"AUC-ROC","value":0.8951733727866058,"timestamp":1757498954526,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7877647058823529,"timestamp":1757498955090,"step":340000,"task":null},{"name":"Well Calibration","value":0.5080484979041341,"timestamp":1757498954673,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40529463521718573,"timestamp":1757498954526,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.989245999982813,"timestamp":1757498954600,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.39008968693957746,"timestamp":1757498954673,"step":0,"task":null},{"name":"Cohen Kappa","value":0.29127092951580635,"timestamp":1757498954526,"step":0,"task":null},{"name":"Equal Opportunity","value":0.189640768588137,"timestamp":1757498954673,"step":0,"task":null},{"name":"Equalized Odds","value":0.18564875262007313,"timestamp":1757498954673,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498954673,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498954600,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498954600,"step":3,"task":null},{"name":"Accuracy","value":0.8005644402634055,"timestamp":1757498954526,"step":0,"task":null},{"name":"F1 Score","value":0.3575757575757576,"timestamp":1757498954526,"step":0,"task":null},{"name":"Log Loss","value":0.4179485630521994,"timestamp":1757498954526,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498954673,"step":0,"task":null},{"name":"Precision","value":0.9672131147540983,"timestamp":1757498954526,"step":0,"task":null},{"name":"Disparate Impact","value":0.07366557045282064,"timestamp":1757498954673,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":30.2,"timestamp":1757498954600,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.990640000003623,"timestamp":1757498954600,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ecc5ddd0f0f4440999750bf81a158edb/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_75","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"ecc5ddd0f0f4440999750bf81a158edb\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:09:11.560865\", \"model_uuid\": \"b0d5a0c5c62441f184bcd9856154f67a\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"fb370b67d3aa44a088978d89a9c88b55","name":"Run_74","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498945312,"endTime":1757498950285,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498949823,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.548004913119052,"timestamp":1757498949823,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.012982446399985206,"timestamp":1757498949823,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498949573,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.5877760194987086,"timestamp":1757498949823,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7502941176470588,"timestamp":1757498950123,"step":340000,"task":null},{"name":"Recall","value":0.3170731707317073,"timestamp":1757498949688,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30232.1,"timestamp":1757498949573,"step":3,"task":null},{"name":"Test Fairness","value":0.24619240408714094,"timestamp":1757498949823,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498949573,"step":3,"task":null},{"name":"Statistical Parity","value":0.8237816575583177,"timestamp":1757498949823,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.03158361018826135,"timestamp":1757498949823,"step":0,"task":null},{"name":"AUC-ROC","value":0.760917537746806,"timestamp":1757498949688,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7454117647058823,"timestamp":1757498950202,"step":340000,"task":null},{"name":"Well Calibration","value":0.7836042945537297,"timestamp":1757498949823,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.39393058117604535,"timestamp":1757498949688,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.058877000003122,"timestamp":1757498949573,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5493533532426528,"timestamp":1757498949823,"step":0,"task":null},{"name":"Cohen Kappa","value":0.336254698605158,"timestamp":1757498949688,"step":0,"task":null},{"name":"Equal Opportunity","value":0.042232277526395176,"timestamp":1757498949823,"step":0,"task":null},{"name":"Equalized Odds","value":-0.012982446399985206,"timestamp":1757498949823,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498949823,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498949573,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498949573,"step":3,"task":null},{"name":"Accuracy","value":0.7638758231420508,"timestamp":1757498949688,"step":0,"task":null},{"name":"F1 Score","value":0.4531590413943355,"timestamp":1757498949688,"step":0,"task":null},{"name":"Log Loss","value":0.529842150055418,"timestamp":1757498949688,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498949823,"step":0,"task":null},{"name":"Precision","value":0.7938931297709924,"timestamp":1757498949688,"step":0,"task":null},{"name":"Disparate Impact","value":0.03158361018826135,"timestamp":1757498949823,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.3,"timestamp":1757498949573,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.058050999999978,"timestamp":1757498949573,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/fb370b67d3aa44a088978d89a9c88b55/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_74","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"fb370b67d3aa44a088978d89a9c88b55\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:09:06.596565\", \"model_uuid\": \"cd81e6aaebad4893a368867edc7ba589\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"0a5a0e139704472fbbcbb0dcabe3e1af","name":"Run_73","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498928304,"endTime":1757498945257,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498944722,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.39909807318775464,"timestamp":1757498944722,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.18564875262007313,"timestamp":1757498944722,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498944679,"step":12,"task":null},{"name":"Balance for Negative Class","value":0.789495290139751,"timestamp":1757498944722,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7952941176470588,"timestamp":1757498945077,"step":340000,"task":null},{"name":"Recall","value":0.21933085501858737,"timestamp":1757498944567,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30246.5,"timestamp":1757498944679,"step":12,"task":null},{"name":"Test Fairness","value":0.3688725490196078,"timestamp":1757498944722,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498944679,"step":12,"task":null},{"name":"Statistical Parity","value":0.9244752994752995,"timestamp":1757498944722,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.07366557045282064,"timestamp":1757498944722,"step":0,"task":null},{"name":"AUC-ROC","value":0.8951897596284402,"timestamp":1757498944567,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7865882352941176,"timestamp":1757498945167,"step":340000,"task":null},{"name":"Well Calibration","value":0.5084309095741382,"timestamp":1757498944722,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40529463521718573,"timestamp":1757498944567,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":8.633460999990348,"timestamp":1757498944679,"step":12,"task":null},{"name":"Equal Negative Predictive Value","value":0.39008968693957746,"timestamp":1757498944722,"step":0,"task":null},{"name":"Cohen Kappa","value":0.29127092951580635,"timestamp":1757498944567,"step":0,"task":null},{"name":"Equal Opportunity","value":0.189640768588137,"timestamp":1757498944722,"step":0,"task":null},{"name":"Equalized Odds","value":0.18564875262007313,"timestamp":1757498944722,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498944722,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498944679,"step":12,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498944679,"step":12,"task":null},{"name":"Accuracy","value":0.8005644402634055,"timestamp":1757498944567,"step":0,"task":null},{"name":"F1 Score","value":0.3575757575757576,"timestamp":1757498944567,"step":0,"task":null},{"name":"Log Loss","value":0.4180305912163541,"timestamp":1757498944567,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498944722,"step":0,"task":null},{"name":"Precision","value":0.9672131147540983,"timestamp":1757498944567,"step":0,"task":null},{"name":"Disparate Impact","value":0.07366557045282064,"timestamp":1757498944722,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.5,"timestamp":1757498944679,"step":12,"task":null},{"name":"system_network_receive_megabytes","value":7.9095760000054725,"timestamp":1757498944679,"step":12,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0a5a0e139704472fbbcbb0dcabe3e1af/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_73","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"0a5a0e139704472fbbcbb0dcabe3e1af\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:09:01.695681\", \"model_uuid\": \"61bb9a25b5d74a5eae7bf6240822377e\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"d27a9657e2e4407d97c295af2b23757c","name":"Run_72","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498923369,"endTime":1757498928251,"params":[{"name":"criterion","value":"log_loss","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498927757,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.39909807318775464,"timestamp":1757498927757,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.18564875262007313,"timestamp":1757498927757,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498927634,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.789495290139751,"timestamp":1757498927757,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7939411764705883,"timestamp":1757498928072,"step":340000,"task":null},{"name":"Recall","value":0.21933085501858737,"timestamp":1757498927610,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30238.7,"timestamp":1757498927634,"step":3,"task":null},{"name":"Test Fairness","value":0.3688725490196078,"timestamp":1757498927757,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498927634,"step":3,"task":null},{"name":"Statistical Parity","value":0.9244752994752995,"timestamp":1757498927757,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.07366557045282064,"timestamp":1757498927757,"step":0,"task":null},{"name":"AUC-ROC","value":0.8951733727866058,"timestamp":1757498927610,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7870588235294117,"timestamp":1757498928162,"step":340000,"task":null},{"name":"Well Calibration","value":0.5080484979041341,"timestamp":1757498927757,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40529463521718573,"timestamp":1757498927610,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.055974999995669,"timestamp":1757498927634,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.39008968693957746,"timestamp":1757498927757,"step":0,"task":null},{"name":"Cohen Kappa","value":0.29127092951580635,"timestamp":1757498927610,"step":0,"task":null},{"name":"Equal Opportunity","value":0.189640768588137,"timestamp":1757498927757,"step":0,"task":null},{"name":"Equalized Odds","value":0.18564875262007313,"timestamp":1757498927757,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498927757,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498927634,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498927634,"step":3,"task":null},{"name":"Accuracy","value":0.8005644402634055,"timestamp":1757498927610,"step":0,"task":null},{"name":"F1 Score","value":0.3575757575757576,"timestamp":1757498927610,"step":0,"task":null},{"name":"Log Loss","value":0.4179485630521994,"timestamp":1757498927610,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498927757,"step":0,"task":null},{"name":"Precision","value":0.9672131147540983,"timestamp":1757498927610,"step":0,"task":null},{"name":"Disparate Impact","value":0.07366557045282064,"timestamp":1757498927757,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.6,"timestamp":1757498927634,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.054930999991484,"timestamp":1757498927634,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d27a9657e2e4407d97c295af2b23757c/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_72","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"d27a9657e2e4407d97c295af2b23757c\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:08:44.652629\", \"model_uuid\": \"bd152865ba8d4fe1a110075277453330\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"96b883f712474d68897868ec9b3bc67a","name":"Run_71","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498917169,"endTime":1757498923321,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.5724693740422665,"timestamp":1757498922259,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5645790022796775,"timestamp":1757498922259,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.28567308841103173,"timestamp":1757498922259,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498922468,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.5213845225422628,"timestamp":1757498922259,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9422941176470587,"timestamp":1757498923145,"step":340000,"task":null},{"name":"Recall","value":0.4481707317073171,"timestamp":1757498922122,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30253.2,"timestamp":1757498922468,"step":4,"task":null},{"name":"Test Fairness","value":0.2433414043583535,"timestamp":1757498922259,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498922468,"step":4,"task":null},{"name":"Statistical Parity","value":0.8440030765054517,"timestamp":1757498922259,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.4526984126984127,"timestamp":1757498922259,"step":0,"task":null},{"name":"AUC-ROC","value":0.7912622366019578,"timestamp":1757498922122,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7934117647058824,"timestamp":1757498923234,"step":340000,"task":null},{"name":"Well Calibration","value":0.656846602771707,"timestamp":1757498922259,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3977364935483471,"timestamp":1757498922122,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.062536999990698,"timestamp":1757498922468,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.5582633167468799,"timestamp":1757498922259,"step":0,"task":null},{"name":"Cohen Kappa","value":0.38384405436737234,"timestamp":1757498922122,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3005366726296959,"timestamp":1757498922259,"step":0,"task":null},{"name":"Equalized Odds","value":0.28567308841103173,"timestamp":1757498922259,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.3504864375173977,"timestamp":1757498922259,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498922468,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498922468,"step":4,"task":null},{"name":"Accuracy","value":0.7610536218250236,"timestamp":1757498922122,"step":0,"task":null},{"name":"F1 Score","value":0.5364963503649635,"timestamp":1757498922122,"step":0,"task":null},{"name":"Log Loss","value":0.49375581819216213,"timestamp":1757498922122,"step":0,"task":null},{"name":"Treatment Equality","value":0.8958972783668204,"timestamp":1757498922259,"step":0,"task":null},{"name":"Precision","value":0.6681818181818182,"timestamp":1757498922122,"step":0,"task":null},{"name":"Disparate Impact","value":0.4526984126984127,"timestamp":1757498922259,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":17.6,"timestamp":1757498922468,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.061293000006117,"timestamp":1757498922468,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/96b883f712474d68897868ec9b3bc67a/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_71","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"96b883f712474d68897868ec9b3bc67a\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:08:38.859599\", \"model_uuid\": \"b4520a89dda14793841c9255c80c0f32\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"9183607e695245d080020df021ecbc3d","name":"Run_70","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498899350,"endTime":1757498917123,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.32311627793662645,"timestamp":1757498916204,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40535320742836445,"timestamp":1757498916204,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8713207093528363,"timestamp":1757498916204,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.7,"timestamp":1757498916670,"step":13,"task":null},{"name":"Balance for Negative Class","value":0.9921722896039176,"timestamp":1757498916204,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9651176470588236,"timestamp":1757498916941,"step":340000,"task":null},{"name":"Recall","value":0.6468401486988847,"timestamp":1757498916051,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30097.6,"timestamp":1757498916670,"step":13,"task":null},{"name":"Test Fairness","value":0.3023255813953488,"timestamp":1757498916204,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498916670,"step":13,"task":null},{"name":"Statistical Parity","value":0.8056656458295802,"timestamp":1757498916204,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3360517002615787,"timestamp":1757498916204,"step":0,"task":null},{"name":"AUC-ROC","value":0.919418875769011,"timestamp":1757498916051,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.852470588235294,"timestamp":1757498917034,"step":340000,"task":null},{"name":"Well Calibration","value":0.3986123151018957,"timestamp":1757498916204,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6224378135171351,"timestamp":1757498916051,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.242005000007339,"timestamp":1757498916670,"step":13,"task":null},{"name":"Equal Negative Predictive Value","value":0.4137953099283335,"timestamp":1757498916204,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6180494465328699,"timestamp":1757498916051,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9494851994851996,"timestamp":1757498916204,"step":0,"task":null},{"name":"Equalized Odds","value":0.8713207093528363,"timestamp":1757498916204,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.15659722643006874,"timestamp":1757498916204,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498916670,"step":13,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498916670,"step":13,"task":null},{"name":"Accuracy","value":0.863593603010348,"timestamp":1757498916051,"step":0,"task":null},{"name":"F1 Score","value":0.7058823529411765,"timestamp":1757498916051,"step":0,"task":null},{"name":"Log Loss","value":0.305327127430211,"timestamp":1757498916051,"step":0,"task":null},{"name":"Treatment Equality","value":0.2951816658022374,"timestamp":1757498916204,"step":0,"task":null},{"name":"Precision","value":0.7767857142857143,"timestamp":1757498916051,"step":0,"task":null},{"name":"Disparate Impact","value":0.3360517002615787,"timestamp":1757498916204,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":4.2,"timestamp":1757498916670,"step":13,"task":null},{"name":"system_network_receive_megabytes","value":5.249088999989908,"timestamp":1757498916670,"step":13,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9183607e695245d080020df021ecbc3d/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_70","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"9183607e695245d080020df021ecbc3d\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:08:32.710834\", \"model_uuid\": \"a02b876b609c494489f86e55fb66c9d8\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"785ead0af11043e1bc46f834d1fb3303","name":"Run_69","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498893211,"endTime":1757498899297,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.33029663966855144,"timestamp":1757498898345,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.4054106445917069,"timestamp":1757498898345,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.9032179032783226,"timestamp":1757498898345,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498898519,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.9591137509004196,"timestamp":1757498898345,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9633529411764705,"timestamp":1757498899120,"step":340000,"task":null},{"name":"Recall","value":0.654275092936803,"timestamp":1757498898197,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30266.9,"timestamp":1757498898519,"step":4,"task":null},{"name":"Test Fairness","value":0.3589743589743589,"timestamp":1757498898345,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498898519,"step":4,"task":null},{"name":"Statistical Parity","value":0.8083158617698092,"timestamp":1757498898345,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3472534236036313,"timestamp":1757498898345,"step":0,"task":null},{"name":"AUC-ROC","value":0.9182717968406168,"timestamp":1757498898197,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8512941176470589,"timestamp":1757498899210,"step":340000,"task":null},{"name":"Well Calibration","value":0.3991511536598711,"timestamp":1757498898345,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.630677156735474,"timestamp":1757498898197,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":8.29055099998368,"timestamp":1757498898519,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.41334506246445907,"timestamp":1757498898345,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6264378625591478,"timestamp":1757498898197,"step":0,"task":null},{"name":"Equal Opportunity","value":0.979386385426654,"timestamp":1757498898345,"step":0,"task":null},{"name":"Equalized Odds","value":0.9032179032783226,"timestamp":1757498898345,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.1551899240281613,"timestamp":1757498898345,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498898519,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498898519,"step":4,"task":null},{"name":"Accuracy","value":0.8664158043273753,"timestamp":1757498898197,"step":0,"task":null},{"name":"F1 Score","value":0.7125506072874493,"timestamp":1757498898197,"step":0,"task":null},{"name":"Log Loss","value":0.3359093947034844,"timestamp":1757498898197,"step":0,"task":null},{"name":"Treatment Equality","value":0.3177831986943243,"timestamp":1757498898345,"step":0,"task":null},{"name":"Precision","value":0.7822222222222223,"timestamp":1757498898197,"step":0,"task":null},{"name":"Disparate Impact","value":0.3472534236036313,"timestamp":1757498898345,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":17.5,"timestamp":1757498898519,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":7.595832999999402,"timestamp":1757498898519,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/785ead0af11043e1bc46f834d1fb3303/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_69","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"785ead0af11043e1bc46f834d1fb3303\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:08:14.906599\", \"model_uuid\": \"c21e51d965f7401b80e8dd857f79c737\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"8572f77242df431f8ddbc875510723f7","name":"Run_68","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498886654,"endTime":1757498893161,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.5612444843551634,"timestamp":1757498892073,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5655747324424282,"timestamp":1757498892073,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.2877180781860828,"timestamp":1757498892073,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.7,"timestamp":1757498893026,"step":5,"task":null},{"name":"Balance for Negative Class","value":0.520196858481802,"timestamp":1757498892073,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9423529411764706,"timestamp":1757498892968,"step":340000,"task":null},{"name":"Recall","value":0.4481707317073171,"timestamp":1757498891909,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30089.1,"timestamp":1757498893026,"step":5,"task":null},{"name":"Test Fairness","value":0.243161094224924,"timestamp":1757498892073,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498893026,"step":5,"task":null},{"name":"Statistical Parity","value":0.8455865719585577,"timestamp":1757498892073,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.4552274541101356,"timestamp":1757498892073,"step":0,"task":null},{"name":"AUC-ROC","value":0.7914364526298324,"timestamp":1757498891909,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7917647058823528,"timestamp":1757498893074,"step":340000,"task":null},{"name":"Well Calibration","value":0.6583299924696931,"timestamp":1757498892073,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3999611176225641,"timestamp":1757498891909,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":26.88825099999667,"timestamp":1757498893026,"step":5,"task":null},{"name":"Equal Negative Predictive Value","value":0.5584900667643707,"timestamp":1757498892073,"step":0,"task":null},{"name":"Cohen Kappa","value":0.38569930812682596,"timestamp":1757498891909,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3005366726296959,"timestamp":1757498892073,"step":0,"task":null},{"name":"Equalized Odds","value":0.2877180781860828,"timestamp":1757498892073,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.3436141544288213,"timestamp":1757498892073,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498893026,"step":5,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498893026,"step":5,"task":null},{"name":"Accuracy","value":0.761994355597366,"timestamp":1757498891909,"step":0,"task":null},{"name":"F1 Score","value":0.5374771480804388,"timestamp":1757498891909,"step":0,"task":null},{"name":"Log Loss","value":0.4937397040467982,"timestamp":1757498891909,"step":0,"task":null},{"name":"Treatment Equality","value":0.9138152239341568,"timestamp":1757498892073,"step":0,"task":null},{"name":"Precision","value":0.6712328767123288,"timestamp":1757498891909,"step":0,"task":null},{"name":"Disparate Impact","value":0.4552274541101356,"timestamp":1757498892073,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":4.7,"timestamp":1757498893026,"step":5,"task":null},{"name":"system_network_receive_megabytes","value":26.88671299998532,"timestamp":1757498893026,"step":5,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/8572f77242df431f8ddbc875510723f7/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_68","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"8572f77242df431f8ddbc875510723f7\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:08:08.393994\", \"model_uuid\": \"c4e99403ace84eac9c701b29d9baee67\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"2aa14458f3984107bf9aeb5d7719086e","name":"Run_67","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498868797,"endTime":1757498886598,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.32311627793662645,"timestamp":1757498885682,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40535320742836445,"timestamp":1757498885682,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8713207093528363,"timestamp":1757498885682,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498885894,"step":13,"task":null},{"name":"Balance for Negative Class","value":0.9921722896039176,"timestamp":1757498885682,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9630588235294117,"timestamp":1757498886419,"step":340000,"task":null},{"name":"Recall","value":0.6468401486988847,"timestamp":1757498885524,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30244.6,"timestamp":1757498885894,"step":13,"task":null},{"name":"Test Fairness","value":0.30952380952380953,"timestamp":1757498885682,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498885894,"step":13,"task":null},{"name":"Statistical Parity","value":0.8056656458295802,"timestamp":1757498885682,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3360517002615787,"timestamp":1757498885682,"step":0,"task":null},{"name":"AUC-ROC","value":0.919461013362299,"timestamp":1757498885524,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8512941176470588,"timestamp":1757498886510,"step":340000,"task":null},{"name":"Well Calibration","value":0.3994312208960823,"timestamp":1757498885682,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6224378135171351,"timestamp":1757498885524,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.215337000001455,"timestamp":1757498885894,"step":13,"task":null},{"name":"Equal Negative Predictive Value","value":0.4137953099283335,"timestamp":1757498885682,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6180494465328699,"timestamp":1757498885524,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9494851994851996,"timestamp":1757498885682,"step":0,"task":null},{"name":"Equalized Odds","value":0.8713207093528363,"timestamp":1757498885682,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.15659722643006874,"timestamp":1757498885682,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498885894,"step":13,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498885894,"step":13,"task":null},{"name":"Accuracy","value":0.863593603010348,"timestamp":1757498885524,"step":0,"task":null},{"name":"F1 Score","value":0.7058823529411765,"timestamp":1757498885524,"step":0,"task":null},{"name":"Log Loss","value":0.3052737454084053,"timestamp":1757498885524,"step":0,"task":null},{"name":"Treatment Equality","value":0.2951816658022374,"timestamp":1757498885682,"step":0,"task":null},{"name":"Precision","value":0.7767857142857143,"timestamp":1757498885524,"step":0,"task":null},{"name":"Disparate Impact","value":0.3360517002615787,"timestamp":1757498885682,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":13.5,"timestamp":1757498885894,"step":13,"task":null},{"name":"system_network_receive_megabytes","value":5.2130229999893345,"timestamp":1757498885894,"step":13,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aa14458f3984107bf9aeb5d7719086e/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_67","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"2aa14458f3984107bf9aeb5d7719086e\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:08:02.271987\", \"model_uuid\": \"2abedf04f8964e5a84c29748e85498cb\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"9130ca04aba64af3bf9fb7e3446f7810","name":"Run_66","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498862652,"endTime":1757498868752,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.33780338147920036,"timestamp":1757498867788,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.4066976625110456,"timestamp":1757498867788,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8753127253209002,"timestamp":1757498867788,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498867956,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.9878301789273138,"timestamp":1757498867788,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9645882352941175,"timestamp":1757498868579,"step":340000,"task":null},{"name":"Recall","value":0.6468401486988847,"timestamp":1757498867635,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30253.0,"timestamp":1757498867956,"step":4,"task":null},{"name":"Test Fairness","value":0.33333333333333337,"timestamp":1757498867788,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498867956,"step":4,"task":null},{"name":"Statistical Parity","value":0.8086831201585299,"timestamp":1757498867788,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.33955223880597013,"timestamp":1757498867788,"step":0,"task":null},{"name":"AUC-ROC","value":0.9182858427050463,"timestamp":1757498867635,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8494117647058823,"timestamp":1757498868668,"step":340000,"task":null},{"name":"Well Calibration","value":0.40183750301725546,"timestamp":1757498867788,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6271855634063113,"timestamp":1757498867635,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.066739000001689,"timestamp":1757498867956,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.4140633911935815,"timestamp":1757498867788,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6223350733048942,"timestamp":1757498867635,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9494851994851996,"timestamp":1757498867788,"step":0,"task":null},{"name":"Equalized Odds","value":0.8753127253209002,"timestamp":1757498867788,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.16371528217689002,"timestamp":1757498867788,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498867956,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498867956,"step":4,"task":null},{"name":"Accuracy","value":0.8654750705550329,"timestamp":1757498867635,"step":0,"task":null},{"name":"F1 Score","value":0.7087576374745418,"timestamp":1757498867635,"step":0,"task":null},{"name":"Log Loss","value":0.3359270743723519,"timestamp":1757498867635,"step":0,"task":null},{"name":"Treatment Equality","value":0.3085990142477937,"timestamp":1757498867788,"step":0,"task":null},{"name":"Precision","value":0.7837837837837838,"timestamp":1757498867635,"step":0,"task":null},{"name":"Disparate Impact","value":0.33955223880597013,"timestamp":1757498867788,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":12.8,"timestamp":1757498867956,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.065748999972129,"timestamp":1757498867956,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9130ca04aba64af3bf9fb7e3446f7810/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_66","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"9130ca04aba64af3bf9fb7e3446f7810\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:07:44.297092\", \"model_uuid\": \"e9ba0b5f263d4c139af35788c37dca82\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"2e97c80193674792b8bf7e1637f3bb88","name":"Run_65","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498856820,"endTime":1757498862603,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.5724693740422665,"timestamp":1757498861877,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.55995130553968,"timestamp":1757498861877,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.3190660520365535,"timestamp":1757498861877,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498862086,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.5337984397456499,"timestamp":1757498861877,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9395294117647058,"timestamp":1757498862430,"step":340000,"task":null},{"name":"Recall","value":0.45426829268292684,"timestamp":1757498861732,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30246.7,"timestamp":1757498862086,"step":4,"task":null},{"name":"Test Fairness","value":0.3039664378337147,"timestamp":1757498861877,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498862086,"step":4,"task":null},{"name":"Statistical Parity","value":0.8494836159632794,"timestamp":1757498861877,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.47533333333333333,"timestamp":1757498861877,"step":0,"task":null},{"name":"AUC-ROC","value":0.7886738841878215,"timestamp":1757498861732,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7917647058823529,"timestamp":1757498862520,"step":340000,"task":null},{"name":"Well Calibration","value":0.667665369556801,"timestamp":1757498861877,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40334021991386354,"timestamp":1757498861732,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.062852000002749,"timestamp":1757498862086,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.5546616179291581,"timestamp":1757498861877,"step":0,"task":null},{"name":"Cohen Kappa","value":0.38982911862383773,"timestamp":1757498861732,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3339296362552177,"timestamp":1757498861877,"step":0,"task":null},{"name":"Equalized Odds","value":0.3190660520365535,"timestamp":1757498861877,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.38942937501933084,"timestamp":1757498861877,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498862086,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498862086,"step":4,"task":null},{"name":"Accuracy","value":0.7629350893697083,"timestamp":1757498861732,"step":0,"task":null},{"name":"F1 Score","value":0.5418181818181819,"timestamp":1757498861732,"step":0,"task":null},{"name":"Log Loss","value":0.49742331582449434,"timestamp":1757498861732,"step":0,"task":null},{"name":"Treatment Equality","value":0.9172281659469828,"timestamp":1757498861877,"step":0,"task":null},{"name":"Precision","value":0.6711711711711712,"timestamp":1757498861732,"step":0,"task":null},{"name":"Disparate Impact","value":0.47533333333333333,"timestamp":1757498861877,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":13.3,"timestamp":1757498862086,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.061742000019876,"timestamp":1757498862086,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2e97c80193674792b8bf7e1637f3bb88/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_65","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"2e97c80193674792b8bf7e1637f3bb88\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:07:38.369160\", \"model_uuid\": \"4f8d3d73cb9a47e6ada47828b2eca88f\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"a6ca1a915405434083ac6b6e2201f907","name":"Run_64","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498839450,"endTime":1757498856771,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.22750023650640025,"timestamp":1757498856129,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40212778758489753,"timestamp":1757498856129,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8852916594894074,"timestamp":1757498856129,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498855866,"step":12,"task":null},{"name":"Balance for Negative Class","value":0.9597263792420276,"timestamp":1757498856129,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9618235294117646,"timestamp":1757498856600,"step":340000,"task":null},{"name":"Recall","value":0.6579925650557621,"timestamp":1757498855972,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30225.3,"timestamp":1757498855866,"step":12,"task":null},{"name":"Test Fairness","value":0.2702702702702703,"timestamp":1757498856129,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498855866,"step":12,"task":null},{"name":"Statistical Parity","value":0.7981219600072059,"timestamp":1757498856129,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3276081902047551,"timestamp":1757498856129,"step":0,"task":null},{"name":"AUC-ROC","value":0.9202452407929358,"timestamp":1757498855972,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8508235294117646,"timestamp":1757498856687,"step":340000,"task":null},{"name":"Well Calibration","value":0.38758622094459044,"timestamp":1757498856129,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6265777830729742,"timestamp":1757498855972,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.22473099999479,"timestamp":1757498855866,"step":12,"task":null},{"name":"Equal Negative Predictive Value","value":0.41352144042646627,"timestamp":1757498856129,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6231350265404804,"timestamp":1757498855972,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9728571428571429,"timestamp":1757498856129,"step":0,"task":null},{"name":"Equalized Odds","value":0.8852916594894074,"timestamp":1757498856129,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.10760840849096927,"timestamp":1757498856129,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498855866,"step":12,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498855866,"step":12,"task":null},{"name":"Accuracy","value":0.8645343367826905,"timestamp":1757498855972,"step":0,"task":null},{"name":"F1 Score","value":0.7108433734939759,"timestamp":1757498855972,"step":0,"task":null},{"name":"Log Loss","value":0.304470887161266,"timestamp":1757498855972,"step":0,"task":null},{"name":"Treatment Equality","value":0.21607511449682767,"timestamp":1757498856129,"step":0,"task":null},{"name":"Precision","value":0.7729257641921398,"timestamp":1757498855972,"step":0,"task":null},{"name":"Disparate Impact","value":0.3276081902047551,"timestamp":1757498856129,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.5,"timestamp":1757498855866,"step":12,"task":null},{"name":"system_network_receive_megabytes","value":5.221717000007629,"timestamp":1757498855866,"step":12,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a6ca1a915405434083ac6b6e2201f907/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_64","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"a6ca1a915405434083ac6b6e2201f907\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:07:32.691622\", \"model_uuid\": \"e2ae2e65983d4344805e7d84c32c1b9d\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"64a88f33ea9d413f92c93c91feff10af","name":"Run_63","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498833855,"endTime":1757498839395,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.23223982476695024,"timestamp":1757498838688,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40019734902166737,"timestamp":1757498838688,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8924040928982347,"timestamp":1757498838688,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.6,"timestamp":1757498839263,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.8749991069802777,"timestamp":1757498838688,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9610588235294116,"timestamp":1757498839214,"step":340000,"task":null},{"name":"Recall","value":0.654275092936803,"timestamp":1757498838534,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29988.6,"timestamp":1757498839263,"step":4,"task":null},{"name":"Test Fairness","value":0.3142857142857143,"timestamp":1757498838688,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498839263,"step":4,"task":null},{"name":"Statistical Parity","value":0.8052884615384616,"timestamp":1757498838688,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.34371002132196166,"timestamp":1757498838688,"step":0,"task":null},{"name":"AUC-ROC","value":0.9175695036191511,"timestamp":1757498838534,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8503529411764706,"timestamp":1757498839310,"step":340000,"task":null},{"name":"Well Calibration","value":0.39611146894757665,"timestamp":1757498838688,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6259710488739406,"timestamp":1757498838534,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.067173000017647,"timestamp":1757498839263,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.41074811887619767,"timestamp":1757498838688,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6221582634366931,"timestamp":1757498838534,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9779735682819383,"timestamp":1757498838688,"step":0,"task":null},{"name":"Equalized Odds","value":0.8924040928982347,"timestamp":1757498838688,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.10451466674685389,"timestamp":1757498838688,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498839263,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498839263,"step":4,"task":null},{"name":"Accuracy","value":0.8645343367826905,"timestamp":1757498838534,"step":0,"task":null},{"name":"F1 Score","value":0.7096774193548387,"timestamp":1757498838534,"step":0,"task":null},{"name":"Log Loss","value":0.3647106493122627,"timestamp":1757498838534,"step":0,"task":null},{"name":"Treatment Equality","value":0.2424706540518379,"timestamp":1757498838688,"step":0,"task":null},{"name":"Precision","value":0.775330396475771,"timestamp":1757498838534,"step":0,"task":null},{"name":"Disparate Impact","value":0.34371002132196166,"timestamp":1757498838688,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":4.4,"timestamp":1757498839263,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.065974999975879,"timestamp":1757498839263,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/64a88f33ea9d413f92c93c91feff10af/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_63","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"64a88f33ea9d413f92c93c91feff10af\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:07:15.330780\", \"model_uuid\": \"92840f6ccb7b463bbd516744bad9d4e8\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"4a01c7ae52e644359eca6561643f8d6e","name":"Run_62","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498828254,"endTime":1757498833803,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.5612444843551634,"timestamp":1757498833097,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5609388739797853,"timestamp":1757498833097,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.3211110418116046,"timestamp":1757498833097,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.6,"timestamp":1757498833618,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.5325824979694639,"timestamp":1757498833097,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9415882352941176,"timestamp":1757498833625,"step":340000,"task":null},{"name":"Recall","value":0.45426829268292684,"timestamp":1757498832955,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30004.2,"timestamp":1757498833618,"step":4,"task":null},{"name":"Test Fairness","value":0.3216374269005848,"timestamp":1757498833097,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498833618,"step":4,"task":null},{"name":"Statistical Parity","value":0.8510773938543924,"timestamp":1757498833097,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.4779888268156424,"timestamp":1757498833097,"step":0,"task":null},{"name":"AUC-ROC","value":0.7890098722415795,"timestamp":1757498832955,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7908235294117647,"timestamp":1757498833714,"step":340000,"task":null},{"name":"Well Calibration","value":0.6686245868985252,"timestamp":1757498833097,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40556020508374363,"timestamp":1757498832955,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.066466999996919,"timestamp":1757498833618,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.5548869050433103,"timestamp":1757498833097,"step":0,"task":null},{"name":"Cohen Kappa","value":0.39168648301114195,"timestamp":1757498832955,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3339296362552177,"timestamp":1757498833097,"step":0,"task":null},{"name":"Equalized Odds","value":0.3211110418116046,"timestamp":1757498833097,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.3817935049209126,"timestamp":1757498833097,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498833618,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498833618,"step":4,"task":null},{"name":"Accuracy","value":0.7638758231420508,"timestamp":1757498832955,"step":0,"task":null},{"name":"F1 Score","value":0.5428051001821493,"timestamp":1757498832955,"step":0,"task":null},{"name":"Log Loss","value":0.49745036192937464,"timestamp":1757498832955,"step":0,"task":null},{"name":"Treatment Equality","value":0.9355727292659224,"timestamp":1757498833097,"step":0,"task":null},{"name":"Precision","value":0.6742081447963801,"timestamp":1757498832955,"step":0,"task":null},{"name":"Disparate Impact","value":0.4779888268156424,"timestamp":1757498833097,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":4.1,"timestamp":1757498833618,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.06544499998563,"timestamp":1757498833618,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a01c7ae52e644359eca6561643f8d6e/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_62","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"4a01c7ae52e644359eca6561643f8d6e\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:07:09.786951\", \"model_uuid\": \"8db09d0c20b04d34ad097ff5b70d65f7\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"e06be28941a94caaa6d8568bf9a88f93","name":"Run_61","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498810333,"endTime":1757498828205,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.22750023650640025,"timestamp":1757498827520,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.4014598012267831,"timestamp":1757498827520,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8918209020589185,"timestamp":1757498827520,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.7,"timestamp":1757498827439,"step":13,"task":null},{"name":"Balance for Negative Class","value":0.9474221948927709,"timestamp":1757498827520,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9623529411764705,"timestamp":1757498828034,"step":340000,"task":null},{"name":"Recall","value":0.654275092936803,"timestamp":1757498827338,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30115.9,"timestamp":1757498827439,"step":13,"task":null},{"name":"Test Fairness","value":0.2702702702702703,"timestamp":1757498827520,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498827439,"step":13,"task":null},{"name":"Statistical Parity","value":0.7996306971716808,"timestamp":1757498827520,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.32926277702397105,"timestamp":1757498827520,"step":0,"task":null},{"name":"AUC-ROC","value":0.9204465648497561,"timestamp":1757498827338,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8484705882352941,"timestamp":1757498828120,"step":340000,"task":null},{"name":"Well Calibration","value":0.38805217109667833,"timestamp":1757498827520,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6236366968296819,"timestamp":1757498827338,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.212557999999262,"timestamp":1757498827439,"step":13,"task":null},{"name":"Equal Negative Predictive Value","value":0.4127412112935861,"timestamp":1757498827520,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6200267720456456,"timestamp":1757498827338,"step":0,"task":null},{"name":"Equal Opportunity","value":0.979386385426654,"timestamp":1757498827520,"step":0,"task":null},{"name":"Equalized Odds","value":0.8918209020589185,"timestamp":1757498827520,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.10689101910102945,"timestamp":1757498827520,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498827439,"step":13,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498827439,"step":13,"task":null},{"name":"Accuracy","value":0.863593603010348,"timestamp":1757498827338,"step":0,"task":null},{"name":"F1 Score","value":0.7082494969818913,"timestamp":1757498827338,"step":0,"task":null},{"name":"Log Loss","value":0.3042315876739282,"timestamp":1757498827338,"step":0,"task":null},{"name":"Treatment Equality","value":0.2188812848149683,"timestamp":1757498827520,"step":0,"task":null},{"name":"Precision","value":0.7719298245614035,"timestamp":1757498827338,"step":0,"task":null},{"name":"Disparate Impact","value":0.32926277702397105,"timestamp":1757498827520,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":21.8,"timestamp":1757498827439,"step":13,"task":null},{"name":"system_network_receive_megabytes","value":5.209444000007352,"timestamp":1757498827439,"step":13,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e06be28941a94caaa6d8568bf9a88f93/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_61","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"e06be28941a94caaa6d8568bf9a88f93\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:07:04.167841\", \"model_uuid\": \"350903e0269b4840abe0a9376b7b11b7\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"f7f3b92977a24caeb44476d5c160ed5a","name":"Run_60","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498804696,"endTime":1757498810285,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.23223982476695024,"timestamp":1757498809584,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3995314632495847,"timestamp":1757498809584,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8857961633828164,"timestamp":1757498809584,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757498809984,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.8640616181430243,"timestamp":1757498809584,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9611176470588235,"timestamp":1757498810122,"step":340000,"task":null},{"name":"Recall","value":0.6505576208178439,"timestamp":1757498809452,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30119.2,"timestamp":1757498809984,"step":4,"task":null},{"name":"Test Fairness","value":0.33333333333333337,"timestamp":1757498809584,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498809984,"step":4,"task":null},{"name":"Statistical Parity","value":0.8068021616541353,"timestamp":1757498809584,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3454726368159204,"timestamp":1757498809584,"step":0,"task":null},{"name":"AUC-ROC","value":0.9175741855739608,"timestamp":1757498809452,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8472941176470588,"timestamp":1757498810207,"step":340000,"task":null},{"name":"Well Calibration","value":0.39913158885541816,"timestamp":1757498809584,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6230293537866536,"timestamp":1757498809452,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":65.88238799999817,"timestamp":1757498809984,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.409977484506824,"timestamp":1757498809584,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6190406750420789,"timestamp":1757498809452,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9713656387665199,"timestamp":1757498809584,"step":0,"task":null},{"name":"Equalized Odds","value":0.8857961633828164,"timestamp":1757498809584,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.10380848656613191,"timestamp":1757498809584,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498809984,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498809984,"step":4,"task":null},{"name":"Accuracy","value":0.863593603010348,"timestamp":1757498809452,"step":0,"task":null},{"name":"F1 Score","value":0.7070707070707071,"timestamp":1757498809452,"step":0,"task":null},{"name":"Log Loss","value":0.36474143767460016,"timestamp":1757498809452,"step":0,"task":null},{"name":"Treatment Equality","value":0.24553990283730417,"timestamp":1757498809584,"step":0,"task":null},{"name":"Precision","value":0.7743362831858407,"timestamp":1757498809452,"step":0,"task":null},{"name":"Disparate Impact","value":0.3454726368159204,"timestamp":1757498809584,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":11.2,"timestamp":1757498809984,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":65.80537899999763,"timestamp":1757498809984,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7f3b92977a24caeb44476d5c160ed5a/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_60","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"f7f3b92977a24caeb44476d5c160ed5a\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:06:46.164243\", \"model_uuid\": \"5a1d10106dc9479a992c414488bb8ce5\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"4e683f281e344022959b6d9810fe891f","name":"Run_59","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498799261,"endTime":1757498804648,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.5475794012578202,"timestamp":1757498804077,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5612639993921658,"timestamp":1757498804077,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.32573991797020385,"timestamp":1757498804077,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498803557,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.5370092574283305,"timestamp":1757498804077,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9341176470588234,"timestamp":1757498804488,"step":340000,"task":null},{"name":"Recall","value":0.45121951219512196,"timestamp":1757498803930,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30256.0,"timestamp":1757498803557,"step":3,"task":null},{"name":"Test Fairness","value":0.9117647058823529,"timestamp":1757498804077,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498803557,"step":3,"task":null},{"name":"Statistical Parity","value":0.8538496329223221,"timestamp":1757498804077,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.48936951316839583,"timestamp":1757498804077,"step":0,"task":null},{"name":"AUC-ROC","value":0.7897336983573918,"timestamp":1757498803930,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7927058823529411,"timestamp":1757498804578,"step":340000,"task":null},{"name":"Well Calibration","value":0.6717668228403628,"timestamp":1757498804077,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3983297531229129,"timestamp":1757498803930,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":54.52532300000894,"timestamp":1757498803557,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5543000023108053,"timestamp":1757498804077,"step":0,"task":null},{"name":"Cohen Kappa","value":0.3849864925811697,"timestamp":1757498803930,"step":0,"task":null},{"name":"Equal Opportunity","value":0.33653846153846156,"timestamp":1757498804077,"step":0,"task":null},{"name":"Equalized Odds","value":0.32573991797020385,"timestamp":1757498804077,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.3754078010546402,"timestamp":1757498804077,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498803557,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498803557,"step":3,"task":null},{"name":"Accuracy","value":0.7610536218250236,"timestamp":1757498803930,"step":0,"task":null},{"name":"F1 Score","value":0.5381818181818182,"timestamp":1757498803930,"step":0,"task":null},{"name":"Log Loss","value":0.5070887064062609,"timestamp":1757498803930,"step":0,"task":null},{"name":"Treatment Equality","value":0.9690142537851377,"timestamp":1757498804077,"step":0,"task":null},{"name":"Precision","value":0.6666666666666666,"timestamp":1757498803930,"step":0,"task":null},{"name":"Disparate Impact","value":0.48936951316839583,"timestamp":1757498804077,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":32.6,"timestamp":1757498803557,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":54.40611500001978,"timestamp":1757498803557,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4e683f281e344022959b6d9810fe891f/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_59","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"4e683f281e344022959b6d9810fe891f\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:06:40.719258\", \"model_uuid\": \"d129a35a13bb45ac931c759759913efa\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"927ccfff07544a2dae69e0b894cc3c36","name":"Run_58","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498780189,"endTime":1757498799211,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.2804405431148078,"timestamp":1757498798631,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3987878557943253,"timestamp":1757498798631,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.89244495177677,"timestamp":1757498798631,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498798293,"step":14,"task":null},{"name":"Balance for Negative Class","value":0.8794680476092737,"timestamp":1757498798631,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9558235294117647,"timestamp":1757498799038,"step":340000,"task":null},{"name":"Recall","value":0.6579925650557621,"timestamp":1757498798472,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30234.8,"timestamp":1757498798293,"step":14,"task":null},{"name":"Test Fairness","value":0.38636363636363635,"timestamp":1757498798631,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498798293,"step":14,"task":null},{"name":"Statistical Parity","value":0.7988340042795489,"timestamp":1757498798631,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3442589035022905,"timestamp":1757498798631,"step":0,"task":null},{"name":"AUC-ROC","value":0.9145449608120382,"timestamp":1757498798472,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8510588235294116,"timestamp":1757498799124,"step":340000,"task":null},{"name":"Well Calibration","value":0.38966429993091334,"timestamp":1757498798631,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6151061756656189,"timestamp":1757498798472,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":157.205993999989,"timestamp":1757498798293,"step":14,"task":null},{"name":"Equal Negative Predictive Value","value":0.41091356970157605,"timestamp":1757498798631,"step":0,"task":null},{"name":"Cohen Kappa","value":0.612553418934092,"timestamp":1757498798472,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9845814977973568,"timestamp":1757498798631,"step":0,"task":null},{"name":"Equalized Odds","value":0.89244495177677,"timestamp":1757498798631,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.12705913591254447,"timestamp":1757498798631,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498798293,"step":14,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498798293,"step":14,"task":null},{"name":"Accuracy","value":0.8598306679209784,"timestamp":1757498798472,"step":0,"task":null},{"name":"F1 Score","value":0.7037773359840954,"timestamp":1757498798472,"step":0,"task":null},{"name":"Log Loss","value":0.34070408545451253,"timestamp":1757498798472,"step":0,"task":null},{"name":"Treatment Equality","value":0.28908848937826004,"timestamp":1757498798631,"step":0,"task":null},{"name":"Precision","value":0.7564102564102564,"timestamp":1757498798472,"step":0,"task":null},{"name":"Disparate Impact","value":0.3442589035022905,"timestamp":1757498798631,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":33.6,"timestamp":1757498798293,"step":14,"task":null},{"name":"system_network_receive_megabytes","value":156.69533099999535,"timestamp":1757498798293,"step":14,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/927ccfff07544a2dae69e0b894cc3c36/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_58","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"927ccfff07544a2dae69e0b894cc3c36\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:06:35.549615\", \"model_uuid\": \"2771d065a0b743cea8781c14bd8d9aab\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"ef1e22f6162a40129bfaeb9adca363be","name":"Run_57","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498774743,"endTime":1757498780131,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.4846744169049396,"timestamp":1757498779633,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.4072668008447684,"timestamp":1757498779633,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8846057309090843,"timestamp":1757498779633,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498779032,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.9977782738078028,"timestamp":1757498779633,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9562352941176471,"timestamp":1757498779970,"step":340000,"task":null},{"name":"Recall","value":0.6431226765799256,"timestamp":1757498779500,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30239.0,"timestamp":1757498779032,"step":3,"task":null},{"name":"Test Fairness","value":0.37222222222222223,"timestamp":1757498779633,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498779032,"step":3,"task":null},{"name":"Statistical Parity","value":0.8125022667101875,"timestamp":1757498779633,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3603124274998067,"timestamp":1757498779633,"step":0,"task":null},{"name":"AUC-ROC","value":0.9152566179431237,"timestamp":1757498779500,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8430588235294119,"timestamp":1757498780056,"step":340000,"task":null},{"name":"Well Calibration","value":0.4038437577810718,"timestamp":1757498779633,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6147860238385814,"timestamp":1757498779500,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":65.35082799999509,"timestamp":1757498779032,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.41317284835064644,"timestamp":1757498779633,"step":0,"task":null},{"name":"Cohen Kappa","value":0.610653546892633,"timestamp":1757498779500,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9559442824748947,"timestamp":1757498779633,"step":0,"task":null},{"name":"Equalized Odds","value":0.8846057309090843,"timestamp":1757498779633,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.233308705593447,"timestamp":1757498779633,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498779032,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498779032,"step":3,"task":null},{"name":"Accuracy","value":0.8607714016933208,"timestamp":1757498779500,"step":0,"task":null},{"name":"F1 Score","value":0.7004048582995951,"timestamp":1757498779500,"step":0,"task":null},{"name":"Log Loss","value":0.3686177624485241,"timestamp":1757498779500,"step":0,"task":null},{"name":"Treatment Equality","value":0.44837721387681645,"timestamp":1757498779633,"step":0,"task":null},{"name":"Precision","value":0.7688888888888888,"timestamp":1757498779500,"step":0,"task":null},{"name":"Disparate Impact","value":0.3603124274998067,"timestamp":1757498779633,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":31.5,"timestamp":1757498779032,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":65.29697699999087,"timestamp":1757498779032,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ef1e22f6162a40129bfaeb9adca363be/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_57","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"ef1e22f6162a40129bfaeb9adca363be\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:06:16.127555\", \"model_uuid\": \"6244856375b34f9f9b93e048f40d9c70\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"2aec23fcd7254e77ba751a938d2ab38f","name":"Run_56","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498769359,"endTime":1757498774690,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.5350530750852557,"timestamp":1757498774172,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5615918117914253,"timestamp":1757498774172,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.33040987799153126,"timestamp":1757498774172,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498773619,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.541405992826332,"timestamp":1757498774172,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9353529411764706,"timestamp":1757498774535,"step":340000,"task":null},{"name":"Recall","value":0.4481707317073171,"timestamp":1757498774039,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30222.4,"timestamp":1757498773619,"step":3,"task":null},{"name":"Test Fairness","value":0.4117647058823529,"timestamp":1757498774172,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498773619,"step":3,"task":null},{"name":"Statistical Parity","value":0.856639991199846,"timestamp":1757498774172,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.5007501995211492,"timestamp":1757498774172,"step":0,"task":null},{"name":"AUC-ROC","value":0.7894682263149162,"timestamp":1757498774039,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7898823529411765,"timestamp":1757498774613,"step":340000,"task":null},{"name":"Well Calibration","value":0.6730422482294774,"timestamp":1757498774172,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.39112645665824225,"timestamp":1757498774039,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":52.131778999988455,"timestamp":1757498773619,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5537160937668367,"timestamp":1757498774172,"step":0,"task":null},{"name":"Cohen Kappa","value":0.37829891335267685,"timestamp":1757498774039,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3391883706844337,"timestamp":1757498774172,"step":0,"task":null},{"name":"Equalized Odds","value":0.33040987799153126,"timestamp":1757498774172,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.3697083876785341,"timestamp":1757498774172,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498773619,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498773619,"step":3,"task":null},{"name":"Accuracy","value":0.7582314205079962,"timestamp":1757498774039,"step":0,"task":null},{"name":"F1 Score","value":0.5335753176043557,"timestamp":1757498774039,"step":0,"task":null},{"name":"Log Loss","value":0.5078818222680181,"timestamp":1757498774039,"step":0,"task":null},{"name":"Treatment Equality","value":0.997973697542047,"timestamp":1757498774172,"step":0,"task":null},{"name":"Precision","value":0.6591928251121076,"timestamp":1757498774039,"step":0,"task":null},{"name":"Disparate Impact","value":0.5007501995211492,"timestamp":1757498774172,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":31.4,"timestamp":1757498773619,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":52.10574099997757,"timestamp":1757498773619,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/2aec23fcd7254e77ba751a938d2ab38f/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_56","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"2aec23fcd7254e77ba751a938d2ab38f\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:06:10.707965\", \"model_uuid\": \"ad961ba52d2a42b48a157f301bd5ba2e\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"9517684ae0164e3f8fc506938316d030","name":"Run_55","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498750765,"endTime":1757498769306,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.29143821147225124,"timestamp":1757498768755,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3994558421524398,"timestamp":1757498768755,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8898290382294153,"timestamp":1757498768755,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498769193,"step":14,"task":null},{"name":"Balance for Negative Class","value":0.872212038626964,"timestamp":1757498768755,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9580588235294119,"timestamp":1757498769118,"step":340000,"task":null},{"name":"Recall","value":0.654275092936803,"timestamp":1757498768610,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30256.3,"timestamp":1757498769193,"step":14,"task":null},{"name":"Test Fairness","value":0.38095238095238093,"timestamp":1757498768755,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498769193,"step":14,"task":null},{"name":"Statistical Parity","value":0.8033900917564284,"timestamp":1757498768755,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3494487362184055,"timestamp":1757498768755,"step":0,"task":null},{"name":"AUC-ROC","value":0.914610508179375,"timestamp":1757498768610,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8461176470588235,"timestamp":1757498769221,"step":340000,"task":null},{"name":"Well Calibration","value":0.3892592635537635,"timestamp":1757498768755,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6167068046550759,"timestamp":1757498768610,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":622.2549910000234,"timestamp":1757498769193,"step":14,"task":null},{"name":"Equal Negative Predictive Value","value":0.4104072793527337,"timestamp":1757498768755,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6136652734871888,"timestamp":1757498768610,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9779735682819383,"timestamp":1757498768755,"step":0,"task":null},{"name":"Equalized Odds","value":0.8898290382294153,"timestamp":1757498768755,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.1311556602313461,"timestamp":1757498768755,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498769193,"step":14,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498769193,"step":14,"task":null},{"name":"Accuracy","value":0.8607714016933208,"timestamp":1757498768610,"step":0,"task":null},{"name":"F1 Score","value":0.704,"timestamp":1757498768610,"step":0,"task":null},{"name":"Log Loss","value":0.3403287663877174,"timestamp":1757498768610,"step":0,"task":null},{"name":"Treatment Equality","value":0.3042768992023064,"timestamp":1757498768755,"step":0,"task":null},{"name":"Precision","value":0.7619047619047619,"timestamp":1757498768610,"step":0,"task":null},{"name":"Disparate Impact","value":0.3494487362184055,"timestamp":1757498768755,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":15.2,"timestamp":1757498769193,"step":14,"task":null},{"name":"system_network_receive_megabytes","value":622.1500710000109,"timestamp":1757498769193,"step":14,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/9517684ae0164e3f8fc506938316d030/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_55","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"9517684ae0164e3f8fc506938316d030\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:06:05.528167\", \"model_uuid\": \"24eae225f5c04f65bd5886bbd3d04c80\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"b91eead87aff405ab36e6550e6ddd709","name":"Run_54","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498745171,"endTime":1757498750712,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.4846744169049396,"timestamp":1757498750093,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.4079433237697597,"timestamp":1757498750093,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8781466479193892,"timestamp":1757498750093,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.7,"timestamp":1757498750553,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.9853060453852053,"timestamp":1757498750093,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9562352941176469,"timestamp":1757498750516,"step":340000,"task":null},{"name":"Recall","value":0.6468401486988847,"timestamp":1757498749949,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30117.2,"timestamp":1757498750553,"step":4,"task":null},{"name":"Test Fairness","value":0.36875,"timestamp":1757498750093,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498750553,"step":4,"task":null},{"name":"Statistical Parity","value":0.810983570884561,"timestamp":1757498750093,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.358455146945684,"timestamp":1757498750093,"step":0,"task":null},{"name":"AUC-ROC","value":0.9152659818527432,"timestamp":1757498749949,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8376470588235294,"timestamp":1757498750621,"step":340000,"task":null},{"name":"Well Calibration","value":0.40610042789156137,"timestamp":1757498750093,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6177408852519041,"timestamp":1757498749949,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":140.66077499999665,"timestamp":1757498750553,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.4139465802763967,"timestamp":1757498750093,"step":0,"task":null},{"name":"Cohen Kappa","value":0.613786063663349,"timestamp":1757498749949,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9494851994851996,"timestamp":1757498750093,"step":0,"task":null},{"name":"Equalized Odds","value":0.8781466479193892,"timestamp":1757498750093,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.23489583964510308,"timestamp":1757498750093,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498750553,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498750553,"step":4,"task":null},{"name":"Accuracy","value":0.8617121354656632,"timestamp":1757498749949,"step":0,"task":null},{"name":"F1 Score","value":0.703030303030303,"timestamp":1757498749949,"step":0,"task":null},{"name":"Log Loss","value":0.3681893703910613,"timestamp":1757498749949,"step":0,"task":null},{"name":"Treatment Equality","value":0.44277249870335605,"timestamp":1757498750093,"step":0,"task":null},{"name":"Precision","value":0.7699115044247787,"timestamp":1757498749949,"step":0,"task":null},{"name":"Disparate Impact","value":0.358455146945684,"timestamp":1757498750093,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":13.3,"timestamp":1757498750553,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":140.6142300000065,"timestamp":1757498750553,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b91eead87aff405ab36e6550e6ddd709/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_54","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"b91eead87aff405ab36e6550e6ddd709\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:05:46.542076\", \"model_uuid\": \"d622f888129a4c16bf4b3a29f12c3b6b\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"a3edf60e1a394cb086d4940130b2bbf7","name":"Run_53","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498739273,"endTime":1757498745123,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498744420,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5348285279048074,"timestamp":1757498744420,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.03476482617586907,"timestamp":1757498744420,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498744587,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.6937613498789346,"timestamp":1757498744420,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7498235294117647,"timestamp":1757498744950,"step":340000,"task":null},{"name":"Recall","value":0.22560975609756098,"timestamp":1757498744318,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30263.9,"timestamp":1757498744587,"step":4,"task":null},{"name":"Test Fairness","value":0.24811632518175808,"timestamp":1757498744420,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498744587,"step":4,"task":null},{"name":"Statistical Parity","value":0.8723702664796634,"timestamp":1757498744420,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.0,"timestamp":1757498744420,"step":0,"task":null},{"name":"AUC-ROC","value":0.7712543554006969,"timestamp":1757498744318,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7465882352941178,"timestamp":1757498745040,"step":340000,"task":null},{"name":"Well Calibration","value":0.8084969412224796,"timestamp":1757498744420,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.33427967926023333,"timestamp":1757498744318,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":144.86979900000733,"timestamp":1757498744587,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.5299843747187151,"timestamp":1757498744420,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2531183481505105,"timestamp":1757498744318,"step":0,"task":null},{"name":"Equal Opportunity","value":0.0,"timestamp":1757498744420,"step":0,"task":null},{"name":"Equalized Odds","value":-0.03476482617586907,"timestamp":1757498744420,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498744420,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498744587,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498744587,"step":4,"task":null},{"name":"Accuracy","value":0.7450611476952023,"timestamp":1757498744318,"step":0,"task":null},{"name":"F1 Score","value":0.3532219570405728,"timestamp":1757498744318,"step":0,"task":null},{"name":"Log Loss","value":0.533890463627045,"timestamp":1757498744318,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498744420,"step":0,"task":null},{"name":"Precision","value":0.8131868131868132,"timestamp":1757498744318,"step":0,"task":null},{"name":"Disparate Impact","value":0.0,"timestamp":1757498744420,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":26.3,"timestamp":1757498744587,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":144.8230620000104,"timestamp":1757498744587,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a3edf60e1a394cb086d4940130b2bbf7/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_53","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"a3edf60e1a394cb086d4940130b2bbf7\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:05:40.900959\", \"model_uuid\": \"0fc22ddea1af4c46be77489b982cbbd6\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"92126c0d100749cb8672c33f2e4e46b4","name":"Run_52","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498721335,"endTime":1757498739215,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498738468,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3954138430671146,"timestamp":1757498738468,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.10830525537845585,"timestamp":1757498738468,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498738353,"step":13,"task":null},{"name":"Balance for Negative Class","value":0.8048728913720855,"timestamp":1757498738468,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7817647058823529,"timestamp":1757498739037,"step":340000,"task":null},{"name":"Recall","value":0.18587360594795538,"timestamp":1757498738320,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30237.3,"timestamp":1757498738353,"step":13,"task":null},{"name":"Test Fairness","value":0.508029197080292,"timestamp":1757498738468,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498738353,"step":13,"task":null},{"name":"Statistical Parity","value":0.9341070606040666,"timestamp":1757498738468,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.04346268656716418,"timestamp":1757498738468,"step":0,"task":null},{"name":"AUC-ROC","value":0.8821083778899367,"timestamp":1757498738320,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7816470588235294,"timestamp":1757498739125,"step":340000,"task":null},{"name":"Well Calibration","value":0.494019931469424,"timestamp":1757498738468,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.37555716822497986,"timestamp":1757498738320,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":412.1447240000125,"timestamp":1757498738353,"step":13,"task":null},{"name":"Equal Negative Predictive Value","value":0.3868410778163413,"timestamp":1757498738468,"step":0,"task":null},{"name":"Cohen Kappa","value":0.252179251859479,"timestamp":1757498738320,"step":0,"task":null},{"name":"Equal Opportunity","value":0.11030126336248784,"timestamp":1757498738468,"step":0,"task":null},{"name":"Equalized Odds","value":0.10830525537845585,"timestamp":1757498738468,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498738468,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498738353,"step":13,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498738353,"step":13,"task":null},{"name":"Accuracy","value":0.793038570084666,"timestamp":1757498738320,"step":0,"task":null},{"name":"F1 Score","value":0.3125,"timestamp":1757498738320,"step":0,"task":null},{"name":"Log Loss","value":0.4242426023011633,"timestamp":1757498738320,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498738468,"step":0,"task":null},{"name":"Precision","value":0.9803921568627451,"timestamp":1757498738320,"step":0,"task":null},{"name":"Disparate Impact","value":0.04346268656716418,"timestamp":1757498738468,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":30.8,"timestamp":1757498738353,"step":13,"task":null},{"name":"system_network_receive_megabytes","value":411.3913870000106,"timestamp":1757498738353,"step":13,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/92126c0d100749cb8672c33f2e4e46b4/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_52","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"92126c0d100749cb8672c33f2e4e46b4\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:05:35.512637\", \"model_uuid\": \"35a5262e2405484982b3d5911d22c9be\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"e24bb06c38c3404f821974ef3a577ece","name":"Run_51","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498715429,"endTime":1757498721281,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498720510,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3954138430671146,"timestamp":1757498720510,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.10830525537845585,"timestamp":1757498720510,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498720729,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.8048728913720855,"timestamp":1757498720510,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.784235294117647,"timestamp":1757498721106,"step":340000,"task":null},{"name":"Recall","value":0.18587360594795538,"timestamp":1757498720357,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30246.3,"timestamp":1757498720729,"step":4,"task":null},{"name":"Test Fairness","value":0.508029197080292,"timestamp":1757498720510,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498720729,"step":4,"task":null},{"name":"Statistical Parity","value":0.9341070606040666,"timestamp":1757498720510,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.04346268656716418,"timestamp":1757498720510,"step":0,"task":null},{"name":"AUC-ROC","value":0.8824244098395964,"timestamp":1757498720357,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7802352941176471,"timestamp":1757498721196,"step":340000,"task":null},{"name":"Well Calibration","value":0.4939147390475009,"timestamp":1757498720510,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.37555716822497986,"timestamp":1757498720357,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":100.48164399998495,"timestamp":1757498720729,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.3868410778163413,"timestamp":1757498720510,"step":0,"task":null},{"name":"Cohen Kappa","value":0.252179251859479,"timestamp":1757498720357,"step":0,"task":null},{"name":"Equal Opportunity","value":0.11030126336248784,"timestamp":1757498720510,"step":0,"task":null},{"name":"Equalized Odds","value":0.10830525537845585,"timestamp":1757498720510,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498720510,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498720729,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498720729,"step":4,"task":null},{"name":"Accuracy","value":0.793038570084666,"timestamp":1757498720357,"step":0,"task":null},{"name":"F1 Score","value":0.3125,"timestamp":1757498720357,"step":0,"task":null},{"name":"Log Loss","value":0.4241863453611334,"timestamp":1757498720357,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498720510,"step":0,"task":null},{"name":"Precision","value":0.9803921568627451,"timestamp":1757498720357,"step":0,"task":null},{"name":"Disparate Impact","value":0.04346268656716418,"timestamp":1757498720510,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":22.5,"timestamp":1757498720729,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":100.38374099999783,"timestamp":1757498720729,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e24bb06c38c3404f821974ef3a577ece/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_51","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"e24bb06c38c3404f821974ef3a577ece\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:05:16.903027\", \"model_uuid\": \"c2fd97fc5f9442c98033df4502aeb1b7\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"f7a8c8b5864241e996c1fbe0c33e5573","name":"Run_50","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498709657,"endTime":1757498715376,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498714570,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5348285279048074,"timestamp":1757498714570,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.03476482617586907,"timestamp":1757498714570,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498715015,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.6937613498789346,"timestamp":1757498714570,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7524117647058823,"timestamp":1757498715196,"step":340000,"task":null},{"name":"Recall","value":0.22560975609756098,"timestamp":1757498714445,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30230.7,"timestamp":1757498715015,"step":4,"task":null},{"name":"Test Fairness","value":0.24811632518175808,"timestamp":1757498714570,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498715015,"step":4,"task":null},{"name":"Statistical Parity","value":0.8723702664796634,"timestamp":1757498714570,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.0,"timestamp":1757498714570,"step":0,"task":null},{"name":"AUC-ROC","value":0.7712294673967148,"timestamp":1757498714445,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7468235294117647,"timestamp":1757498715289,"step":340000,"task":null},{"name":"Well Calibration","value":0.8084637679741107,"timestamp":1757498714570,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.33427967926023333,"timestamp":1757498714445,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":119.14730199999758,"timestamp":1757498715015,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.5299843747187151,"timestamp":1757498714570,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2531183481505105,"timestamp":1757498714445,"step":0,"task":null},{"name":"Equal Opportunity","value":0.0,"timestamp":1757498714570,"step":0,"task":null},{"name":"Equalized Odds","value":-0.03476482617586907,"timestamp":1757498714570,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498714570,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498715015,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498715015,"step":4,"task":null},{"name":"Accuracy","value":0.7450611476952023,"timestamp":1757498714445,"step":0,"task":null},{"name":"F1 Score","value":0.3532219570405728,"timestamp":1757498714445,"step":0,"task":null},{"name":"Log Loss","value":0.5338962989931526,"timestamp":1757498714445,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498714570,"step":0,"task":null},{"name":"Precision","value":0.8131868131868132,"timestamp":1757498714445,"step":0,"task":null},{"name":"Disparate Impact","value":0.0,"timestamp":1757498714570,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":16.1,"timestamp":1757498715015,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":119.06098999999813,"timestamp":1757498715015,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f7a8c8b5864241e996c1fbe0c33e5573/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_50","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"f7a8c8b5864241e996c1fbe0c33e5573\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:05:11.156604\", \"model_uuid\": \"6b26532dc55b4126ba51b6230eaf93fc\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"f714b4ac4a36475fb86ced9fbcae8d80","name":"Run_49","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498691842,"endTime":1757498709604,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498708947,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3954138430671146,"timestamp":1757498708947,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.10830525537845585,"timestamp":1757498708947,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498709207,"step":13,"task":null},{"name":"Balance for Negative Class","value":0.8048728913720855,"timestamp":1757498708947,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7866470588235295,"timestamp":1757498709453,"step":340000,"task":null},{"name":"Recall","value":0.18587360594795538,"timestamp":1757498708843,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30232.4,"timestamp":1757498709207,"step":13,"task":null},{"name":"Test Fairness","value":0.508029197080292,"timestamp":1757498708947,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498709207,"step":13,"task":null},{"name":"Statistical Parity","value":0.9341070606040666,"timestamp":1757498708947,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.04346268656716418,"timestamp":1757498708947,"step":0,"task":null},{"name":"AUC-ROC","value":0.8821083778899367,"timestamp":1757498708843,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7837647058823529,"timestamp":1757498709527,"step":340000,"task":null},{"name":"Well Calibration","value":0.494019931469424,"timestamp":1757498708947,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.37555716822497986,"timestamp":1757498708843,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":493.6024090000137,"timestamp":1757498709207,"step":13,"task":null},{"name":"Equal Negative Predictive Value","value":0.3868410778163413,"timestamp":1757498708947,"step":0,"task":null},{"name":"Cohen Kappa","value":0.252179251859479,"timestamp":1757498708843,"step":0,"task":null},{"name":"Equal Opportunity","value":0.11030126336248784,"timestamp":1757498708947,"step":0,"task":null},{"name":"Equalized Odds","value":0.10830525537845585,"timestamp":1757498708947,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498708947,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498709207,"step":13,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498709207,"step":13,"task":null},{"name":"Accuracy","value":0.793038570084666,"timestamp":1757498708843,"step":0,"task":null},{"name":"F1 Score","value":0.3125,"timestamp":1757498708843,"step":0,"task":null},{"name":"Log Loss","value":0.4242426023011633,"timestamp":1757498708843,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498708947,"step":0,"task":null},{"name":"Precision","value":0.9803921568627451,"timestamp":1757498708843,"step":0,"task":null},{"name":"Disparate Impact","value":0.04346268656716418,"timestamp":1757498708947,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":19.5,"timestamp":1757498709207,"step":13,"task":null},{"name":"system_network_receive_megabytes","value":493.4835520000197,"timestamp":1757498709207,"step":13,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f714b4ac4a36475fb86ced9fbcae8d80/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_49","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"f714b4ac4a36475fb86ced9fbcae8d80\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:05:05.604585\", \"model_uuid\": \"00335e2571194e7cabfc6a608460fab2\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"dd89d577d47c4d2fa5982ad5c2126590","name":"Run_48","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498685977,"endTime":1757498691788,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498691026,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3954138430671146,"timestamp":1757498691026,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.10830525537845585,"timestamp":1757498691026,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498691243,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.8048728913720855,"timestamp":1757498691026,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7871176470588235,"timestamp":1757498691608,"step":340000,"task":null},{"name":"Recall","value":0.18587360594795538,"timestamp":1757498690896,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30241.5,"timestamp":1757498691243,"step":4,"task":null},{"name":"Test Fairness","value":0.508029197080292,"timestamp":1757498691026,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498691243,"step":4,"task":null},{"name":"Statistical Parity","value":0.9341070606040666,"timestamp":1757498691026,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.04346268656716418,"timestamp":1757498691026,"step":0,"task":null},{"name":"AUC-ROC","value":0.8824244098395964,"timestamp":1757498690896,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7858823529411765,"timestamp":1757498691698,"step":340000,"task":null},{"name":"Well Calibration","value":0.4939147390475009,"timestamp":1757498691026,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.37555716822497986,"timestamp":1757498690896,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":142.0660440000065,"timestamp":1757498691243,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.3868410778163413,"timestamp":1757498691026,"step":0,"task":null},{"name":"Cohen Kappa","value":0.252179251859479,"timestamp":1757498690896,"step":0,"task":null},{"name":"Equal Opportunity","value":0.11030126336248784,"timestamp":1757498691026,"step":0,"task":null},{"name":"Equalized Odds","value":0.10830525537845585,"timestamp":1757498691026,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498691026,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498691243,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498691243,"step":4,"task":null},{"name":"Accuracy","value":0.793038570084666,"timestamp":1757498690896,"step":0,"task":null},{"name":"F1 Score","value":0.3125,"timestamp":1757498690896,"step":0,"task":null},{"name":"Log Loss","value":0.4241863453611334,"timestamp":1757498690896,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498691026,"step":0,"task":null},{"name":"Precision","value":0.9803921568627451,"timestamp":1757498690896,"step":0,"task":null},{"name":"Disparate Impact","value":0.04346268656716418,"timestamp":1757498691026,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":31.1,"timestamp":1757498691243,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":142.01777899998706,"timestamp":1757498691243,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/dd89d577d47c4d2fa5982ad5c2126590/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_48","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"dd89d577d47c4d2fa5982ad5c2126590\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:04:47.433231\", \"model_uuid\": \"466b69e5d67942b3a159adfa4650da32\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"e162f5aa1dec4414aaf296afc2b61238","name":"Run_47","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498680732,"endTime":1757498685954,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498685445,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5297365091758119,"timestamp":1757498685445,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.006895286036390343,"timestamp":1757498685445,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757498684980,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.7113439402104297,"timestamp":1757498685445,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7506470588235294,"timestamp":1757498685827,"step":340000,"task":null},{"name":"Recall","value":0.2225609756097561,"timestamp":1757498685333,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30207.6,"timestamp":1757498684980,"step":3,"task":null},{"name":"Test Fairness","value":0.25,"timestamp":1757498685445,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498684980,"step":3,"task":null},{"name":"Statistical Parity","value":0.8762764379895273,"timestamp":1757498685445,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.022634920634920636,"timestamp":1757498685445,"step":0,"task":null},{"name":"AUC-ROC","value":0.7699477351916375,"timestamp":1757498685333,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7487058823529413,"timestamp":1757498685914,"step":340000,"task":null},{"name":"Well Calibration","value":0.8010260003678731,"timestamp":1757498685445,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.32700022461497524,"timestamp":1757498685333,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":112.23522500001127,"timestamp":1757498684980,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5265040240423994,"timestamp":1757498685445,"step":0,"task":null},{"name":"Cohen Kappa","value":0.24760630643944415,"timestamp":1757498685333,"step":0,"task":null},{"name":"Equal Opportunity","value":0.029914529914529912,"timestamp":1757498685445,"step":0,"task":null},{"name":"Equalized Odds","value":-0.006895286036390343,"timestamp":1757498685445,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498685445,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498684980,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498684980,"step":3,"task":null},{"name":"Accuracy","value":0.7431796801505174,"timestamp":1757498685333,"step":0,"task":null},{"name":"F1 Score","value":0.34844868735083534,"timestamp":1757498685333,"step":0,"task":null},{"name":"Log Loss","value":0.5331006643293007,"timestamp":1757498685333,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498685445,"step":0,"task":null},{"name":"Precision","value":0.8021978021978022,"timestamp":1757498685333,"step":0,"task":null},{"name":"Disparate Impact","value":0.022634920634920636,"timestamp":1757498685445,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":31.9,"timestamp":1757498684980,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":111.73994800000219,"timestamp":1757498684980,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e162f5aa1dec4414aaf296afc2b61238/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_47","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"e162f5aa1dec4414aaf296afc2b61238\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:04:42.143353\", \"model_uuid\": \"6d67791f94574ec2ac777bfb07721119\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"3edb18a2d6174456b48eb98cdb8615ff","name":"Run_46","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498662375,"endTime":1757498680679,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498680112,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3996541862246443,"timestamp":1757498680112,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.009980039920159722,"timestamp":1757498680112,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498680511,"step":14,"task":null},{"name":"Balance for Negative Class","value":0.7564480602529489,"timestamp":1757498680112,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7911176470588235,"timestamp":1757498680515,"step":340000,"task":null},{"name":"Recall","value":0.21189591078066913,"timestamp":1757498679992,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30235.9,"timestamp":1757498680511,"step":14,"task":null},{"name":"Test Fairness","value":0.4511070110701107,"timestamp":1757498680112,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498680511,"step":14,"task":null},{"name":"Statistical Parity","value":0.9148351648351648,"timestamp":1757498680112,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.0,"timestamp":1757498680112,"step":0,"task":null},{"name":"AUC-ROC","value":0.8823729083366887,"timestamp":1757498679992,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7854117647058823,"timestamp":1757498680597,"step":340000,"task":null},{"name":"Well Calibration","value":0.5058103510487548,"timestamp":1757498680112,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.38141154362707635,"timestamp":1757498679992,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":585.9262400000007,"timestamp":1757498680511,"step":14,"task":null},{"name":"Equal Negative Predictive Value","value":0.39183034063580136,"timestamp":1757498680112,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2757514199505804,"timestamp":1757498679992,"step":0,"task":null},{"name":"Equal Opportunity","value":0.0,"timestamp":1757498680112,"step":0,"task":null},{"name":"Equalized Odds","value":-0.009980039920159722,"timestamp":1757498680112,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498680112,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498680511,"step":14,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498680511,"step":14,"task":null},{"name":"Accuracy","value":0.7958607714016933,"timestamp":1757498679992,"step":0,"task":null},{"name":"F1 Score","value":0.34441087613293053,"timestamp":1757498679992,"step":0,"task":null},{"name":"Log Loss","value":0.42216810085448053,"timestamp":1757498679992,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498680112,"step":0,"task":null},{"name":"Precision","value":0.9193548387096774,"timestamp":1757498679992,"step":0,"task":null},{"name":"Disparate Impact","value":0.0,"timestamp":1757498680112,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":20.0,"timestamp":1757498680511,"step":14,"task":null},{"name":"system_network_receive_megabytes","value":585.8463210000191,"timestamp":1757498680511,"step":14,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/3edb18a2d6174456b48eb98cdb8615ff/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_46","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"3edb18a2d6174456b48eb98cdb8615ff\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:04:36.722116\", \"model_uuid\": \"27c9aa5a5f8e4ce59d69b355f4594de6\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"e8f52be50634426a8705b5a34a018117","name":"Run_45","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498656663,"endTime":1757498662322,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498661720,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3996541862246443,"timestamp":1757498661720,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.009980039920159722,"timestamp":1757498661720,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757498661985,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.7564480602529489,"timestamp":1757498661720,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7894117647058824,"timestamp":1757498662145,"step":340000,"task":null},{"name":"Recall","value":0.21189591078066913,"timestamp":1757498661592,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30219.7,"timestamp":1757498661985,"step":4,"task":null},{"name":"Test Fairness","value":0.4511070110701107,"timestamp":1757498661720,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498661985,"step":4,"task":null},{"name":"Statistical Parity","value":0.9148351648351648,"timestamp":1757498661720,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.0,"timestamp":1757498661720,"step":0,"task":null},{"name":"AUC-ROC","value":0.8821294466865807,"timestamp":1757498661592,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7868235294117647,"timestamp":1757498662236,"step":340000,"task":null},{"name":"Well Calibration","value":0.5056505239113448,"timestamp":1757498661720,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.38141154362707635,"timestamp":1757498661592,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":185.96710100001656,"timestamp":1757498661985,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.39183034063580136,"timestamp":1757498661720,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2757514199505804,"timestamp":1757498661592,"step":0,"task":null},{"name":"Equal Opportunity","value":0.0,"timestamp":1757498661720,"step":0,"task":null},{"name":"Equalized Odds","value":-0.009980039920159722,"timestamp":1757498661720,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498661720,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498661985,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498661985,"step":4,"task":null},{"name":"Accuracy","value":0.7958607714016933,"timestamp":1757498661592,"step":0,"task":null},{"name":"F1 Score","value":0.34441087613293053,"timestamp":1757498661592,"step":0,"task":null},{"name":"Log Loss","value":0.4221140018000568,"timestamp":1757498661592,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498661720,"step":0,"task":null},{"name":"Precision","value":0.9193548387096774,"timestamp":1757498661592,"step":0,"task":null},{"name":"Disparate Impact","value":0.0,"timestamp":1757498661720,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":47.7,"timestamp":1757498661985,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":185.96325400000205,"timestamp":1757498661985,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8f52be50634426a8705b5a34a018117/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_45","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"e8f52be50634426a8705b5a34a018117\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:04:18.065729\", \"model_uuid\": \"34caaf4f808a4593a178c10a54d23808\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"525bb57b7c8d4dc6ad3ce05f340cc94d","name":"Run_44","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498651099,"endTime":1757498656613,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498656000,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5297365091758119,"timestamp":1757498656000,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.006895286036390343,"timestamp":1757498656000,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.9,"timestamp":1757498656333,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.7113439402104297,"timestamp":1757498656000,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7460000000000001,"timestamp":1757498656446,"step":340000,"task":null},{"name":"Recall","value":0.2225609756097561,"timestamp":1757498655873,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30232.2,"timestamp":1757498656333,"step":4,"task":null},{"name":"Test Fairness","value":0.25,"timestamp":1757498656000,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498656333,"step":4,"task":null},{"name":"Statistical Parity","value":0.8762764379895273,"timestamp":1757498656000,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.022634920634920636,"timestamp":1757498656000,"step":0,"task":null},{"name":"AUC-ROC","value":0.7699269951883192,"timestamp":1757498655873,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.744235294117647,"timestamp":1757498656535,"step":340000,"task":null},{"name":"Well Calibration","value":0.800959533110023,"timestamp":1757498656000,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.32700022461497524,"timestamp":1757498655873,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":70.0574960000231,"timestamp":1757498656333,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.5265040240423994,"timestamp":1757498656000,"step":0,"task":null},{"name":"Cohen Kappa","value":0.24760630643944415,"timestamp":1757498655873,"step":0,"task":null},{"name":"Equal Opportunity","value":0.029914529914529912,"timestamp":1757498656000,"step":0,"task":null},{"name":"Equalized Odds","value":-0.006895286036390343,"timestamp":1757498656000,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498656000,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498656333,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498656333,"step":4,"task":null},{"name":"Accuracy","value":0.7431796801505174,"timestamp":1757498655873,"step":0,"task":null},{"name":"F1 Score","value":0.34844868735083534,"timestamp":1757498655873,"step":0,"task":null},{"name":"Log Loss","value":0.5331125784417615,"timestamp":1757498655873,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498656000,"step":0,"task":null},{"name":"Precision","value":0.8021978021978022,"timestamp":1757498655873,"step":0,"task":null},{"name":"Disparate Impact","value":0.022634920634920636,"timestamp":1757498656000,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":13.1,"timestamp":1757498656333,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":70.04139999998733,"timestamp":1757498656333,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/525bb57b7c8d4dc6ad3ce05f340cc94d/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_44","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"525bb57b7c8d4dc6ad3ce05f340cc94d\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:04:12.473847\", \"model_uuid\": \"3dce1f9583944db083b66ad38a64beb6\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"ae3e7132e686406ea51d013d26c011ef","name":"Run_43","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498631967,"endTime":1757498651045,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498650425,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3996541862246443,"timestamp":1757498650425,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.009980039920159722,"timestamp":1757498650425,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757498650256,"step":14,"task":null},{"name":"Balance for Negative Class","value":0.7564480602529489,"timestamp":1757498650425,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7861176470588236,"timestamp":1757498650870,"step":340000,"task":null},{"name":"Recall","value":0.21189591078066913,"timestamp":1757498650302,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30191.8,"timestamp":1757498650256,"step":14,"task":null},{"name":"Test Fairness","value":0.4511070110701107,"timestamp":1757498650425,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498650256,"step":14,"task":null},{"name":"Statistical Parity","value":0.9148351648351648,"timestamp":1757498650425,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.0,"timestamp":1757498650425,"step":0,"task":null},{"name":"AUC-ROC","value":0.8823729083366887,"timestamp":1757498650302,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7807058823529411,"timestamp":1757498650962,"step":340000,"task":null},{"name":"Well Calibration","value":0.5058103510487548,"timestamp":1757498650425,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.38141154362707635,"timestamp":1757498650302,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":145.82935700000962,"timestamp":1757498650256,"step":14,"task":null},{"name":"Equal Negative Predictive Value","value":0.39183034063580136,"timestamp":1757498650425,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2757514199505804,"timestamp":1757498650302,"step":0,"task":null},{"name":"Equal Opportunity","value":0.0,"timestamp":1757498650425,"step":0,"task":null},{"name":"Equalized Odds","value":-0.009980039920159722,"timestamp":1757498650425,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498650425,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498650256,"step":14,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498650256,"step":14,"task":null},{"name":"Accuracy","value":0.7958607714016933,"timestamp":1757498650302,"step":0,"task":null},{"name":"F1 Score","value":0.34441087613293053,"timestamp":1757498650302,"step":0,"task":null},{"name":"Log Loss","value":0.42216810085448053,"timestamp":1757498650302,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498650425,"step":0,"task":null},{"name":"Precision","value":0.9193548387096774,"timestamp":1757498650302,"step":0,"task":null},{"name":"Disparate Impact","value":0.0,"timestamp":1757498650425,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":31.5,"timestamp":1757498650256,"step":14,"task":null},{"name":"system_network_receive_megabytes","value":145.28642699998454,"timestamp":1757498650256,"step":14,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ae3e7132e686406ea51d013d26c011ef/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_43","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"ae3e7132e686406ea51d013d26c011ef\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:04:07.194677\", \"model_uuid\": \"f2d42300e97f4b5a9d73e47f395f71d2\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"a76856440bb8445a8754ae4f48f06013","name":"Run_42","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498626332,"endTime":1757498631913,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498631329,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3996541862246443,"timestamp":1757498631329,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.009980039920159722,"timestamp":1757498631329,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757498631581,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.7564480602529489,"timestamp":1757498631329,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7902941176470588,"timestamp":1757498631749,"step":340000,"task":null},{"name":"Recall","value":0.21189591078066913,"timestamp":1757498631195,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30185.0,"timestamp":1757498631581,"step":4,"task":null},{"name":"Test Fairness","value":0.4511070110701107,"timestamp":1757498631329,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498631581,"step":4,"task":null},{"name":"Statistical Parity","value":0.9148351648351648,"timestamp":1757498631329,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.0,"timestamp":1757498631329,"step":0,"task":null},{"name":"AUC-ROC","value":0.8821294466865807,"timestamp":1757498631195,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7903529411764707,"timestamp":1757498631823,"step":340000,"task":null},{"name":"Well Calibration","value":0.5056505239113448,"timestamp":1757498631329,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.38141154362707635,"timestamp":1757498631195,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":51.064512000011746,"timestamp":1757498631581,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.39183034063580136,"timestamp":1757498631329,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2757514199505804,"timestamp":1757498631195,"step":0,"task":null},{"name":"Equal Opportunity","value":0.0,"timestamp":1757498631329,"step":0,"task":null},{"name":"Equalized Odds","value":-0.009980039920159722,"timestamp":1757498631329,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498631329,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498631581,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498631581,"step":4,"task":null},{"name":"Accuracy","value":0.7958607714016933,"timestamp":1757498631195,"step":0,"task":null},{"name":"F1 Score","value":0.34441087613293053,"timestamp":1757498631195,"step":0,"task":null},{"name":"Log Loss","value":0.4221140018000568,"timestamp":1757498631195,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498631329,"step":0,"task":null},{"name":"Precision","value":0.9193548387096774,"timestamp":1757498631195,"step":0,"task":null},{"name":"Disparate Impact","value":0.0,"timestamp":1757498631329,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":14.2,"timestamp":1757498631581,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":51.00713199999882,"timestamp":1757498631581,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a76856440bb8445a8754ae4f48f06013/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_42","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"a76856440bb8445a8754ae4f48f06013\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:03:47.685985\", \"model_uuid\": \"2da7717729334960afa93c3f2ec0d168\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"7f26e50c7dba462e832be5e26dc7efdb","name":"Run_41","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498620967,"endTime":1757498626277,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498625759,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.548004913119052,"timestamp":1757498625759,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.012982446399985206,"timestamp":1757498625759,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757498625222,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.5877760194987086,"timestamp":1757498625759,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.748,"timestamp":1757498626112,"step":340000,"task":null},{"name":"Recall","value":0.3170731707317073,"timestamp":1757498625650,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30174.9,"timestamp":1757498625222,"step":3,"task":null},{"name":"Test Fairness","value":0.24619240408714094,"timestamp":1757498625759,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498625222,"step":3,"task":null},{"name":"Statistical Parity","value":0.8237816575583177,"timestamp":1757498625759,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.03158361018826135,"timestamp":1757498625759,"step":0,"task":null},{"name":"AUC-ROC","value":0.7609714617554338,"timestamp":1757498625650,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7454117647058823,"timestamp":1757498626201,"step":340000,"task":null},{"name":"Well Calibration","value":0.7837846708349147,"timestamp":1757498625759,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.39393058117604535,"timestamp":1757498625650,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":84.8040800000017,"timestamp":1757498625222,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5493533532426528,"timestamp":1757498625759,"step":0,"task":null},{"name":"Cohen Kappa","value":0.336254698605158,"timestamp":1757498625650,"step":0,"task":null},{"name":"Equal Opportunity","value":0.042232277526395176,"timestamp":1757498625759,"step":0,"task":null},{"name":"Equalized Odds","value":-0.012982446399985206,"timestamp":1757498625759,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498625759,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498625222,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498625222,"step":3,"task":null},{"name":"Accuracy","value":0.7638758231420508,"timestamp":1757498625650,"step":0,"task":null},{"name":"F1 Score","value":0.4531590413943355,"timestamp":1757498625650,"step":0,"task":null},{"name":"Log Loss","value":0.5298051861747594,"timestamp":1757498625650,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498625759,"step":0,"task":null},{"name":"Precision","value":0.7938931297709924,"timestamp":1757498625650,"step":0,"task":null},{"name":"Disparate Impact","value":0.03158361018826135,"timestamp":1757498625759,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":34.7,"timestamp":1757498625222,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":84.72910600001342,"timestamp":1757498625222,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7f26e50c7dba462e832be5e26dc7efdb/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_41","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"7f26e50c7dba462e832be5e26dc7efdb\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:03:42.399751\", \"model_uuid\": \"fb69c2be352d4ba0b11e74807babac89\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"d4bb6e9eb62f4c8f943713e42f044249","name":"Run_40","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498602511,"endTime":1757498620913,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498620370,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.39909807318775464,"timestamp":1757498620370,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.18564875262007313,"timestamp":1757498620370,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757498620698,"step":14,"task":null},{"name":"Balance for Negative Class","value":0.789495290139751,"timestamp":1757498620370,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7885882352941176,"timestamp":1757498620756,"step":340000,"task":null},{"name":"Recall","value":0.21933085501858737,"timestamp":1757498620226,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30207.8,"timestamp":1757498620698,"step":14,"task":null},{"name":"Test Fairness","value":0.3688725490196078,"timestamp":1757498620370,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498620698,"step":14,"task":null},{"name":"Statistical Parity","value":0.9244752994752995,"timestamp":1757498620370,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.07366557045282064,"timestamp":1757498620370,"step":0,"task":null},{"name":"AUC-ROC","value":0.8951897596284402,"timestamp":1757498620226,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7837647058823529,"timestamp":1757498620822,"step":340000,"task":null},{"name":"Well Calibration","value":0.5084309095741382,"timestamp":1757498620370,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40529463521718573,"timestamp":1757498620226,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":162.07530599998427,"timestamp":1757498620698,"step":14,"task":null},{"name":"Equal Negative Predictive Value","value":0.39008968693957746,"timestamp":1757498620370,"step":0,"task":null},{"name":"Cohen Kappa","value":0.29127092951580635,"timestamp":1757498620226,"step":0,"task":null},{"name":"Equal Opportunity","value":0.189640768588137,"timestamp":1757498620370,"step":0,"task":null},{"name":"Equalized Odds","value":0.18564875262007313,"timestamp":1757498620370,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498620370,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498620698,"step":14,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498620698,"step":14,"task":null},{"name":"Accuracy","value":0.8005644402634055,"timestamp":1757498620226,"step":0,"task":null},{"name":"F1 Score","value":0.3575757575757576,"timestamp":1757498620226,"step":0,"task":null},{"name":"Log Loss","value":0.4180305912163541,"timestamp":1757498620226,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498620370,"step":0,"task":null},{"name":"Precision","value":0.9672131147540983,"timestamp":1757498620226,"step":0,"task":null},{"name":"Disparate Impact","value":0.07366557045282064,"timestamp":1757498620370,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":16.8,"timestamp":1757498620698,"step":14,"task":null},{"name":"system_network_receive_megabytes","value":161.29478100000415,"timestamp":1757498620698,"step":14,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d4bb6e9eb62f4c8f943713e42f044249/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_40","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"d4bb6e9eb62f4c8f943713e42f044249\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:03:36.950807\", \"model_uuid\": \"d89dbde998004d34884ab370299eef4d\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"b678a8f5afe64b51a4cbabf8e15f2ca3","name":"Run_39","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498596976,"endTime":1757498602456,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498601880,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.39909807318775464,"timestamp":1757498601880,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.18564875262007313,"timestamp":1757498601880,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757498602286,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.789495290139751,"timestamp":1757498601880,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7909411764705883,"timestamp":1757498602274,"step":340000,"task":null},{"name":"Recall","value":0.21933085501858737,"timestamp":1757498601752,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30179.0,"timestamp":1757498602286,"step":4,"task":null},{"name":"Test Fairness","value":0.3688725490196078,"timestamp":1757498601880,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498602286,"step":4,"task":null},{"name":"Statistical Parity","value":0.9244752994752995,"timestamp":1757498601880,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.07366557045282064,"timestamp":1757498601880,"step":0,"task":null},{"name":"AUC-ROC","value":0.8951733727866058,"timestamp":1757498601752,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7872941176470588,"timestamp":1757498602366,"step":340000,"task":null},{"name":"Well Calibration","value":0.5080484979041341,"timestamp":1757498601880,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40529463521718573,"timestamp":1757498601752,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":98.1378750000149,"timestamp":1757498602286,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.39008968693957746,"timestamp":1757498601880,"step":0,"task":null},{"name":"Cohen Kappa","value":0.29127092951580635,"timestamp":1757498601752,"step":0,"task":null},{"name":"Equal Opportunity","value":0.189640768588137,"timestamp":1757498601880,"step":0,"task":null},{"name":"Equalized Odds","value":0.18564875262007313,"timestamp":1757498601880,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498601880,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498602286,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498602286,"step":4,"task":null},{"name":"Accuracy","value":0.8005644402634055,"timestamp":1757498601752,"step":0,"task":null},{"name":"F1 Score","value":0.3575757575757576,"timestamp":1757498601752,"step":0,"task":null},{"name":"Log Loss","value":0.4179485630521994,"timestamp":1757498601752,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498601880,"step":0,"task":null},{"name":"Precision","value":0.9672131147540983,"timestamp":1757498601752,"step":0,"task":null},{"name":"Disparate Impact","value":0.07366557045282064,"timestamp":1757498601880,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.5,"timestamp":1757498602286,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":98.03958499999135,"timestamp":1757498602286,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b678a8f5afe64b51a4cbabf8e15f2ca3/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_39","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"b678a8f5afe64b51a4cbabf8e15f2ca3\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:03:18.380998\", \"model_uuid\": \"43efaa5d816f47acafc005f541b81588\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"05eb4fca53a2488bbb4808a8d4b6fec5","name":"Run_38","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498591707,"endTime":1757498596919,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498596394,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.548004913119052,"timestamp":1757498596394,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.012982446399985206,"timestamp":1757498596394,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757498596038,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.5877760194987086,"timestamp":1757498596394,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7478823529411764,"timestamp":1757498596749,"step":340000,"task":null},{"name":"Recall","value":0.3170731707317073,"timestamp":1757498596256,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30146.5,"timestamp":1757498596038,"step":3,"task":null},{"name":"Test Fairness","value":0.24619240408714094,"timestamp":1757498596394,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498596038,"step":3,"task":null},{"name":"Statistical Parity","value":0.8237816575583177,"timestamp":1757498596394,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.03158361018826135,"timestamp":1757498596394,"step":0,"task":null},{"name":"AUC-ROC","value":0.760917537746806,"timestamp":1757498596256,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7418823529411765,"timestamp":1757498596841,"step":340000,"task":null},{"name":"Well Calibration","value":0.7836042945537297,"timestamp":1757498596394,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.39393058117604535,"timestamp":1757498596256,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":75.90046199999051,"timestamp":1757498596038,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5493533532426528,"timestamp":1757498596394,"step":0,"task":null},{"name":"Cohen Kappa","value":0.336254698605158,"timestamp":1757498596256,"step":0,"task":null},{"name":"Equal Opportunity","value":0.042232277526395176,"timestamp":1757498596394,"step":0,"task":null},{"name":"Equalized Odds","value":-0.012982446399985206,"timestamp":1757498596394,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498596394,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498596038,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498596038,"step":3,"task":null},{"name":"Accuracy","value":0.7638758231420508,"timestamp":1757498596256,"step":0,"task":null},{"name":"F1 Score","value":0.4531590413943355,"timestamp":1757498596256,"step":0,"task":null},{"name":"Log Loss","value":0.529842150055418,"timestamp":1757498596256,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498596394,"step":0,"task":null},{"name":"Precision","value":0.7938931297709924,"timestamp":1757498596256,"step":0,"task":null},{"name":"Disparate Impact","value":0.03158361018826135,"timestamp":1757498596394,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":30.0,"timestamp":1757498596038,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":75.33254800000577,"timestamp":1757498596038,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/05eb4fca53a2488bbb4808a8d4b6fec5/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_38","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"05eb4fca53a2488bbb4808a8d4b6fec5\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:03:13.086013\", \"model_uuid\": \"46de4d6fde024aaf8cd768dd8867b573\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"f339d45ea662494eb6e3b82bdf07364c","name":"Run_37","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498573513,"endTime":1757498591651,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498591154,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.39909807318775464,"timestamp":1757498591154,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.18564875262007313,"timestamp":1757498591154,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757498590847,"step":13,"task":null},{"name":"Balance for Negative Class","value":0.789495290139751,"timestamp":1757498591154,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7955294117647058,"timestamp":1757498591473,"step":340000,"task":null},{"name":"Recall","value":0.21933085501858737,"timestamp":1757498591020,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30186.7,"timestamp":1757498590847,"step":13,"task":null},{"name":"Test Fairness","value":0.3688725490196078,"timestamp":1757498591154,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498590847,"step":13,"task":null},{"name":"Statistical Parity","value":0.9244752994752995,"timestamp":1757498591154,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.07366557045282064,"timestamp":1757498591154,"step":0,"task":null},{"name":"AUC-ROC","value":0.8951897596284402,"timestamp":1757498591020,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7898823529411765,"timestamp":1757498591561,"step":340000,"task":null},{"name":"Well Calibration","value":0.5084309095741382,"timestamp":1757498591154,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40529463521718573,"timestamp":1757498591020,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":505.3319859999756,"timestamp":1757498590847,"step":13,"task":null},{"name":"Equal Negative Predictive Value","value":0.39008968693957746,"timestamp":1757498591154,"step":0,"task":null},{"name":"Cohen Kappa","value":0.29127092951580635,"timestamp":1757498591020,"step":0,"task":null},{"name":"Equal Opportunity","value":0.189640768588137,"timestamp":1757498591154,"step":0,"task":null},{"name":"Equalized Odds","value":0.18564875262007313,"timestamp":1757498591154,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498591154,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498590847,"step":13,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498590847,"step":13,"task":null},{"name":"Accuracy","value":0.8005644402634055,"timestamp":1757498591020,"step":0,"task":null},{"name":"F1 Score","value":0.3575757575757576,"timestamp":1757498591020,"step":0,"task":null},{"name":"Log Loss","value":0.4180305912163541,"timestamp":1757498591020,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498591154,"step":0,"task":null},{"name":"Precision","value":0.9672131147540983,"timestamp":1757498591020,"step":0,"task":null},{"name":"Disparate Impact","value":0.07366557045282064,"timestamp":1757498591154,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.4,"timestamp":1757498590847,"step":13,"task":null},{"name":"system_network_receive_megabytes","value":505.2190890000202,"timestamp":1757498590847,"step":13,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f339d45ea662494eb6e3b82bdf07364c/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_37","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"f339d45ea662494eb6e3b82bdf07364c\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:03:07.779992\", \"model_uuid\": \"16ea3ce268334da186b889595806c180\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"62191f4881db4c00b288cf3d698435a1","name":"Run_36","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498568273,"endTime":1757498573464,"params":[{"name":"criterion","value":"entropy","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498572936,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.39909807318775464,"timestamp":1757498572936,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.18564875262007313,"timestamp":1757498572936,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757498572515,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.789495290139751,"timestamp":1757498572936,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7911176470588236,"timestamp":1757498573297,"step":340000,"task":null},{"name":"Recall","value":0.21933085501858737,"timestamp":1757498572826,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30193.8,"timestamp":1757498572515,"step":3,"task":null},{"name":"Test Fairness","value":0.3688725490196078,"timestamp":1757498572936,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498572515,"step":3,"task":null},{"name":"Statistical Parity","value":0.9244752994752995,"timestamp":1757498572936,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.07366557045282064,"timestamp":1757498572936,"step":0,"task":null},{"name":"AUC-ROC","value":0.8951733727866058,"timestamp":1757498572826,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7847058823529411,"timestamp":1757498573385,"step":340000,"task":null},{"name":"Well Calibration","value":0.5080484979041341,"timestamp":1757498572936,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40529463521718573,"timestamp":1757498572826,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":116.78727400000207,"timestamp":1757498572515,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.39008968693957746,"timestamp":1757498572936,"step":0,"task":null},{"name":"Cohen Kappa","value":0.29127092951580635,"timestamp":1757498572826,"step":0,"task":null},{"name":"Equal Opportunity","value":0.189640768588137,"timestamp":1757498572936,"step":0,"task":null},{"name":"Equalized Odds","value":0.18564875262007313,"timestamp":1757498572936,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498572936,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498572515,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498572515,"step":3,"task":null},{"name":"Accuracy","value":0.8005644402634055,"timestamp":1757498572826,"step":0,"task":null},{"name":"F1 Score","value":0.3575757575757576,"timestamp":1757498572826,"step":0,"task":null},{"name":"Log Loss","value":0.4179485630521994,"timestamp":1757498572826,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498572936,"step":0,"task":null},{"name":"Precision","value":0.9672131147540983,"timestamp":1757498572826,"step":0,"task":null},{"name":"Disparate Impact","value":0.07366557045282064,"timestamp":1757498572936,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":43.7,"timestamp":1757498572515,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":116.76117700000759,"timestamp":1757498572515,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/62191f4881db4c00b288cf3d698435a1/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_36","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"62191f4881db4c00b288cf3d698435a1\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:02:49.522789\", \"model_uuid\": \"5b51a926f6f24918adad599f720085e7\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"56429eba3eaf4dca9f7ba3f09017a1df","name":"Run_35","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498561749,"endTime":1757498568226,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.6420636901023068,"timestamp":1757498567164,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5563783953295431,"timestamp":1757498567164,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.30379281857511337,"timestamp":1757498567164,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.7,"timestamp":1757498568094,"step":5,"task":null},{"name":"Balance for Negative Class","value":0.5284327648009587,"timestamp":1757498567164,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9490000000000001,"timestamp":1757498568039,"step":340000,"task":null},{"name":"Recall","value":0.4603658536585366,"timestamp":1757498566965,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30068.0,"timestamp":1757498568094,"step":5,"task":null},{"name":"Test Fairness","value":0.24864864864864866,"timestamp":1757498567164,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498568094,"step":5,"task":null},{"name":"Statistical Parity","value":0.839252590146134,"timestamp":1757498567164,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.4452771272443403,"timestamp":1757498567164,"step":0,"task":null},{"name":"AUC-ROC","value":0.7885784801725568,"timestamp":1757498566965,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7919999999999999,"timestamp":1757498568137,"step":340000,"task":null},{"name":"Well Calibration","value":0.6676182960895994,"timestamp":1757498567164,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.41113523577335526,"timestamp":1757498566965,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":226.8787810000067,"timestamp":1757498568094,"step":5,"task":null},{"name":"Equal Negative Predictive Value","value":0.5551845049256973,"timestamp":1757498567164,"step":0,"task":null},{"name":"Cohen Kappa","value":0.39765147636115383,"timestamp":1757498566965,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3288314738696418,"timestamp":1757498567164,"step":0,"task":null},{"name":"Equalized Odds","value":0.30379281857511337,"timestamp":1757498567164,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.43010349833747014,"timestamp":1757498567164,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498568094,"step":5,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498568094,"step":5,"task":null},{"name":"Accuracy","value":0.7657572906867357,"timestamp":1757498566965,"step":0,"task":null},{"name":"F1 Score","value":0.5480943738656987,"timestamp":1757498566965,"step":0,"task":null},{"name":"Log Loss","value":0.4998417523790357,"timestamp":1757498566965,"step":0,"task":null},{"name":"Treatment Equality","value":0.8005913123088957,"timestamp":1757498567164,"step":0,"task":null},{"name":"Precision","value":0.6771300448430493,"timestamp":1757498566965,"step":0,"task":null},{"name":"Disparate Impact","value":0.4452771272443403,"timestamp":1757498567164,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":9.4,"timestamp":1757498568094,"step":5,"task":null},{"name":"system_network_receive_megabytes","value":226.48052200002712,"timestamp":1757498568094,"step":5,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/56429eba3eaf4dca9f7ba3f09017a1df/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_35","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"56429eba3eaf4dca9f7ba3f09017a1df\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:02:43.433789\", \"model_uuid\": \"c79fc9d4384748688963bb276dac84d8\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"5fdb8c36e6b14e049f16a7324ddbcdd1","name":"Run_34","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498542155,"endTime":1757498561724,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.32311627793662645,"timestamp":1757498560723,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40407044411371773,"timestamp":1757498560723,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.9078393708714979,"timestamp":1757498560723,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.7,"timestamp":1757498561524,"step":15,"task":null},{"name":"Balance for Negative Class","value":0.944896375179211,"timestamp":1757498560723,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9696470588235295,"timestamp":1757498561576,"step":340000,"task":null},{"name":"Recall","value":0.6505576208178439,"timestamp":1757498560579,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30067.7,"timestamp":1757498561524,"step":15,"task":null},{"name":"Test Fairness","value":0.17500000000000004,"timestamp":1757498560723,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498561524,"step":15,"task":null},{"name":"Statistical Parity","value":0.8083158617698092,"timestamp":1757498560723,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3472534236036313,"timestamp":1757498560723,"step":0,"task":null},{"name":"AUC-ROC","value":0.9180096073712696,"timestamp":1757498560579,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8472941176470588,"timestamp":1757498561657,"step":340000,"task":null},{"name":"Well Calibration","value":0.3939937402470741,"timestamp":1757498560723,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6253801124365098,"timestamp":1757498560579,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":529.061870000005,"timestamp":1757498561524,"step":15,"task":null},{"name":"Equal Negative Predictive Value","value":0.4124386039941423,"timestamp":1757498560723,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6211764240036428,"timestamp":1757498560579,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9860038610038612,"timestamp":1757498560723,"step":0,"task":null},{"name":"Equalized Odds","value":0.9078393708714979,"timestamp":1757498560723,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.150797329154881,"timestamp":1757498560723,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18201.2,"timestamp":1757498561524,"step":15,"task":null},{"name":"system_disk_available_megabytes","value":16.2,"timestamp":1757498561524,"step":15,"task":null},{"name":"Accuracy","value":0.8645343367826905,"timestamp":1757498560579,"step":0,"task":null},{"name":"F1 Score","value":0.708502024291498,"timestamp":1757498560579,"step":0,"task":null},{"name":"Log Loss","value":0.3100721939561194,"timestamp":1757498560579,"step":0,"task":null},{"name":"Treatment Equality","value":0.3148604435223865,"timestamp":1757498560723,"step":0,"task":null},{"name":"Precision","value":0.7777777777777778,"timestamp":1757498560579,"step":0,"task":null},{"name":"Disparate Impact","value":0.3472534236036313,"timestamp":1757498560723,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":10.6,"timestamp":1757498561524,"step":15,"task":null},{"name":"system_network_receive_megabytes","value":528.674954999995,"timestamp":1757498561524,"step":15,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5fdb8c36e6b14e049f16a7324ddbcdd1/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_34","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"5fdb8c36e6b14e049f16a7324ddbcdd1\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:02:37.067139\", \"model_uuid\": \"6893b066dd104fecac0ff31b309360a0\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"323a3f6f3fab4d16baa3ce6408494a9f","name":"Run_33","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498535748,"endTime":1757498542097,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.30333364867520035,"timestamp":1757498541176,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40273024363572857,"timestamp":1757498541176,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8952338713421949,"timestamp":1757498541176,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757498541054,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.9507004723837493,"timestamp":1757498541176,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9690588235294119,"timestamp":1757498541919,"step":340000,"task":null},{"name":"Recall","value":0.654275092936803,"timestamp":1757498541008,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30206.8,"timestamp":1757498541054,"step":4,"task":null},{"name":"Test Fairness","value":0.2777777777777778,"timestamp":1757498541176,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498541054,"step":4,"task":null},{"name":"Statistical Parity","value":0.8022610613071139,"timestamp":1757498541176,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3402382029247701,"timestamp":1757498541176,"step":0,"task":null},{"name":"AUC-ROC","value":0.9173494517430918,"timestamp":1757498541008,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8465882352941175,"timestamp":1757498542006,"step":340000,"task":null},{"name":"Well Calibration","value":0.3936070730253333,"timestamp":1757498541176,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6213146287197021,"timestamp":1757498541008,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":101.63072299998021,"timestamp":1757498541054,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.4128114491762726,"timestamp":1757498541176,"step":0,"task":null},{"name":"Cohen Kappa","value":0.617900790797987,"timestamp":1757498541008,"step":0,"task":null},{"name":"Equal Opportunity","value":0.979386385426654,"timestamp":1757498541176,"step":0,"task":null},{"name":"Equalized Odds","value":0.8952338713421949,"timestamp":1757498541176,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.14252135880137262,"timestamp":1757498541176,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498541054,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498541054,"step":4,"task":null},{"name":"Accuracy","value":0.8626528692380057,"timestamp":1757498541008,"step":0,"task":null},{"name":"F1 Score","value":0.7068273092369478,"timestamp":1757498541008,"step":0,"task":null},{"name":"Log Loss","value":0.30983547713276954,"timestamp":1757498541008,"step":0,"task":null},{"name":"Treatment Equality","value":0.2918417130866244,"timestamp":1757498541176,"step":0,"task":null},{"name":"Precision","value":0.7685589519650655,"timestamp":1757498541008,"step":0,"task":null},{"name":"Disparate Impact","value":0.3402382029247701,"timestamp":1757498541176,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":23.7,"timestamp":1757498541054,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":101.5306790000177,"timestamp":1757498541054,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/323a3f6f3fab4d16baa3ce6408494a9f/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_33","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"323a3f6f3fab4d16baa3ce6408494a9f\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:02:17.451240\", \"model_uuid\": \"ca7ae3384c9c447ba91defb052171fda\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"f5d9257787b44c6f9d2241b5cbe1b0d5","name":"Run_32","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498528980,"endTime":1757498535697,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.6114892286688636,"timestamp":1757498534604,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5609388739797853,"timestamp":1757498534604,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.29141628553203774,"timestamp":1757498534604,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.6,"timestamp":1757498535384,"step":5,"task":null},{"name":"Balance for Negative Class","value":0.5199052188827756,"timestamp":1757498534604,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9492352941176471,"timestamp":1757498535525,"step":340000,"task":null},{"name":"Recall","value":0.4573170731707317,"timestamp":1757498534439,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29983.3,"timestamp":1757498535384,"step":5,"task":null},{"name":"Test Fairness","value":0.27835880933226065,"timestamp":1757498534604,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498535384,"step":5,"task":null},{"name":"Statistical Parity","value":0.839252590146134,"timestamp":1757498534604,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.4452771272443403,"timestamp":1757498534604,"step":0,"task":null},{"name":"AUC-ROC","value":0.7885038161606104,"timestamp":1757498534439,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7931764705882353,"timestamp":1757498535610,"step":340000,"task":null},{"name":"Well Calibration","value":0.6674341637196834,"timestamp":1757498534604,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.406133040994577,"timestamp":1757498534439,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":165.34967799999868,"timestamp":1757498535384,"step":5,"task":null},{"name":"Equal Negative Predictive Value","value":0.5576519916142558,"timestamp":1757498534604,"step":0,"task":null},{"name":"Cohen Kappa","value":0.39281333560903464,"timestamp":1757498534439,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3123899001761597,"timestamp":1757498534604,"step":0,"task":null},{"name":"Equalized Odds","value":0.29141628553203774,"timestamp":1757498534604,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.3891412604005681,"timestamp":1757498534604,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18204.0,"timestamp":1757498535384,"step":5,"task":null},{"name":"system_disk_available_megabytes","value":13.4,"timestamp":1757498535384,"step":5,"task":null},{"name":"Accuracy","value":0.7638758231420508,"timestamp":1757498534439,"step":0,"task":null},{"name":"F1 Score","value":0.5444646098003629,"timestamp":1757498534439,"step":0,"task":null},{"name":"Log Loss","value":0.49997135625670375,"timestamp":1757498534439,"step":0,"task":null},{"name":"Treatment Equality","value":0.8307312205369954,"timestamp":1757498534604,"step":0,"task":null},{"name":"Precision","value":0.672645739910314,"timestamp":1757498534439,"step":0,"task":null},{"name":"Disparate Impact","value":0.4452771272443403,"timestamp":1757498534604,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":11.8,"timestamp":1757498535384,"step":5,"task":null},{"name":"system_network_receive_megabytes","value":164.91823900002055,"timestamp":1757498535384,"step":5,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f5d9257787b44c6f9d2241b5cbe1b0d5/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_32","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"f5d9257787b44c6f9d2241b5cbe1b0d5\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:02:10.791215\", \"model_uuid\": \"331710a7971c42e1a63509da8ae4a53e\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"c6c8d7ba4af64d269b18858a88941b36","name":"Run_31","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498510145,"endTime":1757498528938,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.32311627793662645,"timestamp":1757498528078,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40273024363572857,"timestamp":1757498528078,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.9213462730770302,"timestamp":1757498528078,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.8,"timestamp":1757498527990,"step":14,"task":null},{"name":"Balance for Negative Class","value":0.9215656004834281,"timestamp":1757498528078,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9681764705882353,"timestamp":1757498528807,"step":340000,"task":null},{"name":"Recall","value":0.6431226765799256,"timestamp":1757498527911,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30137.9,"timestamp":1757498527990,"step":14,"task":null},{"name":"Test Fairness","value":0.18333333333333335,"timestamp":1757498528078,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498527990,"step":14,"task":null},{"name":"Statistical Parity","value":0.8113432620011567,"timestamp":1757498528078,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.35087064676616914,"timestamp":1757498528078,"step":0,"task":null},{"name":"AUC-ROC","value":0.917735713014898,"timestamp":1757498527911,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8470588235294118,"timestamp":1757498528877,"step":340000,"task":null},{"name":"Well Calibration","value":0.3943760427005522,"timestamp":1757498528078,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6194908569086688,"timestamp":1757498527911,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":503.2794100000174,"timestamp":1757498527990,"step":14,"task":null},{"name":"Equal Negative Predictive Value","value":0.4108996539792388,"timestamp":1757498528078,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6149143222950608,"timestamp":1757498527911,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9995107632093935,"timestamp":1757498528078,"step":0,"task":null},{"name":"Equalized Odds","value":0.9213462730770302,"timestamp":1757498528078,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.14875952740954476,"timestamp":1757498528078,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498527990,"step":14,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498527990,"step":14,"task":null},{"name":"Accuracy","value":0.8626528692380057,"timestamp":1757498527911,"step":0,"task":null},{"name":"F1 Score","value":0.7032520325203252,"timestamp":1757498527911,"step":0,"task":null},{"name":"Log Loss","value":0.3103158943753724,"timestamp":1757498527911,"step":0,"task":null},{"name":"Treatment Equality","value":0.32283159399130773,"timestamp":1757498528078,"step":0,"task":null},{"name":"Precision","value":0.7757847533632287,"timestamp":1757498527911,"step":0,"task":null},{"name":"Disparate Impact","value":0.35087064676616914,"timestamp":1757498528078,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":20.5,"timestamp":1757498527990,"step":14,"task":null},{"name":"system_network_receive_megabytes","value":503.056156000006,"timestamp":1757498527990,"step":14,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/c6c8d7ba4af64d269b18858a88941b36/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_31","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"c6c8d7ba4af64d269b18858a88941b36\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:02:04.502242\", \"model_uuid\": \"f8ae115ed03245d489a39e0322c4890a\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"63ed45943d39474095fa27e0cecd6ea3","name":"Run_30","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498504251,"endTime":1757498510095,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.30333364867520035,"timestamp":1757498509212,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.4034003438747231,"timestamp":1757498509212,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8887046287726837,"timestamp":1757498509212,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.6,"timestamp":1757498509554,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.9630472317653564,"timestamp":1757498509212,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.967764705882353,"timestamp":1757498509982,"step":340000,"task":null},{"name":"Recall","value":0.6579925650557621,"timestamp":1757498509079,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29942.8,"timestamp":1757498509554,"step":4,"task":null},{"name":"Test Fairness","value":0.25,"timestamp":1757498509212,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498509554,"step":4,"task":null},{"name":"Statistical Parity","value":0.8007473611914402,"timestamp":1757498509212,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3385284632115803,"timestamp":1757498509212,"step":0,"task":null},{"name":"AUC-ROC","value":0.9170685344545053,"timestamp":1757498509079,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8489411764705881,"timestamp":1757498510037,"step":340000,"task":null},{"name":"Well Calibration","value":0.3963848235645967,"timestamp":1757498509212,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6242595607139598,"timestamp":1757498509079,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":141.3936270000122,"timestamp":1757498509554,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.41359181108397824,"timestamp":1757498509212,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6210077772887432,"timestamp":1757498509079,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9728571428571429,"timestamp":1757498509212,"step":0,"task":null},{"name":"Equalized Odds","value":0.8887046287726837,"timestamp":1757498509212,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.14347787798795902,"timestamp":1757498509212,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498509554,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498509554,"step":4,"task":null},{"name":"Accuracy","value":0.863593603010348,"timestamp":1757498509079,"step":0,"task":null},{"name":"F1 Score","value":0.7094188376753507,"timestamp":1757498509079,"step":0,"task":null},{"name":"Log Loss","value":0.3097164528892504,"timestamp":1757498509079,"step":0,"task":null},{"name":"Treatment Equality","value":0.2881001526624369,"timestamp":1757498509212,"step":0,"task":null},{"name":"Precision","value":0.7695652173913043,"timestamp":1757498509079,"step":0,"task":null},{"name":"Disparate Impact","value":0.3385284632115803,"timestamp":1757498509212,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":33.8,"timestamp":1757498509554,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":141.34492500001215,"timestamp":1757498509554,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/63ed45943d39474095fa27e0cecd6ea3/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_30","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"63ed45943d39474095fa27e0cecd6ea3\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:01:45.963021\", \"model_uuid\": \"578dbbc672b8480caccf3d80a91da5be\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"0f82137bdd9b4d52b4cd0b89688684d3","name":"Run_29","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498498609,"endTime":1757498504216,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.6702862698870236,"timestamp":1757498503525,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.556001031779259,"timestamp":1757498503525,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.28119133665678214,"timestamp":1757498503525,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.7,"timestamp":1757498503838,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.5259226403976225,"timestamp":1757498503525,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9497058823529413,"timestamp":1757498504118,"step":340000,"task":null},{"name":"Recall","value":0.4573170731707317,"timestamp":1757498503412,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30064.4,"timestamp":1757498503838,"step":4,"task":null},{"name":"Test Fairness","value":0.2507575757575758,"timestamp":1757498503525,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498503838,"step":4,"task":null},{"name":"Statistical Parity","value":0.8313351128806045,"timestamp":1757498503525,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.43343465045592705,"timestamp":1757498503525,"step":0,"task":null},{"name":"AUC-ROC","value":0.7878401360544216,"timestamp":1757498503412,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7915294117647058,"timestamp":1757498504163,"step":340000,"task":null},{"name":"Well Calibration","value":0.6677731702542085,"timestamp":1757498503525,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3952007768426773,"timestamp":1757498503412,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":157.2440099999949,"timestamp":1757498503838,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.5565217391304348,"timestamp":1757498503525,"step":0,"task":null},{"name":"Cohen Kappa","value":0.38357269061749655,"timestamp":1757498503412,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3123899001761597,"timestamp":1757498503525,"step":0,"task":null},{"name":"Equalized Odds","value":0.28119133665678214,"timestamp":1757498503525,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.4265586892852381,"timestamp":1757498503525,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498503838,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498503838,"step":4,"task":null},{"name":"Accuracy","value":0.7591721542803387,"timestamp":1757498503412,"step":0,"task":null},{"name":"F1 Score","value":0.539568345323741,"timestamp":1757498503412,"step":0,"task":null},{"name":"Log Loss","value":0.5018406284443718,"timestamp":1757498503412,"step":0,"task":null},{"name":"Treatment Equality","value":0.7578600608407677,"timestamp":1757498503525,"step":0,"task":null},{"name":"Precision","value":0.6578947368421053,"timestamp":1757498503412,"step":0,"task":null},{"name":"Disparate Impact","value":0.43343465045592705,"timestamp":1757498503525,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":19.5,"timestamp":1757498503838,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":156.97417300002417,"timestamp":1757498503838,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0f82137bdd9b4d52b4cd0b89688684d3/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_29","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"0f82137bdd9b4d52b4cd0b89688684d3\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:01:40.077808\", \"model_uuid\": \"8897497bc6bf4cb2bd72cbdc07993213\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"7fbb2f748f1e48989f18a263de21c314","name":"Run_28","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498479627,"endTime":1757498498568,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.39530182939055364,"timestamp":1757498497910,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.406025434969705,"timestamp":1757498497910,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8961096140240241,"timestamp":1757498497910,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.5,"timestamp":1757498498348,"step":15,"task":null},{"name":"Balance for Negative Class","value":0.9706672127811989,"timestamp":1757498497910,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9655882352941176,"timestamp":1757498498404,"step":340000,"task":null},{"name":"Recall","value":0.6579925650557621,"timestamp":1757498497771,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29897.2,"timestamp":1757498498348,"step":15,"task":null},{"name":"Test Fairness","value":0.27601809954751133,"timestamp":1757498497910,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498498348,"step":15,"task":null},{"name":"Statistical Parity","value":0.8064274834076814,"timestamp":1757498497910,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3529964391241761,"timestamp":1757498497910,"step":0,"task":null},{"name":"AUC-ROC","value":0.9185925107450863,"timestamp":1757498497771,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8451764705882352,"timestamp":1757498498484,"step":340000,"task":null},{"name":"Well Calibration","value":0.3905069808130995,"timestamp":1757498497910,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6265777830729742,"timestamp":1757498497771,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":604.6258810000145,"timestamp":1757498498348,"step":15,"task":null},{"name":"Equal Negative Predictive Value","value":0.41392809070245523,"timestamp":1757498497910,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6231350265404804,"timestamp":1757498497771,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9728571428571429,"timestamp":1757498497910,"step":0,"task":null},{"name":"Equalized Odds","value":0.8961096140240241,"timestamp":1757498497910,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.18697914950558495,"timestamp":1757498497910,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498498348,"step":15,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498498348,"step":15,"task":null},{"name":"Accuracy","value":0.8645343367826905,"timestamp":1757498497771,"step":0,"task":null},{"name":"F1 Score","value":0.7108433734939759,"timestamp":1757498497771,"step":0,"task":null},{"name":"Log Loss","value":0.3092557429134278,"timestamp":1757498497771,"step":0,"task":null},{"name":"Treatment Equality","value":0.37544966703349497,"timestamp":1757498497910,"step":0,"task":null},{"name":"Precision","value":0.7729257641921398,"timestamp":1757498497771,"step":0,"task":null},{"name":"Disparate Impact","value":0.3529964391241761,"timestamp":1757498497910,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":28.2,"timestamp":1757498498348,"step":15,"task":null},{"name":"system_network_receive_megabytes","value":604.4460529999924,"timestamp":1757498498348,"step":15,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7fbb2f748f1e48989f18a263de21c314/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_28","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"7fbb2f748f1e48989f18a263de21c314\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:01:34.709859\", \"model_uuid\": \"61f8f72ff2644e4a8862bcdf76fc84ee\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"13f2e1a8073549f68f64a33cf976a366","name":"Run_27","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498473662,"endTime":1757498479579,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.309653099689267,"timestamp":1757498478880,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40413174665924095,"timestamp":1757498478880,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.9134575289872922,"timestamp":1757498478880,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.7,"timestamp":1757498478983,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.924854880780538,"timestamp":1757498478880,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9682941176470587,"timestamp":1757498479409,"step":340000,"task":null},{"name":"Recall","value":0.6691449814126395,"timestamp":1757498478737,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30018.4,"timestamp":1757498478983,"step":4,"task":null},{"name":"Test Fairness","value":0.39473684210526316,"timestamp":1757498478880,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498478983,"step":4,"task":null},{"name":"Statistical Parity","value":0.8018713959308019,"timestamp":1757498478880,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3477014925373134,"timestamp":1757498478880,"step":0,"task":null},{"name":"AUC-ROC","value":0.915785678836628,"timestamp":1757498478737,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.843764705882353,"timestamp":1757498479500,"step":340000,"task":null},{"name":"Well Calibration","value":0.38949958279247604,"timestamp":1757498478880,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6353745629306218,"timestamp":1757498478737,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":203.23032599998987,"timestamp":1757498478983,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.4139257963057703,"timestamp":1757498478880,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6324117651386767,"timestamp":1757498478737,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9956140350877193,"timestamp":1757498478880,"step":0,"task":null},{"name":"Equalized Odds","value":0.9134575289872922,"timestamp":1757498478880,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.14311918329298914,"timestamp":1757498478880,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498478983,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498478983,"step":4,"task":null},{"name":"Accuracy","value":0.8673565380997178,"timestamp":1757498478737,"step":0,"task":null},{"name":"F1 Score","value":0.718562874251497,"timestamp":1757498478737,"step":0,"task":null},{"name":"Log Loss","value":0.31149756315889365,"timestamp":1757498478737,"step":0,"task":null},{"name":"Treatment Equality","value":0.3069248785466303,"timestamp":1757498478880,"step":0,"task":null},{"name":"Precision","value":0.7758620689655172,"timestamp":1757498478737,"step":0,"task":null},{"name":"Disparate Impact","value":0.3477014925373134,"timestamp":1757498478880,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":11.2,"timestamp":1757498478983,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":203.22654999999213,"timestamp":1757498478983,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/13f2e1a8073549f68f64a33cf976a366/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_27","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"13f2e1a8073549f68f64a33cf976a366\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:01:15.147768\", \"model_uuid\": \"53ce5258930a41ff9031b0c1ad48bc95\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"14f232f24e614248bf7a88e259de9d42","name":"Run_26","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498467662,"endTime":1757498473613,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.6398187121648861,"timestamp":1757498472867,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5582891018688856,"timestamp":1757498472867,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.2852563773071887,"timestamp":1757498472867,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.6,"timestamp":1757498472989,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.5235852064402996,"timestamp":1757498472867,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9462941176470588,"timestamp":1757498473440,"step":340000,"task":null},{"name":"Recall","value":0.4573170731707317,"timestamp":1757498472688,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":30013.9,"timestamp":1757498472989,"step":4,"task":null},{"name":"Test Fairness","value":0.26343873517786565,"timestamp":1757498472867,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498472989,"step":4,"task":null},{"name":"Statistical Parity","value":0.8340255177766582,"timestamp":1757498472867,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.44427051671732526,"timestamp":1757498472867,"step":0,"task":null},{"name":"AUC-ROC","value":0.7877260660361706,"timestamp":1757498472688,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7917647058823529,"timestamp":1757498473526,"step":340000,"task":null},{"name":"Well Calibration","value":0.6665941447799624,"timestamp":1757498472867,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3930446960093313,"timestamp":1757498472688,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":33.52041999998619,"timestamp":1757498472989,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.5572029653376077,"timestamp":1757498472867,"step":0,"task":null},{"name":"Cohen Kappa","value":0.3817347753962165,"timestamp":1757498472688,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3123899001761597,"timestamp":1757498472867,"step":0,"task":null},{"name":"Equalized Odds","value":0.2852563773071887,"timestamp":1757498472867,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.40716965795409094,"timestamp":1757498472867,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498472989,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498472989,"step":4,"task":null},{"name":"Accuracy","value":0.7582314205079962,"timestamp":1757498472688,"step":0,"task":null},{"name":"F1 Score","value":0.5385996409335727,"timestamp":1757498472688,"step":0,"task":null},{"name":"Log Loss","value":0.5020386763634526,"timestamp":1757498472688,"step":0,"task":null},{"name":"Treatment Equality","value":0.7939486351665185,"timestamp":1757498472867,"step":0,"task":null},{"name":"Precision","value":0.6550218340611353,"timestamp":1757498472688,"step":0,"task":null},{"name":"Disparate Impact","value":0.44427051671732526,"timestamp":1757498472867,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":20.3,"timestamp":1757498472989,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":33.40175600000657,"timestamp":1757498472989,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/14f232f24e614248bf7a88e259de9d42/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_26","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"14f232f24e614248bf7a88e259de9d42\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:01:09.190496\", \"model_uuid\": \"305ae3edccd041db93cbbb85d052f979\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"846de510090845b79d37b392b6928d5f","name":"Run_25","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498448791,"endTime":1757498467613,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.39530182939055364,"timestamp":1757498466798,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40535320742836445,"timestamp":1757498466798,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.9026388565935353,"timestamp":1757498466798,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.6,"timestamp":1757498466946,"step":14,"task":null},{"name":"Balance for Negative Class","value":0.9582227613352863,"timestamp":1757498466798,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9662352941176471,"timestamp":1757498467447,"step":340000,"task":null},{"name":"Recall","value":0.654275092936803,"timestamp":1757498466622,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29982.4,"timestamp":1757498466946,"step":14,"task":null},{"name":"Test Fairness","value":0.27601809954751133,"timestamp":1757498466798,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498466946,"step":14,"task":null},{"name":"Statistical Parity","value":0.8079461792333079,"timestamp":1757498466798,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.35479744136460556,"timestamp":1757498466798,"step":0,"task":null},{"name":"AUC-ROC","value":0.91810558744487,"timestamp":1757498466622,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.847058823529412,"timestamp":1757498467534,"step":340000,"task":null},{"name":"Well Calibration","value":0.3907618373364285,"timestamp":1757498466798,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6236366968296819,"timestamp":1757498466622,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":176.607107000018,"timestamp":1757498466946,"step":14,"task":null},{"name":"Equal Negative Predictive Value","value":0.4131500303815859,"timestamp":1757498466798,"step":0,"task":null},{"name":"Cohen Kappa","value":0.6200267720456456,"timestamp":1757498466622,"step":0,"task":null},{"name":"Equal Opportunity","value":0.979386385426654,"timestamp":1757498466798,"step":0,"task":null},{"name":"Equalized Odds","value":0.9026388565935353,"timestamp":1757498466798,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.18573262184221434,"timestamp":1757498466798,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498466946,"step":14,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498466946,"step":14,"task":null},{"name":"Accuracy","value":0.863593603010348,"timestamp":1757498466622,"step":0,"task":null},{"name":"F1 Score","value":0.7082494969818913,"timestamp":1757498466622,"step":0,"task":null},{"name":"Log Loss","value":0.30975663460170083,"timestamp":1757498466622,"step":0,"task":null},{"name":"Treatment Equality","value":0.38032563673522873,"timestamp":1757498466798,"step":0,"task":null},{"name":"Precision","value":0.7719298245614035,"timestamp":1757498466622,"step":0,"task":null},{"name":"Disparate Impact","value":0.35479744136460556,"timestamp":1757498466798,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":22.4,"timestamp":1757498466946,"step":14,"task":null},{"name":"system_network_receive_megabytes","value":175.5909260000044,"timestamp":1757498466946,"step":14,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/846de510090845b79d37b392b6928d5f/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_25","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"846de510090845b79d37b392b6928d5f\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:01:03.308629\", \"model_uuid\": \"d27c7b353b5040d78b45b850677761c2\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"547952b97c8442ea96ace30caa932cc6","name":"Run_24","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498443530,"endTime":1757498448737,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.309653099689267,"timestamp":1757498448026,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40479973301735533,"timestamp":1757498448026,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.906950247712209,"timestamp":1757498448026,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.5,"timestamp":1757498447767,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.9373529197100049,"timestamp":1757498448026,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9674117647058823,"timestamp":1757498448567,"step":340000,"task":null},{"name":"Recall","value":0.6728624535315985,"timestamp":1757498447879,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29892.3,"timestamp":1757498447767,"step":3,"task":null},{"name":"Test Fairness","value":0.42105263157894735,"timestamp":1757498448026,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498447767,"step":3,"task":null},{"name":"Statistical Parity","value":0.8003527001051753,"timestamp":1757498448026,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.345971634365486,"timestamp":1757498448026,"step":0,"task":null},{"name":"AUC-ROC","value":0.9155000795932318,"timestamp":1757498447879,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.844470588235294,"timestamp":1757498448650,"step":340000,"task":null},{"name":"Well Calibration","value":0.3919483402149546,"timestamp":1757498448026,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6382982139995698,"timestamp":1757498447879,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.061176999995951,"timestamp":1757498447767,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.41471123424942463,"timestamp":1757498448026,"step":0,"task":null},{"name":"Cohen Kappa","value":0.63548810596857,"timestamp":1757498447879,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9891067538126361,"timestamp":1757498448026,"step":0,"task":null},{"name":"Equalized Odds","value":0.906950247712209,"timestamp":1757498448026,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.14406075686728512,"timestamp":1757498448026,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498447767,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498447767,"step":3,"task":null},{"name":"Accuracy","value":0.8682972718720602,"timestamp":1757498447879,"step":0,"task":null},{"name":"F1 Score","value":0.7211155378486056,"timestamp":1757498447879,"step":0,"task":null},{"name":"Log Loss","value":0.31165989038943553,"timestamp":1757498447879,"step":0,"task":null},{"name":"Treatment Equality","value":0.3028325468326752,"timestamp":1757498448026,"step":0,"task":null},{"name":"Precision","value":0.776824034334764,"timestamp":1757498447879,"step":0,"task":null},{"name":"Disparate Impact","value":0.345971634365486,"timestamp":1757498448026,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.5,"timestamp":1757498447767,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.061751000001095,"timestamp":1757498447767,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/547952b97c8442ea96ace30caa932cc6/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_24","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"547952b97c8442ea96ace30caa932cc6\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:00:44.966105\", \"model_uuid\": \"32561d20f587419289ec80e8f0a30d31\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"b558fb79b065488eaa2052a2acc7b457","name":"Run_23","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498438500,"endTime":1757498443478,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.5926741754790524,"timestamp":1757498442936,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5543225718378456,"timestamp":1757498442936,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.3434236711772135,"timestamp":1757498442936,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.5,"timestamp":1757498442728,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.5450144986879073,"timestamp":1757498442936,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.942529411764706,"timestamp":1757498443307,"step":340000,"task":null},{"name":"Recall","value":0.4634146341463415,"timestamp":1757498442797,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29873.6,"timestamp":1757498442728,"step":3,"task":null},{"name":"Test Fairness","value":0.5595238095238095,"timestamp":1757498442936,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498442728,"step":3,"task":null},{"name":"Statistical Parity","value":0.8473007188516888,"timestamp":1757498442936,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.5039248120300752,"timestamp":1757498442936,"step":0,"task":null},{"name":"AUC-ROC","value":0.7794466567114651,"timestamp":1757498442797,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7896470588235294,"timestamp":1757498443391,"step":340000,"task":null},{"name":"Well Calibration","value":0.6597842905260425,"timestamp":1757498442936,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.38592817129095586,"timestamp":1757498442797,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.369646000006469,"timestamp":1757498442728,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5520577011627341,"timestamp":1757498442936,"step":0,"task":null},{"name":"Cohen Kappa","value":0.37670486584606955,"timestamp":1757498442797,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3644970414201183,"timestamp":1757498442936,"step":0,"task":null},{"name":"Equalized Odds","value":0.3434236711772135,"timestamp":1757498442936,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.440079863506008,"timestamp":1757498442936,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498442728,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498442728,"step":3,"task":null},{"name":"Accuracy","value":0.7544684854186265,"timestamp":1757498442797,"step":0,"task":null},{"name":"F1 Score","value":0.5380530973451327,"timestamp":1757498442797,"step":0,"task":null},{"name":"Log Loss","value":0.63620345385152,"timestamp":1757498442797,"step":0,"task":null},{"name":"Treatment Equality","value":0.8980144355481633,"timestamp":1757498442936,"step":0,"task":null},{"name":"Precision","value":0.6413502109704642,"timestamp":1757498442797,"step":0,"task":null},{"name":"Disparate Impact","value":0.5039248120300752,"timestamp":1757498442936,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":36.6,"timestamp":1757498442728,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.3718600000138395,"timestamp":1757498442728,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b558fb79b065488eaa2052a2acc7b457/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_23","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"b558fb79b065488eaa2052a2acc7b457\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:00:39.857189\", \"model_uuid\": \"25e46d61024e430dad98de88b5faa221\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"d7ef31f520564126b5d1137724ab7f7f","name":"Run_22","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498421550,"endTime":1757498438447,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.3870663746115837,"timestamp":1757498437942,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40259793084968504,"timestamp":1757498437942,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8971162515426377,"timestamp":1757498437942,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.5,"timestamp":1757498437736,"step":12,"task":null},{"name":"Balance for Negative Class","value":0.9584160231840775,"timestamp":1757498437942,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9622941176470589,"timestamp":1757498438273,"step":340000,"task":null},{"name":"Recall","value":0.6319702602230484,"timestamp":1757498437798,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29875.6,"timestamp":1757498437736,"step":12,"task":null},{"name":"Test Fairness","value":0.32839506172839505,"timestamp":1757498437942,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498437736,"step":12,"task":null},{"name":"Statistical Parity","value":0.8113432620011567,"timestamp":1757498437942,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.35087064676616914,"timestamp":1757498437942,"step":0,"task":null},{"name":"AUC-ROC","value":0.9157294953789106,"timestamp":1757498437798,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8470588235294116,"timestamp":1757498438362,"step":340000,"task":null},{"name":"Well Calibration","value":0.37583703205332863,"timestamp":1757498437942,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6035476363756958,"timestamp":1757498437798,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.612109000008786,"timestamp":1757498437736,"step":12,"task":null},{"name":"Equal Negative Predictive Value","value":0.4105139652014652,"timestamp":1757498437942,"step":0,"task":null},{"name":"Cohen Kappa","value":0.5990888834852688,"timestamp":1757498437798,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9758597883597884,"timestamp":1757498437942,"step":0,"task":null},{"name":"Equalized Odds","value":0.8971162515426377,"timestamp":1757498437942,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.1825204159404517,"timestamp":1757498437942,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498437736,"step":12,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498437736,"step":12,"task":null},{"name":"Accuracy","value":0.8570084666039511,"timestamp":1757498437798,"step":0,"task":null},{"name":"F1 Score","value":0.6910569105691057,"timestamp":1757498437798,"step":0,"task":null},{"name":"Log Loss","value":0.31789133810315273,"timestamp":1757498437798,"step":0,"task":null},{"name":"Treatment Equality","value":0.37150698840748375,"timestamp":1757498437942,"step":0,"task":null},{"name":"Precision","value":0.7623318385650224,"timestamp":1757498437798,"step":0,"task":null},{"name":"Disparate Impact","value":0.35087064676616914,"timestamp":1757498437942,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.4,"timestamp":1757498437736,"step":12,"task":null},{"name":"system_network_receive_megabytes","value":5.704835999989882,"timestamp":1757498437736,"step":12,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d7ef31f520564126b5d1137724ab7f7f/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_22","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"d7ef31f520564126b5d1137724ab7f7f\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:00:34.705635\", \"model_uuid\": \"754ae014e8414435984cf5874a824732\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"f18f4e6cd0e94383b3368e48c551cd54","name":"Run_21","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498416647,"endTime":1757498421504,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.2804405431148078,"timestamp":1757498420937,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.39544792400375306,"timestamp":1757498420937,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8594053041996775,"timestamp":1757498420937,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.5,"timestamp":1757498420885,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.826488044741245,"timestamp":1757498420937,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9591764705882353,"timestamp":1757498421333,"step":340000,"task":null},{"name":"Recall","value":0.6394052044609665,"timestamp":1757498420763,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29856.3,"timestamp":1757498420885,"step":3,"task":null},{"name":"Test Fairness","value":0.33625730994152037,"timestamp":1757498420937,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498420885,"step":3,"task":null},{"name":"Statistical Parity","value":0.8064274834076814,"timestamp":1757498420937,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3529964391241761,"timestamp":1757498420937,"step":0,"task":null},{"name":"AUC-ROC","value":0.9076343955128146,"timestamp":1757498420763,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8423529411764706,"timestamp":1757498421419,"step":340000,"task":null},{"name":"Well Calibration","value":0.3907713772348822,"timestamp":1757498420937,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.6002620113066142,"timestamp":1757498420763,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.058980000001611,"timestamp":1757498420885,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.40704432704901894,"timestamp":1757498420937,"step":0,"task":null},{"name":"Cohen Kappa","value":0.5969638478280137,"timestamp":1757498420763,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9515418502202644,"timestamp":1757498420937,"step":0,"task":null},{"name":"Equalized Odds","value":0.8594053041996775,"timestamp":1757498420937,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.12279540651950609,"timestamp":1757498420937,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498420885,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498420885,"step":3,"task":null},{"name":"Accuracy","value":0.8551269990592663,"timestamp":1757498420763,"step":0,"task":null},{"name":"F1 Score","value":0.6907630522088354,"timestamp":1757498420763,"step":0,"task":null},{"name":"Log Loss","value":0.3527816883531838,"timestamp":1757498420763,"step":0,"task":null},{"name":"Treatment Equality","value":0.30761980279994344,"timestamp":1757498420937,"step":0,"task":null},{"name":"Precision","value":0.7510917030567685,"timestamp":1757498420763,"step":0,"task":null},{"name":"Disparate Impact","value":0.3529964391241761,"timestamp":1757498420937,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":29.8,"timestamp":1757498420885,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.058771999989403,"timestamp":1757498420885,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/f18f4e6cd0e94383b3368e48c551cd54/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_21","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"f18f4e6cd0e94383b3368e48c551cd54\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:00:17.988159\", \"model_uuid\": \"8f6024e2b0664541bb62eb57445aa92f\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"d64d22d95963438887e253971f07e279","name":"Run_20","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498410273,"endTime":1757498416597,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.5926741754790524,"timestamp":1757498416048,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5543225718378456,"timestamp":1757498416048,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.3434236711772135,"timestamp":1757498416048,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.5,"timestamp":1757498415647,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.5450144986879073,"timestamp":1757498416048,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9440588235294116,"timestamp":1757498416420,"step":340000,"task":null},{"name":"Recall","value":0.4634146341463415,"timestamp":1757498415909,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29849.2,"timestamp":1757498415647,"step":4,"task":null},{"name":"Test Fairness","value":0.5548780487804879,"timestamp":1757498416048,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498415647,"step":4,"task":null},{"name":"Statistical Parity","value":0.8473007188516888,"timestamp":1757498416048,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.5039248120300752,"timestamp":1757498416048,"step":0,"task":null},{"name":"AUC-ROC","value":0.7794881367181019,"timestamp":1757498415909,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7832941176470588,"timestamp":1757498416510,"step":340000,"task":null},{"name":"Well Calibration","value":0.6587583921963329,"timestamp":1757498416048,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.38592817129095586,"timestamp":1757498415909,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":6.387893000006443,"timestamp":1757498415647,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.5520577011627341,"timestamp":1757498416048,"step":0,"task":null},{"name":"Cohen Kappa","value":0.37670486584606955,"timestamp":1757498415909,"step":0,"task":null},{"name":"Equal Opportunity","value":0.3644970414201183,"timestamp":1757498416048,"step":0,"task":null},{"name":"Equalized Odds","value":0.3434236711772135,"timestamp":1757498416048,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.440079863506008,"timestamp":1757498416048,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498415647,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498415647,"step":4,"task":null},{"name":"Accuracy","value":0.7544684854186265,"timestamp":1757498415909,"step":0,"task":null},{"name":"F1 Score","value":0.5380530973451327,"timestamp":1757498415909,"step":0,"task":null},{"name":"Log Loss","value":0.6363512498711555,"timestamp":1757498415909,"step":0,"task":null},{"name":"Treatment Equality","value":0.8980144355481633,"timestamp":1757498416048,"step":0,"task":null},{"name":"Precision","value":0.6413502109704642,"timestamp":1757498415909,"step":0,"task":null},{"name":"Disparate Impact","value":0.5039248120300752,"timestamp":1757498416048,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":30.2,"timestamp":1757498415647,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":6.2408330000180285,"timestamp":1757498415647,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/d64d22d95963438887e253971f07e279/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64d22d95963438887e253971f07e279/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64d22d95963438887e253971f07e279/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64d22d95963438887e253971f07e279/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64d22d95963438887e253971f07e279/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64d22d95963438887e253971f07e279/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64d22d95963438887e253971f07e279/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64d22d95963438887e253971f07e279/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64d22d95963438887e253971f07e279/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64d22d95963438887e253971f07e279/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64d22d95963438887e253971f07e279/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64d22d95963438887e253971f07e279/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64d22d95963438887e253971f07e279/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64d22d95963438887e253971f07e279/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64d22d95963438887e253971f07e279/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64d22d95963438887e253971f07e279/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_20","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"d64d22d95963438887e253971f07e279\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:00:12.559833\", \"model_uuid\": \"1981a69f31ab489f8888ea70db368a9a\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"ccfb9827c56a4bd5bc2a3faf00861185","name":"Run_19","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498392278,"endTime":1757498410238,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.3715837196271204,"timestamp":1757498409650,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40057482566953584,"timestamp":1757498409650,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8999484298987682,"timestamp":1757498409650,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.5,"timestamp":1757498409656,"step":13,"task":null},{"name":"Balance for Negative Class","value":0.9428252653171862,"timestamp":1757498409650,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9654705882352941,"timestamp":1757498410085,"step":340000,"task":null},{"name":"Recall","value":0.6282527881040892,"timestamp":1757498409496,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29848.2,"timestamp":1757498409656,"step":13,"task":null},{"name":"Test Fairness","value":0.2666666666666667,"timestamp":1757498409650,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498409656,"step":13,"task":null},{"name":"Statistical Parity","value":0.8098295618854829,"timestamp":1757498409650,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3490526641404377,"timestamp":1757498409650,"step":0,"task":null},{"name":"AUC-ROC","value":0.9150248611800399,"timestamp":1757498409496,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.8470588235294118,"timestamp":1757498410155,"step":340000,"task":null},{"name":"Well Calibration","value":0.3789517676272049,"timestamp":1757498409650,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.5959093627702298,"timestamp":1757498409496,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":6.5708900000026915,"timestamp":1757498409656,"step":13,"task":null},{"name":"Equal Negative Predictive Value","value":0.4094654696290211,"timestamp":1757498409650,"step":0,"task":null},{"name":"Cohen Kappa","value":0.5917080290523781,"timestamp":1757498409496,"step":0,"task":null},{"name":"Equal Opportunity","value":0.9826839826839827,"timestamp":1757498409650,"step":0,"task":null},{"name":"Equalized Odds","value":0.8999484298987682,"timestamp":1757498409650,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.1740027965298973,"timestamp":1757498409650,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498409656,"step":13,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498409656,"step":13,"task":null},{"name":"Accuracy","value":0.8541862652869238,"timestamp":1757498409496,"step":0,"task":null},{"name":"F1 Score","value":0.6855983772819473,"timestamp":1757498409496,"step":0,"task":null},{"name":"Log Loss","value":0.3183710906833347,"timestamp":1757498409496,"step":0,"task":null},{"name":"Treatment Equality","value":0.36094365717083726,"timestamp":1757498409650,"step":0,"task":null},{"name":"Precision","value":0.7544642857142857,"timestamp":1757498409496,"step":0,"task":null},{"name":"Disparate Impact","value":0.3490526641404377,"timestamp":1757498409650,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":15.7,"timestamp":1757498409656,"step":13,"task":null},{"name":"system_network_receive_megabytes","value":6.355409000010695,"timestamp":1757498409656,"step":13,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ccfb9827c56a4bd5bc2a3faf00861185/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_19","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"ccfb9827c56a4bd5bc2a3faf00861185\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 10:00:06.292668\", \"model_uuid\": \"8f76d0cb4201407bb8016c042b5a262a\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"5abcb2fe08704e74b23e8c3b06750629","name":"Run_18","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498387493,"endTime":1757498392224,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"20","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.2804405431148078,"timestamp":1757498391739,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.3966993414847776,"timestamp":1757498391739,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.8946475949485762,"timestamp":1757498391739,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.5,"timestamp":1757498391788,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.8855229050799055,"timestamp":1757498391739,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.9616470588235293,"timestamp":1757498392054,"step":340000,"task":null},{"name":"Recall","value":0.6356877323420075,"timestamp":1757498391617,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29819.5,"timestamp":1757498391788,"step":3,"task":null},{"name":"Test Fairness","value":0.3504273504273504,"timestamp":1757498391739,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498391788,"step":3,"task":null},{"name":"Statistical Parity","value":0.8037747614227877,"timestamp":1757498391739,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.3419653004015456,"timestamp":1757498391739,"step":0,"task":null},{"name":"AUC-ROC","value":0.9072926128117011,"timestamp":1757498391617,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.847764705882353,"timestamp":1757498392139,"step":340000,"task":null},{"name":"Well Calibration","value":0.39548864249896926,"timestamp":1757498391739,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.5972790753750414,"timestamp":1757498391617,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.062542999978177,"timestamp":1757498391788,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.4083877076663424,"timestamp":1757498391739,"step":0,"task":null},{"name":"Cohen Kappa","value":0.5938217218418971,"timestamp":1757498391617,"step":0,"task":null},{"name":"Equal Opportunity","value":0.986784140969163,"timestamp":1757498391739,"step":0,"task":null},{"name":"Equalized Odds","value":0.8946475949485762,"timestamp":1757498391739,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.12734338453874705,"timestamp":1757498391739,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498391788,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498391788,"step":3,"task":null},{"name":"Accuracy","value":0.8541862652869238,"timestamp":1757498391617,"step":0,"task":null},{"name":"F1 Score","value":0.6881287726358148,"timestamp":1757498391617,"step":0,"task":null},{"name":"Log Loss","value":0.35289215710154204,"timestamp":1757498391617,"step":0,"task":null},{"name":"Treatment Equality","value":0.28711181594661384,"timestamp":1757498391739,"step":0,"task":null},{"name":"Precision","value":0.75,"timestamp":1757498391617,"step":0,"task":null},{"name":"Disparate Impact","value":0.3419653004015456,"timestamp":1757498391739,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":42.2,"timestamp":1757498391788,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.062314999988303,"timestamp":1757498391788,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5abcb2fe08704e74b23e8c3b06750629/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_18","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"5abcb2fe08704e74b23e8c3b06750629\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:59:48.835971\", \"model_uuid\": \"e9584ec61d8a46878a364b604fb90ab0\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"7217b5445f864cc9aee09bc2def8c240","name":"Run_17","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498382784,"endTime":1757498387445,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498386801,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5336388039027056,"timestamp":1757498386801,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.0011701047922579588,"timestamp":1757498386801,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.5,"timestamp":1757498387007,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.7021914886146176,"timestamp":1757498386801,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.751235294117647,"timestamp":1757498387294,"step":340000,"task":null},{"name":"Recall","value":0.22560975609756098,"timestamp":1757498386708,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29824.1,"timestamp":1757498387007,"step":3,"task":null},{"name":"Test Fairness","value":0.2334675847732883,"timestamp":1757498386801,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498387007,"step":3,"task":null},{"name":"Statistical Parity","value":0.8790895244678243,"timestamp":1757498386801,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.02314935064935065,"timestamp":1757498386801,"step":0,"task":null},{"name":"AUC-ROC","value":0.770617637298822,"timestamp":1757498386708,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7454117647058824,"timestamp":1757498387370,"step":340000,"task":null},{"name":"Well Calibration","value":0.8085451800794128,"timestamp":1757498386801,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3422053832795047,"timestamp":1757498386708,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.067352999991272,"timestamp":1757498387007,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5281620086888106,"timestamp":1757498386801,"step":0,"task":null},{"name":"Cohen Kappa","value":0.25706246248899023,"timestamp":1757498386708,"step":0,"task":null},{"name":"Equal Opportunity","value":0.029504741833508957,"timestamp":1757498386801,"step":0,"task":null},{"name":"Equalized Odds","value":-0.0011701047922579588,"timestamp":1757498386801,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498386801,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498387007,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498387007,"step":3,"task":null},{"name":"Accuracy","value":0.7469426152398871,"timestamp":1757498386708,"step":0,"task":null},{"name":"F1 Score","value":0.354916067146283,"timestamp":1757498386708,"step":0,"task":null},{"name":"Log Loss","value":0.533826091726627,"timestamp":1757498386708,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498386801,"step":0,"task":null},{"name":"Precision","value":0.8314606741573034,"timestamp":1757498386708,"step":0,"task":null},{"name":"Disparate Impact","value":0.02314935064935065,"timestamp":1757498386801,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":29.8,"timestamp":1757498387007,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.067326999997022,"timestamp":1757498387007,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/7217b5445f864cc9aee09bc2def8c240/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_17","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"7217b5445f864cc9aee09bc2def8c240\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:59:44.140559\", \"model_uuid\": \"3f831dd2698d414a8d85b93c5d24f3ef\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"a583ba70378041e8ad703ce720c13b15","name":"Run_16","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498364961,"endTime":1757498382733,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498382013,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.4004555496271688,"timestamp":1757498382013,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.08362210204288759,"timestamp":1757498382013,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.4,"timestamp":1757498382388,"step":13,"task":null},{"name":"Balance for Negative Class","value":0.7642407577516243,"timestamp":1757498382013,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7902941176470588,"timestamp":1757498382561,"step":340000,"task":null},{"name":"Recall","value":0.22304832713754646,"timestamp":1757498381899,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29804.7,"timestamp":1757498382388,"step":13,"task":null},{"name":"Test Fairness","value":0.46470588235294114,"timestamp":1757498382013,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498382388,"step":13,"task":null},{"name":"Statistical Parity","value":0.9161964532473514,"timestamp":1757498382013,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.03449419568822554,"timestamp":1757498382013,"step":0,"task":null},{"name":"AUC-ROC","value":0.8837774947796203,"timestamp":1757498381899,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.788235294117647,"timestamp":1757498382648,"step":340000,"task":null},{"name":"Well Calibration","value":0.4946057576832021,"timestamp":1757498382013,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3984658973078198,"timestamp":1757498381899,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":7.348347000021022,"timestamp":1757498382388,"step":13,"task":null},{"name":"Equal Negative Predictive Value","value":0.3920369642931167,"timestamp":1757498382013,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2914375663048002,"timestamp":1757498381899,"step":0,"task":null},{"name":"Equal Opportunity","value":0.09160613397901532,"timestamp":1757498382013,"step":0,"task":null},{"name":"Equalized Odds","value":0.08362210204288759,"timestamp":1757498382013,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498382013,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498382388,"step":13,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498382388,"step":13,"task":null},{"name":"Accuracy","value":0.7996237064910631,"timestamp":1757498381899,"step":0,"task":null},{"name":"F1 Score","value":0.36036036036036034,"timestamp":1757498381899,"step":0,"task":null},{"name":"Log Loss","value":0.42177422022505107,"timestamp":1757498381899,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498382013,"step":0,"task":null},{"name":"Precision","value":0.9375,"timestamp":1757498381899,"step":0,"task":null},{"name":"Disparate Impact","value":0.03449419568822554,"timestamp":1757498382013,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":21.5,"timestamp":1757498382388,"step":13,"task":null},{"name":"system_network_receive_megabytes","value":7.327763999986928,"timestamp":1757498382388,"step":13,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a583ba70378041e8ad703ce720c13b15/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_16","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"a583ba70378041e8ad703ce720c13b15\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:59:38.420246\", \"model_uuid\": \"d985d3d0ee234c0381ef4545fd18c379\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"6d9917df9ee24a17811f5a5323926c82","name":"Run_15","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498359155,"endTime":1757498364914,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498364185,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.4004555496271688,"timestamp":1757498364185,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.08362210204288759,"timestamp":1757498364185,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.4,"timestamp":1757498364427,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.7642407577516243,"timestamp":1757498364185,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7881764705882353,"timestamp":1757498364736,"step":340000,"task":null},{"name":"Recall","value":0.22304832713754646,"timestamp":1757498364057,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29796.8,"timestamp":1757498364427,"step":4,"task":null},{"name":"Test Fairness","value":0.4777777777777778,"timestamp":1757498364185,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498364427,"step":4,"task":null},{"name":"Statistical Parity","value":0.9161964532473514,"timestamp":1757498364185,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.03449419568822554,"timestamp":1757498364185,"step":0,"task":null},{"name":"AUC-ROC","value":0.8836838556834249,"timestamp":1757498364057,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7861176470588236,"timestamp":1757498364824,"step":340000,"task":null},{"name":"Well Calibration","value":0.49491325498963495,"timestamp":1757498364185,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3984658973078198,"timestamp":1757498364057,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.1060339999967255,"timestamp":1757498364427,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.3920369642931167,"timestamp":1757498364185,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2914375663048002,"timestamp":1757498364057,"step":0,"task":null},{"name":"Equal Opportunity","value":0.09160613397901532,"timestamp":1757498364185,"step":0,"task":null},{"name":"Equalized Odds","value":0.08362210204288759,"timestamp":1757498364185,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498364185,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498364427,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498364427,"step":4,"task":null},{"name":"Accuracy","value":0.7996237064910631,"timestamp":1757498364057,"step":0,"task":null},{"name":"F1 Score","value":0.36036036036036034,"timestamp":1757498364057,"step":0,"task":null},{"name":"Log Loss","value":0.42175285527749257,"timestamp":1757498364057,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498364185,"step":0,"task":null},{"name":"Precision","value":0.9375,"timestamp":1757498364057,"step":0,"task":null},{"name":"Disparate Impact","value":0.03449419568822554,"timestamp":1757498364185,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":20.1,"timestamp":1757498364427,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.0988609999767505,"timestamp":1757498364427,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/6d9917df9ee24a17811f5a5323926c82/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_15","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"6d9917df9ee24a17811f5a5323926c82\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:59:20.645514\", \"model_uuid\": \"495ae7f139dd4b51850bd6249ca25815\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"a0e36456ed92493caa7c5825e2590bbd","name":"Run_14","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498353487,"endTime":1757498359098,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498358357,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5336388039027056,"timestamp":1757498358357,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.0011701047922579588,"timestamp":1757498358357,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.4,"timestamp":1757498358768,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.7021914886146176,"timestamp":1757498358357,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7537058823529411,"timestamp":1757498358924,"step":340000,"task":null},{"name":"Recall","value":0.22560975609756098,"timestamp":1757498358236,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29796.1,"timestamp":1757498358768,"step":4,"task":null},{"name":"Test Fairness","value":0.2334675847732883,"timestamp":1757498358357,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498358768,"step":4,"task":null},{"name":"Statistical Parity","value":0.8790895244678243,"timestamp":1757498358357,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.02314935064935065,"timestamp":1757498358357,"step":0,"task":null},{"name":"AUC-ROC","value":0.7705803052928488,"timestamp":1757498358236,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7489411764705882,"timestamp":1757498359018,"step":340000,"task":null},{"name":"Well Calibration","value":0.8084786344658639,"timestamp":1757498358357,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3422053832795047,"timestamp":1757498358236,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.223162000009324,"timestamp":1757498358768,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.5281620086888106,"timestamp":1757498358357,"step":0,"task":null},{"name":"Cohen Kappa","value":0.25706246248899023,"timestamp":1757498358236,"step":0,"task":null},{"name":"Equal Opportunity","value":0.029504741833508957,"timestamp":1757498358357,"step":0,"task":null},{"name":"Equalized Odds","value":-0.0011701047922579588,"timestamp":1757498358357,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498358357,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498358768,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498358768,"step":4,"task":null},{"name":"Accuracy","value":0.7469426152398871,"timestamp":1757498358236,"step":0,"task":null},{"name":"F1 Score","value":0.354916067146283,"timestamp":1757498358236,"step":0,"task":null},{"name":"Log Loss","value":0.5338377683077884,"timestamp":1757498358236,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498358357,"step":0,"task":null},{"name":"Precision","value":0.8314606741573034,"timestamp":1757498358236,"step":0,"task":null},{"name":"Disparate Impact","value":0.02314935064935065,"timestamp":1757498358357,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":14.1,"timestamp":1757498358768,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.228728000016417,"timestamp":1757498358768,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a0e36456ed92493caa7c5825e2590bbd/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_14","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"a0e36456ed92493caa7c5825e2590bbd\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:59:14.975339\", \"model_uuid\": \"7c988be15e714313b599c571cc7bdc8c\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"b0b26ae8a9ef4e8e983550f2dc031fc2","name":"Run_13","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498335710,"endTime":1757498353437,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498352739,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.4004555496271688,"timestamp":1757498352739,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.08362210204288759,"timestamp":1757498352739,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.4,"timestamp":1757498352325,"step":12,"task":null},{"name":"Balance for Negative Class","value":0.7642407577516243,"timestamp":1757498352739,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7942352941176469,"timestamp":1757498353264,"step":340000,"task":null},{"name":"Recall","value":0.22304832713754646,"timestamp":1757498352608,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29766.5,"timestamp":1757498352325,"step":12,"task":null},{"name":"Test Fairness","value":0.46470588235294114,"timestamp":1757498352739,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498352325,"step":12,"task":null},{"name":"Statistical Parity","value":0.9161964532473514,"timestamp":1757498352739,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.03449419568822554,"timestamp":1757498352739,"step":0,"task":null},{"name":"AUC-ROC","value":0.8837774947796203,"timestamp":1757498352608,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7858823529411765,"timestamp":1757498353352,"step":340000,"task":null},{"name":"Well Calibration","value":0.4946057576832021,"timestamp":1757498352739,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3984658973078198,"timestamp":1757498352608,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":7.113252000010107,"timestamp":1757498352325,"step":12,"task":null},{"name":"Equal Negative Predictive Value","value":0.3920369642931167,"timestamp":1757498352739,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2914375663048002,"timestamp":1757498352608,"step":0,"task":null},{"name":"Equal Opportunity","value":0.09160613397901532,"timestamp":1757498352739,"step":0,"task":null},{"name":"Equalized Odds","value":0.08362210204288759,"timestamp":1757498352739,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498352739,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498352325,"step":12,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498352325,"step":12,"task":null},{"name":"Accuracy","value":0.7996237064910631,"timestamp":1757498352608,"step":0,"task":null},{"name":"F1 Score","value":0.36036036036036034,"timestamp":1757498352608,"step":0,"task":null},{"name":"Log Loss","value":0.42177422022505107,"timestamp":1757498352608,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498352739,"step":0,"task":null},{"name":"Precision","value":0.9375,"timestamp":1757498352608,"step":0,"task":null},{"name":"Disparate Impact","value":0.03449419568822554,"timestamp":1757498352739,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":30.1,"timestamp":1757498352325,"step":12,"task":null},{"name":"system_network_receive_megabytes","value":6.890374999988126,"timestamp":1757498352325,"step":12,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/b0b26ae8a9ef4e8e983550f2dc031fc2/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_13","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"b0b26ae8a9ef4e8e983550f2dc031fc2\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:59:09.241134\", \"model_uuid\": \"f8d37ed96c024a739d1ee4c80813008f\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"ee522b58206c43b586277f2c4fba3d1b","name":"Run_12","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498330127,"endTime":1757498335664,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"100","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498334912,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.4004555496271688,"timestamp":1757498334912,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.08362210204288759,"timestamp":1757498334912,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.3,"timestamp":1757498335431,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.7642407577516243,"timestamp":1757498334912,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7887058823529413,"timestamp":1757498335494,"step":340000,"task":null},{"name":"Recall","value":0.22304832713754646,"timestamp":1757498334780,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29632.4,"timestamp":1757498335431,"step":4,"task":null},{"name":"Test Fairness","value":0.4777777777777778,"timestamp":1757498334912,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498335431,"step":4,"task":null},{"name":"Statistical Parity","value":0.9161964532473514,"timestamp":1757498334912,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.03449419568822554,"timestamp":1757498334912,"step":0,"task":null},{"name":"AUC-ROC","value":0.8836838556834249,"timestamp":1757498334780,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7865882352941177,"timestamp":1757498335580,"step":340000,"task":null},{"name":"Well Calibration","value":0.49491325498963495,"timestamp":1757498334912,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3984658973078198,"timestamp":1757498334780,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":7.006391000002623,"timestamp":1757498335431,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.3920369642931167,"timestamp":1757498334912,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2914375663048002,"timestamp":1757498334780,"step":0,"task":null},{"name":"Equal Opportunity","value":0.09160613397901532,"timestamp":1757498334912,"step":0,"task":null},{"name":"Equalized Odds","value":0.08362210204288759,"timestamp":1757498334912,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498334912,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498335431,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498335431,"step":4,"task":null},{"name":"Accuracy","value":0.7996237064910631,"timestamp":1757498334780,"step":0,"task":null},{"name":"F1 Score","value":0.36036036036036034,"timestamp":1757498334780,"step":0,"task":null},{"name":"Log Loss","value":0.42175285527749257,"timestamp":1757498334780,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498334912,"step":0,"task":null},{"name":"Precision","value":0.9375,"timestamp":1757498334780,"step":0,"task":null},{"name":"Disparate Impact","value":0.03449419568822554,"timestamp":1757498334912,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":8.3,"timestamp":1757498335431,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":6.400489999999991,"timestamp":1757498335431,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/ee522b58206c43b586277f2c4fba3d1b/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_12","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"ee522b58206c43b586277f2c4fba3d1b\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:58:51.648734\", \"model_uuid\": \"bce165cc3ffe455ba1f1498b9946991b\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"e8da56d7f81e4497a83cf5dff75d467f","name":"Run_11","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498324932,"endTime":1757498330079,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.2382025005124001,"timestamp":1757498329525,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5348285279048074,"timestamp":1757498329525,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.0015937564194334897,"timestamp":1757498329525,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.4,"timestamp":1757498329248,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.6976493486034909,"timestamp":1757498329525,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7486470588235294,"timestamp":1757498329908,"step":340000,"task":null},{"name":"Recall","value":0.22865853658536586,"timestamp":1757498329409,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29749.8,"timestamp":1757498329248,"step":3,"task":null},{"name":"Test Fairness","value":0.2702702702702703,"timestamp":1757498329525,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498329248,"step":3,"task":null},{"name":"Statistical Parity","value":0.8773838887008109,"timestamp":1757498329525,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.0447723704866562,"timestamp":1757498329525,"step":0,"task":null},{"name":"AUC-ROC","value":0.7686971129915381,"timestamp":1757498329409,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7468235294117647,"timestamp":1757498329994,"step":340000,"task":null},{"name":"Well Calibration","value":0.8004424449100239,"timestamp":1757498329525,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.33376631203831963,"timestamp":1757498329409,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.224014000006719,"timestamp":1757498329248,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5291067329553325,"timestamp":1757498329525,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2546912797692198,"timestamp":1757498329409,"step":0,"task":null},{"name":"Equal Opportunity","value":0.029106029106029108,"timestamp":1757498329525,"step":0,"task":null},{"name":"Equalized Odds","value":-0.0015937564194334897,"timestamp":1757498329525,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.24891904646505877,"timestamp":1757498329525,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498329248,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498329248,"step":3,"task":null},{"name":"Accuracy","value":0.7450611476952023,"timestamp":1757498329409,"step":0,"task":null},{"name":"F1 Score","value":0.35629453681710216,"timestamp":1757498329409,"step":0,"task":null},{"name":"Log Loss","value":0.5334582626891378,"timestamp":1757498329409,"step":0,"task":null},{"name":"Treatment Equality","value":0.16105924965297508,"timestamp":1757498329525,"step":0,"task":null},{"name":"Precision","value":0.8064516129032258,"timestamp":1757498329409,"step":0,"task":null},{"name":"Disparate Impact","value":0.0447723704866562,"timestamp":1757498329525,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.0,"timestamp":1757498329248,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.178039999998873,"timestamp":1757498329248,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/e8da56d7f81e4497a83cf5dff75d467f/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_11","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"e8da56d7f81e4497a83cf5dff75d467f\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:58:46.275592\", \"model_uuid\": \"e27788d788f648fbab2f45e72e49f55e\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"a5ae6d2e159f40229c46b8f0596d79d6","name":"Run_10","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498308753,"endTime":1757498324879,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498324304,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40189603721575584,"timestamp":1757498324304,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.07047734887368116,"timestamp":1757498324304,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.3,"timestamp":1757498324859,"step":12,"task":null},{"name":"Balance for Negative Class","value":0.7459985061959807,"timestamp":1757498324304,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7894705882352941,"timestamp":1757498324726,"step":340000,"task":null},{"name":"Recall","value":0.241635687732342,"timestamp":1757498324176,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29627.0,"timestamp":1757498324859,"step":12,"task":null},{"name":"Test Fairness","value":0.3979591836734694,"timestamp":1757498324304,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498324859,"step":12,"task":null},{"name":"Statistical Parity","value":0.9051745410278345,"timestamp":1757498324304,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.03060752575152407,"timestamp":1757498324304,"step":0,"task":null},{"name":"AUC-ROC","value":0.8850931240811662,"timestamp":1757498324176,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.788235294117647,"timestamp":1757498324811,"step":340000,"task":null},{"name":"Well Calibration","value":0.50520698101618,"timestamp":1757498324304,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40281236420433,"timestamp":1757498324176,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.8244160000176635,"timestamp":1757498324859,"step":12,"task":null},{"name":"Equal Negative Predictive Value","value":0.394415390792033,"timestamp":1757498324304,"step":0,"task":null},{"name":"Cohen Kappa","value":0.30719666900388265,"timestamp":1757498324176,"step":0,"task":null},{"name":"Equal Opportunity","value":0.08444940476190475,"timestamp":1757498324304,"step":0,"task":null},{"name":"Equalized Odds","value":0.07047734887368116,"timestamp":1757498324304,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498324304,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498324859,"step":12,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498324859,"step":12,"task":null},{"name":"Accuracy","value":0.8015051740357478,"timestamp":1757498324176,"step":0,"task":null},{"name":"F1 Score","value":0.3812316715542522,"timestamp":1757498324176,"step":0,"task":null},{"name":"Log Loss","value":0.42121400614779814,"timestamp":1757498324176,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498324304,"step":0,"task":null},{"name":"Precision","value":0.9027777777777778,"timestamp":1757498324176,"step":0,"task":null},{"name":"Disparate Impact","value":0.03060752575152407,"timestamp":1757498324304,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":15.3,"timestamp":1757498324859,"step":12,"task":null},{"name":"system_network_receive_megabytes","value":5.620510999986436,"timestamp":1757498324859,"step":12,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/a5ae6d2e159f40229c46b8f0596d79d6/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_10","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"a5ae6d2e159f40229c46b8f0596d79d6\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:58:41.405667\", \"model_uuid\": \"b3fe48fe459e4057b97f513ab4a1044c\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"345433679b77435393beeab2e472f77d","name":"Run_9","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498303414,"endTime":1757498308705,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498308089,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40189603721575584,"timestamp":1757498308089,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.07047734887368116,"timestamp":1757498308089,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.4,"timestamp":1757498307663,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.7459985061959807,"timestamp":1757498308089,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7907058823529411,"timestamp":1757498308531,"step":340000,"task":null},{"name":"Recall","value":0.241635687732342,"timestamp":1757498307980,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29731.4,"timestamp":1757498307663,"step":3,"task":null},{"name":"Test Fairness","value":0.39,"timestamp":1757498308089,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498307663,"step":3,"task":null},{"name":"Statistical Parity","value":0.9051745410278345,"timestamp":1757498308089,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.03060752575152407,"timestamp":1757498308089,"step":0,"task":null},{"name":"AUC-ROC","value":0.884978416188327,"timestamp":1757498307980,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7856470588235294,"timestamp":1757498308620,"step":340000,"task":null},{"name":"Well Calibration","value":0.5053599608809243,"timestamp":1757498308089,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40281236420433,"timestamp":1757498307980,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.05936399998609,"timestamp":1757498307663,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.394415390792033,"timestamp":1757498308089,"step":0,"task":null},{"name":"Cohen Kappa","value":0.30719666900388265,"timestamp":1757498307980,"step":0,"task":null},{"name":"Equal Opportunity","value":0.08444940476190475,"timestamp":1757498308089,"step":0,"task":null},{"name":"Equalized Odds","value":0.07047734887368116,"timestamp":1757498308089,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498308089,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498307663,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498307663,"step":3,"task":null},{"name":"Accuracy","value":0.8015051740357478,"timestamp":1757498307980,"step":0,"task":null},{"name":"F1 Score","value":0.3812316715542522,"timestamp":1757498307980,"step":0,"task":null},{"name":"Log Loss","value":0.4211209316340222,"timestamp":1757498307980,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498308089,"step":0,"task":null},{"name":"Precision","value":0.9027777777777778,"timestamp":1757498307980,"step":0,"task":null},{"name":"Disparate Impact","value":0.03060752575152407,"timestamp":1757498308089,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.8,"timestamp":1757498307663,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.057276000006823,"timestamp":1757498307663,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/345433679b77435393beeab2e472f77d/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/345433679b77435393beeab2e472f77d/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/345433679b77435393beeab2e472f77d/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/345433679b77435393beeab2e472f77d/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/345433679b77435393beeab2e472f77d/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/345433679b77435393beeab2e472f77d/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/345433679b77435393beeab2e472f77d/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/345433679b77435393beeab2e472f77d/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/345433679b77435393beeab2e472f77d/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/345433679b77435393beeab2e472f77d/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/345433679b77435393beeab2e472f77d/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/345433679b77435393beeab2e472f77d/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/345433679b77435393beeab2e472f77d/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/345433679b77435393beeab2e472f77d/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/345433679b77435393beeab2e472f77d/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/345433679b77435393beeab2e472f77d/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_9","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"345433679b77435393beeab2e472f77d\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:58:24.798288\", \"model_uuid\": \"dc427b86f60f404ca8e3b5d1681bcd5a\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"bd4291ccb6d441c9965d711a94a354dc","name":"Run_8","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498298169,"endTime":1757498303366,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.2382025005124001,"timestamp":1757498302799,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.5348285279048074,"timestamp":1757498302799,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.0015937564194334897,"timestamp":1757498302799,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.4,"timestamp":1757498302489,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.6976493486034909,"timestamp":1757498302799,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7528823529411764,"timestamp":1757498303190,"step":340000,"task":null},{"name":"Recall","value":0.22865853658536586,"timestamp":1757498302672,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29739.4,"timestamp":1757498302489,"step":3,"task":null},{"name":"Test Fairness","value":0.2702702702702703,"timestamp":1757498302799,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498302489,"step":3,"task":null},{"name":"Statistical Parity","value":0.8773838887008109,"timestamp":1757498302799,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.0447723704866562,"timestamp":1757498302799,"step":0,"task":null},{"name":"AUC-ROC","value":0.7686473369835739,"timestamp":1757498302672,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7487058823529412,"timestamp":1757498303281,"step":340000,"task":null},{"name":"Well Calibration","value":0.8003126024800133,"timestamp":1757498302799,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.33376631203831963,"timestamp":1757498302672,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.062650000007125,"timestamp":1757498302489,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.5291067329553325,"timestamp":1757498302799,"step":0,"task":null},{"name":"Cohen Kappa","value":0.2546912797692198,"timestamp":1757498302672,"step":0,"task":null},{"name":"Equal Opportunity","value":0.029106029106029108,"timestamp":1757498302799,"step":0,"task":null},{"name":"Equalized Odds","value":-0.0015937564194334897,"timestamp":1757498302799,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.24891904646505877,"timestamp":1757498302799,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498302489,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498302489,"step":3,"task":null},{"name":"Accuracy","value":0.7450611476952023,"timestamp":1757498302672,"step":0,"task":null},{"name":"F1 Score","value":0.35629453681710216,"timestamp":1757498302672,"step":0,"task":null},{"name":"Log Loss","value":0.5334812565049954,"timestamp":1757498302672,"step":0,"task":null},{"name":"Treatment Equality","value":0.16105924965297508,"timestamp":1757498302799,"step":0,"task":null},{"name":"Precision","value":0.8064516129032258,"timestamp":1757498302672,"step":0,"task":null},{"name":"Disparate Impact","value":0.0447723704866562,"timestamp":1757498302799,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":29.2,"timestamp":1757498302489,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.0607639999943785,"timestamp":1757498302489,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/bd4291ccb6d441c9965d711a94a354dc/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_8","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"bd4291ccb6d441c9965d711a94a354dc\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:58:19.568479\", \"model_uuid\": \"7ae8e1efa9aa49bd8af3421101133e59\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"d64205509d314de3b9d655b064bdbc22","name":"Run_7","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498281414,"endTime":1757498298117,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498297543,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40189603721575584,"timestamp":1757498297543,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.07047734887368116,"timestamp":1757498297543,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.4,"timestamp":1757498297753,"step":12,"task":null},{"name":"Balance for Negative Class","value":0.7459985061959807,"timestamp":1757498297543,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7897058823529411,"timestamp":1757498297948,"step":340000,"task":null},{"name":"Recall","value":0.241635687732342,"timestamp":1757498297414,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29744.7,"timestamp":1757498297753,"step":12,"task":null},{"name":"Test Fairness","value":0.3979591836734694,"timestamp":1757498297543,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498297753,"step":12,"task":null},{"name":"Statistical Parity","value":0.9051745410278345,"timestamp":1757498297543,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.03060752575152407,"timestamp":1757498297543,"step":0,"task":null},{"name":"AUC-ROC","value":0.8850931240811662,"timestamp":1757498297414,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7856470588235295,"timestamp":1757498298032,"step":340000,"task":null},{"name":"Well Calibration","value":0.50520698101618,"timestamp":1757498297543,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40281236420433,"timestamp":1757498297414,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.7357670000055805,"timestamp":1757498297753,"step":12,"task":null},{"name":"Equal Negative Predictive Value","value":0.394415390792033,"timestamp":1757498297543,"step":0,"task":null},{"name":"Cohen Kappa","value":0.30719666900388265,"timestamp":1757498297414,"step":0,"task":null},{"name":"Equal Opportunity","value":0.08444940476190475,"timestamp":1757498297543,"step":0,"task":null},{"name":"Equalized Odds","value":0.07047734887368116,"timestamp":1757498297543,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498297543,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498297753,"step":12,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498297753,"step":12,"task":null},{"name":"Accuracy","value":0.8015051740357478,"timestamp":1757498297414,"step":0,"task":null},{"name":"F1 Score","value":0.3812316715542522,"timestamp":1757498297414,"step":0,"task":null},{"name":"Log Loss","value":0.42121400614779814,"timestamp":1757498297414,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498297543,"step":0,"task":null},{"name":"Precision","value":0.9027777777777778,"timestamp":1757498297414,"step":0,"task":null},{"name":"Disparate Impact","value":0.03060752575152407,"timestamp":1757498297543,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.4,"timestamp":1757498297753,"step":12,"task":null},{"name":"system_network_receive_megabytes","value":5.605608999991091,"timestamp":1757498297753,"step":12,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/d64205509d314de3b9d655b064bdbc22/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_7","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"d64205509d314de3b9d655b064bdbc22\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:58:14.598419\", \"model_uuid\": \"cbf140974d334c488054c5722911ae17\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"4a95843dfb04464d95249b9708b99f11","name":"Run_6","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498275929,"endTime":1757498281360,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"50","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498280602,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40189603721575584,"timestamp":1757498280602,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.07047734887368116,"timestamp":1757498280602,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.3,"timestamp":1757498281333,"step":4,"task":null},{"name":"Balance for Negative Class","value":0.7459985061959807,"timestamp":1757498280602,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7886470588235295,"timestamp":1757498281205,"step":340000,"task":null},{"name":"Recall","value":0.241635687732342,"timestamp":1757498280476,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29689.7,"timestamp":1757498281333,"step":4,"task":null},{"name":"Test Fairness","value":0.39,"timestamp":1757498280602,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498281333,"step":4,"task":null},{"name":"Statistical Parity","value":0.9051745410278345,"timestamp":1757498280602,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.03060752575152407,"timestamp":1757498280602,"step":0,"task":null},{"name":"AUC-ROC","value":0.884978416188327,"timestamp":1757498280476,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7849411764705883,"timestamp":1757498281296,"step":340000,"task":null},{"name":"Well Calibration","value":0.5053599608809243,"timestamp":1757498280602,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.40281236420433,"timestamp":1757498280476,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.2810150000150315,"timestamp":1757498281333,"step":4,"task":null},{"name":"Equal Negative Predictive Value","value":0.394415390792033,"timestamp":1757498280602,"step":0,"task":null},{"name":"Cohen Kappa","value":0.30719666900388265,"timestamp":1757498280476,"step":0,"task":null},{"name":"Equal Opportunity","value":0.08444940476190475,"timestamp":1757498280602,"step":0,"task":null},{"name":"Equalized Odds","value":0.07047734887368116,"timestamp":1757498280602,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498280602,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498281333,"step":4,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498281333,"step":4,"task":null},{"name":"Accuracy","value":0.8015051740357478,"timestamp":1757498280476,"step":0,"task":null},{"name":"F1 Score","value":0.3812316715542522,"timestamp":1757498280476,"step":0,"task":null},{"name":"Log Loss","value":0.4211209316340222,"timestamp":1757498280476,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498280602,"step":0,"task":null},{"name":"Precision","value":0.9027777777777778,"timestamp":1757498280476,"step":0,"task":null},{"name":"Disparate Impact","value":0.03060752575152407,"timestamp":1757498280602,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":5.3,"timestamp":1757498281333,"step":4,"task":null},{"name":"system_network_receive_megabytes","value":5.315169000008609,"timestamp":1757498281333,"step":4,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/4a95843dfb04464d95249b9708b99f11/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_6","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"4a95843dfb04464d95249b9708b99f11\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:57:57.282650\", \"model_uuid\": \"11e2b480b91b45619cbb1d93d10cb097\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"42bf795efe3f4603b37836b8c050d10e","name":"Run_5","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498270941,"endTime":1757498275882,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498275355,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.551199130173727,"timestamp":1757498275355,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.036549654529833824,"timestamp":1757498275355,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.4,"timestamp":1757498275206,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.5737692587723124,"timestamp":1757498275355,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7475882352941177,"timestamp":1757498275718,"step":340000,"task":null},{"name":"Recall","value":0.3201219512195122,"timestamp":1757498275224,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29802.9,"timestamp":1757498275206,"step":3,"task":null},{"name":"Test Fairness","value":0.25555555555555554,"timestamp":1757498275355,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498275206,"step":3,"task":null},{"name":"Statistical Parity","value":0.8172016219452894,"timestamp":1757498275355,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.015432900432900434,"timestamp":1757498275355,"step":0,"task":null},{"name":"AUC-ROC","value":0.7611664177866269,"timestamp":1757498275224,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7447058823529413,"timestamp":1757498275801,"step":340000,"task":null},{"name":"Well Calibration","value":0.7818490387237245,"timestamp":1757498275355,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3937345654273045,"timestamp":1757498275224,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.130512999981875,"timestamp":1757498275206,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.552578047283033,"timestamp":1757498275355,"step":0,"task":null},{"name":"Cohen Kappa","value":0.33759604761727424,"timestamp":1757498275224,"step":0,"task":null},{"name":"Equal Opportunity","value":0.020710059171597635,"timestamp":1757498275355,"step":0,"task":null},{"name":"Equalized Odds","value":-0.036549654529833824,"timestamp":1757498275355,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498275355,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498275206,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498275206,"step":3,"task":null},{"name":"Accuracy","value":0.7638758231420508,"timestamp":1757498275224,"step":0,"task":null},{"name":"F1 Score","value":0.455531453362256,"timestamp":1757498275224,"step":0,"task":null},{"name":"Log Loss","value":0.5299841685206211,"timestamp":1757498275224,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498275355,"step":0,"task":null},{"name":"Precision","value":0.7894736842105263,"timestamp":1757498275224,"step":0,"task":null},{"name":"Disparate Impact","value":0.015432900432900434,"timestamp":1757498275355,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":27.6,"timestamp":1757498275206,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.119671999971615,"timestamp":1757498275206,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/42bf795efe3f4603b37836b8c050d10e/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_5","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"42bf795efe3f4603b37836b8c050d10e\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:57:52.245826\", \"model_uuid\": \"869d7b81ecab4aa6bf29ed5596890491\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"cf59b44a9bd548d98b7ccee73c1c27f6","name":"Run_4","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498253958,"endTime":1757498270894,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498270379,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40053367776756676,"timestamp":1757498270379,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.17922025198996677,"timestamp":1757498270379,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.4,"timestamp":1757498270137,"step":12,"task":null},{"name":"Balance for Negative Class","value":0.7802071102557538,"timestamp":1757498270379,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7937647058823529,"timestamp":1757498270733,"step":340000,"task":null},{"name":"Recall","value":0.22676579925650558,"timestamp":1757498270267,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29754.2,"timestamp":1757498270137,"step":12,"task":null},{"name":"Test Fairness","value":0.3548387096774194,"timestamp":1757498270379,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498270137,"step":12,"task":null},{"name":"Statistical Parity","value":0.9217115467115468,"timestamp":1757498270379,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.07125030584781014,"timestamp":1757498270379,"step":0,"task":null},{"name":"AUC-ROC","value":0.8919451649452679,"timestamp":1757498270267,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7910588235294118,"timestamp":1757498270814,"step":340000,"task":null},{"name":"Well Calibration","value":0.5136919712181838,"timestamp":1757498270379,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.4128978098124252,"timestamp":1757498270267,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.600021000020206,"timestamp":1757498270137,"step":12,"task":null},{"name":"Equal Negative Predictive Value","value":0.39125937115828685,"timestamp":1757498270379,"step":0,"task":null},{"name":"Cohen Kappa","value":0.3002676931371504,"timestamp":1757498270267,"step":0,"task":null},{"name":"Equal Opportunity","value":0.18321226795803064,"timestamp":1757498270379,"step":0,"task":null},{"name":"Equalized Odds","value":0.17922025198996677,"timestamp":1757498270379,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498270379,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498270137,"step":12,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498270137,"step":12,"task":null},{"name":"Accuracy","value":0.8024459078080903,"timestamp":1757498270267,"step":0,"task":null},{"name":"F1 Score","value":0.3674698795180723,"timestamp":1757498270267,"step":0,"task":null},{"name":"Log Loss","value":0.42070802579302446,"timestamp":1757498270267,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498270379,"step":0,"task":null},{"name":"Precision","value":0.9682539682539683,"timestamp":1757498270267,"step":0,"task":null},{"name":"Disparate Impact","value":0.07125030584781014,"timestamp":1757498270379,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":23.4,"timestamp":1757498270137,"step":12,"task":null},{"name":"system_network_receive_megabytes","value":5.476440999977058,"timestamp":1757498270137,"step":12,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/cf59b44a9bd548d98b7ccee73c1c27f6/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_4","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"cf59b44a9bd548d98b7ccee73c1c27f6\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:57:47.430968\", \"model_uuid\": \"2724690cf3dd41c8927e03fce5c0dc0b\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"0019dd3eec2f43b7b09deb426d436342","name":"Run_3","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498248739,"endTime":1757498253907,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"standard","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498253393,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40053367776756676,"timestamp":1757498253393,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.17922025198996677,"timestamp":1757498253393,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.4,"timestamp":1757498253086,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.7802071102557538,"timestamp":1757498253393,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7977058823529413,"timestamp":1757498253737,"step":340000,"task":null},{"name":"Recall","value":0.22676579925650558,"timestamp":1757498253261,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29727.5,"timestamp":1757498253086,"step":3,"task":null},{"name":"Test Fairness","value":0.3548387096774194,"timestamp":1757498253393,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498253086,"step":3,"task":null},{"name":"Statistical Parity","value":0.9217115467115468,"timestamp":1757498253393,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.07125030584781014,"timestamp":1757498253393,"step":0,"task":null},{"name":"AUC-ROC","value":0.8919779386289364,"timestamp":1757498253261,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7903529411764706,"timestamp":1757498253823,"step":340000,"task":null},{"name":"Well Calibration","value":0.5133941348808254,"timestamp":1757498253393,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.4128978098124252,"timestamp":1757498253261,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.062213999975938,"timestamp":1757498253086,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.39125937115828685,"timestamp":1757498253393,"step":0,"task":null},{"name":"Cohen Kappa","value":0.3002676931371504,"timestamp":1757498253261,"step":0,"task":null},{"name":"Equal Opportunity","value":0.18321226795803064,"timestamp":1757498253393,"step":0,"task":null},{"name":"Equalized Odds","value":0.17922025198996677,"timestamp":1757498253393,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498253393,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498253086,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498253086,"step":3,"task":null},{"name":"Accuracy","value":0.8024459078080903,"timestamp":1757498253261,"step":0,"task":null},{"name":"F1 Score","value":0.3674698795180723,"timestamp":1757498253261,"step":0,"task":null},{"name":"Log Loss","value":0.42064043877572554,"timestamp":1757498253261,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498253393,"step":0,"task":null},{"name":"Precision","value":0.9682539682539683,"timestamp":1757498253261,"step":0,"task":null},{"name":"Disparate Impact","value":0.07125030584781014,"timestamp":1757498253393,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":43.7,"timestamp":1757498253086,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.060360000003129,"timestamp":1757498253086,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0019dd3eec2f43b7b09deb426d436342/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_3","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"0019dd3eec2f43b7b09deb426d436342\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:57:30.034362\", \"model_uuid\": \"d75587da392d4563bd65ada5a7f64e44\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"994fb7fd6cb0489c895ecc4c955e6737","name":"Run_2","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498243382,"endTime":1757498248690,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"relabel_data","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498248204,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.551199130173727,"timestamp":1757498248204,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":-0.036549654529833824,"timestamp":1757498248204,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":29.3,"timestamp":1757498247630,"step":3,"task":null},{"name":"Balance for Negative Class","value":0.5737692587723124,"timestamp":1757498248204,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7506470588235294,"timestamp":1757498248520,"step":340000,"task":null},{"name":"Recall","value":0.3201219512195122,"timestamp":1757498248082,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29687.0,"timestamp":1757498247630,"step":3,"task":null},{"name":"Test Fairness","value":0.25555555555555554,"timestamp":1757498248204,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498247630,"step":3,"task":null},{"name":"Statistical Parity","value":0.8172016219452894,"timestamp":1757498248204,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.015432900432900434,"timestamp":1757498248204,"step":0,"task":null},{"name":"AUC-ROC","value":0.7610917537746806,"timestamp":1757498248082,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7463529411764707,"timestamp":1757498248608,"step":340000,"task":null},{"name":"Well Calibration","value":0.7815245943812625,"timestamp":1757498248204,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.3937345654273045,"timestamp":1757498248082,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":5.089841000008164,"timestamp":1757498247630,"step":3,"task":null},{"name":"Equal Negative Predictive Value","value":0.552578047283033,"timestamp":1757498248204,"step":0,"task":null},{"name":"Cohen Kappa","value":0.33759604761727424,"timestamp":1757498248082,"step":0,"task":null},{"name":"Equal Opportunity","value":0.020710059171597635,"timestamp":1757498248204,"step":0,"task":null},{"name":"Equalized Odds","value":-0.036549654529833824,"timestamp":1757498248204,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498248204,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498247630,"step":3,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498247630,"step":3,"task":null},{"name":"Accuracy","value":0.7638758231420508,"timestamp":1757498248082,"step":0,"task":null},{"name":"F1 Score","value":0.455531453362256,"timestamp":1757498248082,"step":0,"task":null},{"name":"Log Loss","value":0.5300461966514416,"timestamp":1757498248082,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498248204,"step":0,"task":null},{"name":"Precision","value":0.7894736842105263,"timestamp":1757498248082,"step":0,"task":null},{"name":"Disparate Impact","value":0.015432900432900434,"timestamp":1757498248204,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":26.1,"timestamp":1757498247630,"step":3,"task":null},{"name":"system_network_receive_megabytes","value":5.072211999999126,"timestamp":1757498247630,"step":3,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/994fb7fd6cb0489c895ecc4c955e6737/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_2","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"994fb7fd6cb0489c895ecc4c955e6737\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:57:24.707475\", \"model_uuid\": \"4d41981ce9814e7c8d4097246280220d\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"0e77d79e9f4c4e74bb69ef591d6f0a12","name":"Run_1","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498224248,"endTime":1757498243325,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"disparate_impact_remover","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498242264,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40053367776756676,"timestamp":1757498242264,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.17922025198996677,"timestamp":1757498242264,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":28.8,"timestamp":1757498242426,"step":13,"task":null},{"name":"Balance for Negative Class","value":0.7802071102557538,"timestamp":1757498242264,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7915882352941177,"timestamp":1757498243159,"step":340000,"task":null},{"name":"Recall","value":0.22676579925650558,"timestamp":1757498242137,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":29191.0,"timestamp":1757498242426,"step":13,"task":null},{"name":"Test Fairness","value":0.3548387096774194,"timestamp":1757498242264,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498242426,"step":13,"task":null},{"name":"Statistical Parity","value":0.9217115467115468,"timestamp":1757498242264,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.07125030584781014,"timestamp":1757498242264,"step":0,"task":null},{"name":"AUC-ROC","value":0.8919451649452679,"timestamp":1757498242137,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7837647058823529,"timestamp":1757498243240,"step":340000,"task":null},{"name":"Well Calibration","value":0.5136919712181838,"timestamp":1757498242264,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.4128978098124252,"timestamp":1757498242137,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":6.233831999998074,"timestamp":1757498242426,"step":13,"task":null},{"name":"Equal Negative Predictive Value","value":0.39125937115828685,"timestamp":1757498242264,"step":0,"task":null},{"name":"Cohen Kappa","value":0.3002676931371504,"timestamp":1757498242137,"step":0,"task":null},{"name":"Equal Opportunity","value":0.18321226795803064,"timestamp":1757498242264,"step":0,"task":null},{"name":"Equalized Odds","value":0.17922025198996677,"timestamp":1757498242264,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498242264,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498242426,"step":13,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498242426,"step":13,"task":null},{"name":"Accuracy","value":0.8024459078080903,"timestamp":1757498242137,"step":0,"task":null},{"name":"F1 Score","value":0.3674698795180723,"timestamp":1757498242137,"step":0,"task":null},{"name":"Log Loss","value":0.42070802579302446,"timestamp":1757498242137,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498242264,"step":0,"task":null},{"name":"Precision","value":0.9682539682539683,"timestamp":1757498242137,"step":0,"task":null},{"name":"Disparate Impact","value":0.07125030584781014,"timestamp":1757498242264,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":26.4,"timestamp":1757498242426,"step":13,"task":null},{"name":"system_network_receive_megabytes","value":5.2638459999870975,"timestamp":1757498242426,"step":13,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/0e77d79e9f4c4e74bb69ef591d6f0a12/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_1","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"0e77d79e9f4c4e74bb69ef591d6f0a12\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:57:19.188038\", \"model_uuid\": \"7a922c7a774a49cda683fad4a1929ce8\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null},{"id":"5e6fd4cb5fe44a3ab3857f9add48541a","name":"Run_0","experimentId":"706300898836261706","status":"COMPLETED","startTime":1757498212344,"endTime":1757498224209,"params":[{"name":"criterion","value":"gini","task":null},{"name":"fairness_method","value":"none","task":null},{"name":"random_state","value":"17","task":null},{"name":"max_depth","value":"3","task":null},{"name":"normalization","value":"none","task":null},{"name":"n_estimators","value":"20","task":null}],"metrics":[{"name":"Predictive Equality","value":0.0,"timestamp":1757498221084,"step":0,"task":null},{"name":"Overall Accuracy Equality","value":0.40053367776756676,"timestamp":1757498221084,"step":0,"task":null},{"name":"Conditional Use Accuracy Equality","value":0.17922025198996677,"timestamp":1757498221084,"step":0,"task":null},{"name":"system_system_memory_usage_percentage","value":27.2,"timestamp":1757498223396,"step":9,"task":null},{"name":"Balance for Negative Class","value":0.7802071102557538,"timestamp":1757498221084,"step":0,"task":null},{"name":"learning_curve_train_score","value":0.7982941176470588,"timestamp":1757498224063,"step":340000,"task":null},{"name":"Recall","value":0.22676579925650558,"timestamp":1757498220968,"step":0,"task":null},{"name":"system_system_memory_usage_megabytes","value":27516.3,"timestamp":1757498223396,"step":9,"task":null},{"name":"Test Fairness","value":0.3548387096774194,"timestamp":1757498221084,"step":0,"task":null},{"name":"system_disk_usage_percentage","value":99.9,"timestamp":1757498223396,"step":9,"task":null},{"name":"Statistical Parity","value":0.9217115467115468,"timestamp":1757498221084,"step":0,"task":null},{"name":"Conditional Statistical Parity","value":0.07125030584781014,"timestamp":1757498221084,"step":0,"task":null},{"name":"AUC-ROC","value":0.8919779386289364,"timestamp":1757498220968,"step":0,"task":null},{"name":"learning_curve_test_score","value":0.7936470588235294,"timestamp":1757498224132,"step":340000,"task":null},{"name":"Well Calibration","value":0.5133941348808254,"timestamp":1757498221084,"step":0,"task":null},{"name":"Matthews Corrcoef","value":0.4128978098124252,"timestamp":1757498220968,"step":0,"task":null},{"name":"system_network_transmit_megabytes","value":6.63907800000743,"timestamp":1757498223396,"step":9,"task":null},{"name":"Equal Negative Predictive Value","value":0.39125937115828685,"timestamp":1757498221084,"step":0,"task":null},{"name":"Cohen Kappa","value":0.3002676931371504,"timestamp":1757498220968,"step":0,"task":null},{"name":"Equal Opportunity","value":0.18321226795803064,"timestamp":1757498221084,"step":0,"task":null},{"name":"Equalized Odds","value":0.17922025198996677,"timestamp":1757498221084,"step":0,"task":null},{"name":"Balance for Positive Class","value":0.0,"timestamp":1757498221084,"step":0,"task":null},{"name":"system_disk_usage_megabytes","value":18199.8,"timestamp":1757498223396,"step":9,"task":null},{"name":"system_disk_available_megabytes","value":17.6,"timestamp":1757498223396,"step":9,"task":null},{"name":"Accuracy","value":0.8024459078080903,"timestamp":1757498220968,"step":0,"task":null},{"name":"F1 Score","value":0.3674698795180723,"timestamp":1757498220968,"step":0,"task":null},{"name":"Log Loss","value":0.42064043877572554,"timestamp":1757498220968,"step":0,"task":null},{"name":"Treatment Equality","value":0.0,"timestamp":1757498221084,"step":0,"task":null},{"name":"Precision","value":0.9682539682539683,"timestamp":1757498220968,"step":0,"task":null},{"name":"Disparate Impact","value":0.07125030584781014,"timestamp":1757498221084,"step":0,"task":null},{"name":"system_cpu_utilization_percentage","value":62.2,"timestamp":1757498223396,"step":9,"task":null},{"name":"system_network_receive_megabytes","value":5.681020000018179,"timestamp":1757498223396,"step":9,"task":null}],"dataAssets":[{"name":"input_data","sourceType":null,"source":"706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/input_data/input_data.csv","format":null,"role":"INPUT","task":null,"folder":null,"tags":{"schema":"{\"mlflow_colspec\": [{\"type\": \"long\", \"name\": \"age\", \"required\": true}, {\"type\": \"string\", \"name\": \"workclass\", \"required\": true}, {\"type\": \"string\", \"name\": \"education\", \"required\": true}, {\"type\": \"long\", \"name\": \"education-num\", \"required\": true}, {\"type\": \"string\", \"name\": \"marital-status\", \"required\": true}, {\"type\": \"string\", \"name\": \"occupation\", \"required\": true}, {\"type\": \"string\", \"name\": \"relationship\", \"required\": true}, {\"type\": \"string\", \"name\": \"race\", \"required\": true}, {\"type\": \"string\", \"name\": \"sex\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-gain\", \"required\": true}, {\"type\": \"long\", \"name\": \"capital-loss\", \"required\": true}, {\"type\": \"long\", \"name\": \"hours-per-week\", \"required\": true}, {\"type\": \"string\", \"name\": \"native-country\", \"required\": true}, {\"type\": \"long\", \"name\": \"income\", \"required\": true}]}","profile":"{\"num_rows\": 45222, \"num_elements\": 633108}","digest":"3dc8a779","mlflow.data.context":"input_data"}},{"name":"MLmodel","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/adult_rf_model/MLmodel","format":null,"role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"conda.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/adult_rf_model/conda.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/adult_rf_model/input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/adult_rf_model/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"python_env.yaml","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/adult_rf_model/python_env.yaml","format":"yaml","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"requirements.txt","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/adult_rf_model/requirements.txt","format":"txt","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"serving_input_example.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/adult_rf_model/serving_input_example.json","format":"json","role":"OUTPUT","task":null,"folder":"adult_rf_model","tags":null},{"name":"X_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/explainability/X_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"X_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/explainability/X_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_pred.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/explainability/Y_pred.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_test.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/explainability/Y_test.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"Y_train.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/explainability/Y_train.csv","format":"csv","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"model.pkl","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/explainability/model.pkl","format":"pkl","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"roc_data.json","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/explainability/roc_data.json","format":"json","role":"OUTPUT","task":null,"folder":"explainability","tags":null},{"name":"input_data.csv","sourceType":"local","source":"/data/xtreme/mlflow/mlartifacts/706300898836261706/5e6fd4cb5fe44a3ab3857f9add48541a/artifacts/input_data/input_data.csv","format":"csv","role":"OUTPUT","task":null,"folder":"input_data","tags":null}],"tasks":null,"tags":{"mlflow.runName":"Run_0","mlflow.source.git.commit":"3830f348f50aa7f24439f3700c1237ad9b9eae83","model_type":"RandomForestClassifier","mlflow.source.type":"LOCAL","mlflow.log-model.history":"[{\"run_id\": \"5e6fd4cb5fe44a3ab3857f9add48541a\", \"artifact_path\": \"adult_rf_model\", \"utc_time_created\": \"2025-09-10 09:56:53.844204\", \"model_uuid\": \"2383c3a61c5c4c438a210931b2ca1636\", \"flavors\": {\"python_function\": {\"model_path\": \"model.pkl\", \"predict_fn\": \"predict\", \"loader_module\": \"mlflow.sklearn\", \"python_version\": \"3.10.12\", \"env\": {\"conda\": \"conda.yaml\", \"virtualenv\": \"python_env.yaml\"}}, \"sklearn\": {\"pickled_model\": \"model.pkl\", \"sklearn_version\": \"1.4.2\", \"serialization_format\": \"cloudpickle\", \"code\": null}}}]","dataset":"adult","mlflow.user":"ktsopelas","mlflow.source.name":"adult-rf-experiment.py"},"space":null}]